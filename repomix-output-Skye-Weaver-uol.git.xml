This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: haarcascade_frontalface_default.xml, readme.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
Components/
  Captions.py
  config.py
  Database.py
  Edit.py
  FaceCrop.py
  LanguageTasks.py
  Logger.py
  Paths.py
  Speaker.py
  SpeakerDetection.py
  Transcription.py
  YoutubeDownloader.py
models/
  deploy.prototxt
.env.example
.gitattributes
.gitignore
clean_url.py
clear_highlights.py
config.yaml
LICENSE
main.py
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="Components/Captions.py">
import math
import cv2
import numpy as np # Add numpy import
import os
import subprocess
import traceback # For detailed error printing in animate_captions
from PIL import Image, ImageDraw, ImageFont # Pillow imports for custom font
from Components.Paths import fonts_path

# pure helper for unit tests and reuse
def _compute_bottom_margin_px(frame_h: int, bottom_offset_pct: int) -> int:
    try:
        # positioning via bottom_offset_pct
        pct = int(bottom_offset_pct)
    except Exception:
        pct = 0
    pct = max(0, min(pct, 100))
    try:
        fh = int(frame_h)
    except Exception:
        fh = 0
    return int(fh * pct / 100)
# Function to format time in SRT format
def format_time(seconds):
    milliseconds = int((seconds - math.floor(seconds)) * 1000)
    seconds = int(math.floor(seconds))
    minutes = seconds // 60
    seconds %= 60
    hours = minutes // 60
    minutes %= 60
    return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"

# Function to format time in ASS format (hours:mm:ss.cc)
def format_time_ass(seconds):
    centiseconds = int((seconds - math.floor(seconds)) * 100)
    seconds = int(math.floor(seconds))
    minutes = seconds // 60
    seconds %= 60
    hours = minutes // 60
    minutes %= 60
    return f"{hours:d}:{minutes:02d}:{seconds:02d}.{centiseconds:02d}"

# Function to generate ASS content (more compatible with ffmpeg filter)
def generate_ass_content(transcriptions, start_time, end_time, style_cfg=None, video_w=None, video_h=None):
    """
    Генерирует ASS-контент. Обратная совместимость сохранена:
    - При style_cfg is None — используются прежние константы (PlayRes 384x720, Fontsize=36, цвета/отступы как раньше).
    - При наличии style_cfg — применяются параметры из конфигурации:
      font_size, base_color, shadow (offset -> Shadow, color -> BackColour), letter_spacing -> Spacing,
      позиционирование Alignment/MarginV через position.mode и bottom_offset_pct.
    """

    # Локальный helper: HEX (#RRGGBB или #RRGGBBAA) -> &HAABBGGRR
    def _hex_to_ass_color(hex_str: str, default: str = "#FFFFFFFF") -> str:
        try:
            s = str(hex_str).strip()
            if not s.startswith("#"):
                raise ValueError("No #")
            s = s[1:]
            if len(s) == 6:
                rr, gg, bb = s[0:2], s[2:4], s[4:6]
                aa = "00"  # 00 — непрозрачно в ASS
            elif len(s) == 8:
                rr, gg, bb, aa = s[0:2], s[2:4], s[4:6], s[6:8]
            else:
                raise ValueError("Bad length")
            # Формат &HAABBGGRR
            return f"&H{aa}{bb}{gg}{rr}"
        except Exception:
            # На некорректный ввод — вернуть default
            if default != hex_str:
                return _hex_to_ass_color(default, "#FFFFFFFF")
            # Подстраховка (белый непрозрачный)
            return "&H00FFFFFF"

    # Дефолтные значения (как в предыдущей реализации)
    use_style = style_cfg is not None
    default_play_x, default_play_y = 384, 720
    play_res_x = default_play_x
    play_res_y = default_play_y
    if use_style and isinstance(video_w, (int, float)) and isinstance(video_h, (int, float)) and video_w > 0 and video_h > 0:
        # Используем реальный размер видео только если style_cfg задан (чтобы не менять старое поведение)
        play_res_x = int(video_w)
        play_res_y = int(video_h)

    font_size_default = 38
    font_size = font_size_default
    primary_colour = "&H00FFFFFF"  # белый
    outline_colour = "&H00000000"  # чёрный
    back_colour = "&H70000000"     # как было в коде
    outline = 2
    shadow_val = 1
    spacing_val = 0
    alignment = 2  # bottom-center
    margin_l = 10
    margin_r = 10
    margin_v = 30  # как было

    if use_style:
        # Font size
        try:
            fs = int(getattr(style_cfg, "font_size_px", font_size_default))
            font_size = max(20, min(60, fs))
        except Exception:
            font_size = font_size_default

        # Primary colour (base_color)
        try:
            primary_colour = _hex_to_ass_color(getattr(style_cfg, "base_color", "#FFFFFF"))
        except Exception:
            primary_colour = "&H00FFFFFF"

        # Back colour from shadow.color
        try:
            shadow_cfg = getattr(style_cfg, "shadow", None)
            if shadow_cfg:
                back_colour = _hex_to_ass_color(getattr(shadow_cfg, "color", "#00000080"))
                # Shadow offset (ASS поддерживает только интенсивность)
                sx = int(getattr(shadow_cfg, "x_px", 2) or 0)
                sy = int(getattr(shadow_cfg, "y_px", 2) or 0)
                shadow_int = max(sx, sy)
                try:
                    shadow_val = max(1, min(4, int(round(shadow_int))))
                except Exception:
                    shadow_val = 1
        except Exception:
            pass

        # Letter spacing
        try:
            spacing_val = int(round(float(getattr(style_cfg, "letter_spacing_px", 0) or 0)))
        except Exception:
            spacing_val = 0

        # Alignment/MarginV
        # positioning via bottom_offset_pct and center_offset_pct
        # Alignment mapping: safe_bottom→2, center→5
        try:
            position = getattr(style_cfg, "position", None)
            mode = getattr(position, "mode", "safe_bottom") if position else "safe_bottom"
            bottom_offset_pct = int(getattr(position, "bottom_offset_pct", 22)) if position else 22
            center_offset_pct = int(getattr(position, "center_offset_pct", 12)) if position else 12
            if mode == "center":
                alignment = 5  # middle-center
                # Для режима center рассчитываем margin_v как смещение ниже центра
                try:
                    center_y = play_res_y // 2
                    offset_px = int(play_res_y * center_offset_pct / 100.0)
                    margin_v = play_res_y - (center_y + offset_px)  # Расстояние от низа до позиции текста
                except Exception:
                    margin_v = play_res_y // 2 - 50  # Резервное значение
            else:
                alignment = 2  # bottom-center
                try:
                    margin_v = int(play_res_y * float(bottom_offset_pct) / 100.0)
                except Exception:
                    margin_v = 30
        except Exception:
            pass

    ass_content = (
        f"[Script Info]\n"
        f"Title: Auto-generated by AI-Youtube-Shorts-Generator\n"
        f"ScriptType: v4.00+\n"
        f"PlayResX: {play_res_x}\n"
        f"PlayResY: {play_res_y}\n"
        f"WrapStyle: 0\n"
        f"ScaledBorderAndShadow: yes\n"
        f"\n"
        f"[V4+ Styles]\n"
        f"Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, "
        f"OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, "
        f"ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, "
        f"MarginL, MarginR, MarginV, Encoding\n"
        f"Style: Default,Poppins,{font_size},{primary_colour},&H000000FF,{outline_colour},{back_colour},"
        f"-1,0,0,0,100,100,{spacing_val},0,1,{outline},{shadow_val},{alignment},{margin_l},{margin_r},{margin_v},1\n"
        f"Style: Fallback,Arial,{font_size},{primary_colour},&H000000FF,{outline_colour},{back_colour},"
        f"-1,0,0,0,100,100,{spacing_val},0,1,{outline},{shadow_val},{alignment},{margin_l},{margin_r},{margin_v},1\n"
        f"\n"
        f"[Events]\n"
        f"Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n"
    )

    for segment in transcriptions:
        text, seg_start, seg_end = segment
        if str(text).strip() == '[*]':
            continue
        if seg_start >= start_time and seg_end <= end_time:
            relative_start = seg_start - start_time
            relative_end = seg_end - start_time
            if relative_start < 0:
                relative_start = 0.0
            if relative_end <= relative_start:
                relative_end = relative_start + 0.1
            ass_content += (
                f"Dialogue: 0,{format_time_ass(relative_start)},{format_time_ass(relative_end)},"
                f"Default,,0,0,0,,{str(text).strip().upper()}\\N"
            )

    return ass_content

# Function to burn captions using FFmpeg
def burn_captions(vertical_video_path, audio_source_path, transcriptions, start_time, end_time, output_path, style_cfg=None):
    """Burns captions onto the vertical video using audio from the source segment."""
    temp_ass_path = "temp_subtitles.ass"  # Simple name in current directory
    # Пытаемся получить реальные размеры видео (для PlayRes при наличии style_cfg)
    vw, vh = None, None
    try:
        cap = cv2.VideoCapture(vertical_video_path)
        if cap.isOpened():
            vw = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            vh = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        if cap:
            cap.release()
    except Exception:
        vw, vh = None, None

    try:
        # Create an ASS subtitle file (more compatible than SRT for styling)
        ass_content = generate_ass_content(transcriptions, start_time, end_time, style_cfg=style_cfg, video_w=vw, video_h=vh)

        if not ass_content.count("Dialogue:"):
            print("No relevant transcriptions found for the highlight duration. Using video without captions.")
            # Need to add audio even if no captions are burned
            ffmpeg_command_no_subs = [
                'ffmpeg',
                '-i', vertical_video_path, # Silent video input
                '-i', audio_source_path,  # Audio source input
                '-map', '0:v:0', # Video from input 0
                '-map', '1:a:0', # Audio from input 1
                '-c:v', 'copy',  # Copy video stream (faster if no filter applied)
                '-c:a', 'aac',   # Re-encode audio
                '-b:a', '128k',
                '-shortest',    # Ensure output duration matches shortest input
                '-y',
                output_path
            ]
            print("Running FFmpeg command (no subtitles, adding audio):")
            cmd_string = ' '.join([str(arg) for arg in ffmpeg_command_no_subs])
            print(f"Command: {cmd_string}")
            process = subprocess.run(ffmpeg_command_no_subs, check=True, capture_output=True, text=True)
            print(f"Successfully muxed audio into: {output_path}")
            return True # Return true as the operation (adding audio) succeeded

        # Write the ASS content to the current directory
        with open(temp_ass_path, 'w', encoding='utf-8') as f:
            f.write(ass_content)

        print(f"Generated subtitle file: {temp_ass_path}")

        # FFmpeg command using two inputs and mapping streams
        ffmpeg_command = [
            'ffmpeg',
            '-i', vertical_video_path,  # Input 0: Vertically cropped video (silent)
            '-i', audio_source_path,   # Input 1: Original segment (with audio)
            # Use absolute path for subtitles file to avoid potential issues with ffmpeg's working directory
            '-filter_complex', f"[0:v]ass='{os.path.abspath(temp_ass_path)}'[video_out]",
            '-map', '[video_out]',     # Map the filtered video stream
            '-map', '1:a:0',           # Map the audio stream from input 1
            '-c:v', 'libx264',
            '-crf', '23',
            '-preset', 'medium',
            '-c:a', 'aac',           # Re-encode audio (required when filtering/mapping)
            '-b:a', '128k',
            '-shortest',             # Finish encoding when the shortest input ends
            '-y',
            output_path
        ]

        # Print the command for debugging
        print("Running FFmpeg command (burning subtitles and adding audio):")
        cmd_string = ' '.join([str(arg) for arg in ffmpeg_command])
        print(f"Command: {cmd_string}")

        # Run FFmpeg with the new command
        process = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)
        print(f"Successfully burned captions and added audio into: {output_path}")
        return True

    except subprocess.CalledProcessError as e:
        print(f"Error running FFmpeg: {e}")
        print(f"FFmpeg stdout: {e.stdout}")
        print(f"FFmpeg stderr: {e.stderr}")
        return False
    except Exception as e:
        print(f"An error occurred during caption burning: {e}")
        return False
    finally:
        # Always clean up the subtitle file
        if os.path.exists(temp_ass_path):
            try:
                os.remove(temp_ass_path)
                print(f"Removed temporary subtitle file: {temp_ass_path}")
            except Exception as e:
                print(f"Warning: Could not remove temporary subtitle file: {e}")


# --- Word-Level Animation Helpers ---

def find_active_segment_and_word(transcription_result, current_time):
    """Finds the segment and word active at the current time."""
    active_segment = None
    active_word_index_in_segment = -1 # Index within the segment's word list

    for segment in transcription_result.get("segments", []):
        # Use segment boundaries to find the active segment
        if segment['start'] <= current_time < segment['end']:
            active_segment = segment
            # Find the specific word within this segment based on word timings
            for i, word_info in enumerate(segment.get("words", [])):
                # Ensure word timings exist before comparing
                if 'start' in word_info and 'end' in word_info:
                    if word_info['start'] <= current_time < word_info['end']:
                        active_word_index_in_segment = i
                        break # Found the active word
            # If no specific word is active but the segment is, keep active_segment
            # active_word_index_in_segment will remain -1 or the found index
            break # Found the active segment

    return active_segment, active_word_index_in_segment


# --- Main Animation Function ---

def animate_captions(vertical_video_path, audio_source_path, transcription_result, output_path, style_cfg=None, highlight_meta=None):
    """Creates a video with word-by-word highlighted captions based on segments.
    - tone/keywords heuristic via optional highlight_meta for accent coloring
    """
    temp_animated_video = output_path + "_temp_anim.mp4"
    success = False
    cap = None
    out = None

    # Локальный helper: HEX -> RGBA (tuple)
    def _hex_to_rgba(hex_str: str, default=(255, 255, 0, 255)):
        try:
            s = str(hex_str).strip()
            if not s.startswith("#"):
                raise ValueError("No #")
            s = s[1:]
            if len(s) == 6:
                rr, gg, bb = int(s[0:2], 16), int(s[2:4], 16), int(s[4:6], 16)
                aa = 255
            elif len(s) == 8:
                rr, gg, bb, aa = int(s[0:2], 16), int(s[2:4], 16), int(s[4:6], 16), int(s[6:8], 16)
            else:
                raise ValueError("Bad length")
            return (rr, gg, bb, aa)
        except Exception:
            return default

    # Посимвольная отрисовка с letter-spacing (упрощение для одной строки)
    def _draw_text_with_spacing(draw_obj, start_xy, text, font, fill, stroke_width=0, stroke_fill=None, spacing_px=0):
        x, y = start_xy
        for idx, ch in enumerate(text):
            draw_obj.text((x, y), ch, font=font, fill=fill, stroke_width=stroke_width, stroke_fill=stroke_fill)
            # Используем getlength для более точного измерения ширины символа
            try:
                ch_w = font.getlength(ch)
            except AttributeError:  # Fallback for older/different PIL versions
                bbox = font.getbbox(ch) if hasattr(font, 'getbbox') else draw_obj.textbbox((0, 0), ch, font=font)
                ch_w = (bbox[2] - bbox[0]) if bbox else font.getsize(ch)[0]
            x += ch_w + (spacing_px if idx < len(text) - 1 else 0)

    # Оценка ширины текста с letter-spacing
    def _measure_text_width(draw_obj, text, font, spacing_px=0):
        if not text:
            return 0
        try:
            # Предпочтительный, более точный метод
            base_width = font.getlength(text)
            total_spacing = (len(text) - 1) * spacing_px if len(text) > 1 else 0
            return base_width + total_spacing
        except AttributeError:
            # Fallback для старых версий PIL или шрифтов без getlength
            total = 0
            for idx, ch in enumerate(text):
                bbox = font.getbbox(ch) if hasattr(font, 'getbbox') else draw_obj.textbbox((0, 0), ch, font=font)
                ch_w = (bbox[2] - bbox[0]) if bbox else font.getsize(ch)[0]
                total += ch_w
            total += (len(text) - 1) * spacing_px if len(text) > 1 else 0
            return total

    try:
        # --- Font Setup (Pillow) ---
        font_size = 38  # default legacy
        if style_cfg is not None:
            try:
                font_size = int(getattr(style_cfg, "font_size_px", font_size) or font_size)
            except Exception:
                pass

        font_path = fonts_path("Montserrat-Bold.ttf")
        font = None
        try:
            if os.path.exists(font_path):
                font = ImageFont.truetype(font_path, font_size)
                print(f"Successfully loaded font: {font_path}")
            else:
                print(f"Font file not found at {font_path}. Using PIL default font.")
                font = ImageFont.load_default()
        except Exception as e:
            print(f"Warning: could not load TTF font at {font_path}: {e}. Using PIL default font.")
            font = ImageFont.load_default()

        # --- Pre-filter segments ---
        original_segments = transcription_result.get("segments", [])
        filtered_segments = [seg for seg in original_segments if seg.get('text', '').strip() != '[*]']
        if not filtered_segments:
            print("Warning: No non-[*] segments found in transcription. Captions might be empty.")

        # Update the transcription_result to use filtered segments for further processing
        transcription_result_filtered = transcription_result.copy() # Avoid modifying original dict directly if reused
        transcription_result_filtered['segments'] = filtered_segments

        print("Starting animated caption generation (Static Window/Highlight Style)...")
        cap = cv2.VideoCapture(vertical_video_path)
        if not cap.isOpened():
            print(f"Error: Cannot open video file {vertical_video_path}")
            return False

        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps <= 0: # Handle zero or negative fps
             print(f"Error: Invalid video FPS ({fps}), cannot calculate time.")
             cap.release() # Release resource
             return False
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        print(f"Input video properties: {width}x{height} @ {fps:.2f}fps")

        # --- Text Styling (Pillow) ---
        # Base color
        text_rgba = _hex_to_rgba(getattr(style_cfg, "base_color", "#FFFF00FF") if style_cfg else "#FFFF00FF", (255, 255, 0, 255))
        stroke_color_rgb = (0, 0, 0)    # Black outline (legacy)
        stroke_width = 1                # Outline width in pixels

        # accent color mapping (palette from style_cfg or defaults)
        tone_color_rgba = None
        kw_set = set()
        if style_cfg is not None and highlight_meta:
            # «accent color mapping»
            try:
                # Prepare palette
                default_palette = {"urgency": "#FFD400", "drama": "#FF3B30", "positive": "#34C759"}
                pal_obj = getattr(style_cfg, "accent_palette", None)
                if pal_obj:
                    palette = {
                        "urgency": getattr(pal_obj, "urgency", default_palette["urgency"]),
                        "drama": getattr(pal_obj, "drama", default_palette["drama"]),
                        "positive": getattr(pal_obj, "positive", default_palette["positive"]),
                    }
                else:
                    palette = default_palette
                tone = str(highlight_meta.get("tone", "neutral") or "neutral")
                if tone in ("urgency", "drama", "positive"):
                    tone_hex = palette.get(tone)
                    if isinstance(tone_hex, str):
                        tone_color_rgba = _hex_to_rgba(tone_hex, text_rgba)
                # Build keyword set (lowercase)
                kws = highlight_meta.get("keywords", []) or []
                if isinstance(kws, (list, tuple)):
                    kw_set = {str(k).lower() for k in kws if isinstance(k, str)}
            except Exception:
                tone_color_rgba = None
                kw_set = set()

        # Shadow config
        sx = sy = 0
        shadow_rgba = (0, 0, 0, 128)
        if style_cfg is not None:
            sh = getattr(style_cfg, "shadow", None)
            if sh:
                try:
                    sx = int(getattr(sh, "x_px", 2) or 0)
                    sy = int(getattr(sh, "y_px", 2) or 0)
                except Exception:
                    sx = sy = 0
                shadow_rgba = _hex_to_rgba(getattr(sh, "color", "#00000080"), (0, 0, 0, 128))

        # Letter-spacing
        letter_spacing_px = 0
        if style_cfg is not None:
            try:
                letter_spacing_px = int(round(float(getattr(style_cfg, "letter_spacing_px", 0) or 0)))
            except Exception:
                letter_spacing_px = 0

        # Positioning
        position_mode = "safe_bottom"
        bottom_offset_pct = 22
        center_offset_pct = 12
        boundary_padding_px = 10
        if style_cfg is not None:
            pos = getattr(style_cfg, "position", None)
            if pos:
                position_mode = getattr(pos, "mode", "safe_bottom")
                try:
                    bottom_offset_pct = int(getattr(pos, "bottom_offset_pct", 22))
                except (ValueError, TypeError):
                    bottom_offset_pct = 22
                try:
                    center_offset_pct = int(getattr(pos, "center_offset_pct", 12))
                except (ValueError, TypeError):
                    center_offset_pct = 12
                try:
                    boundary_padding_px = int(getattr(pos, "boundary_padding_px", 10))
                except (ValueError, TypeError):
                    boundary_padding_px = 10

        # Emoji config and font (best-effort)
        emoji_enabled = False
        emoji_list_prepared = []
        emoji_max = 0
        emoji_window_s = 1.0  # emoji timing window ~1s from segment start
        emoji_font_size = max(8, int(font_size * 0.9))
        emoji_font = None

        if style_cfg is not None and isinstance(highlight_meta, dict):
            try:
                em_cfg = getattr(style_cfg, "emoji", None)
                emoji_enabled = bool(getattr(em_cfg, "enabled", False)) if em_cfg else False
                if emoji_enabled:
                    raw = list(highlight_meta.get("emojis", []) or [])
                    # дополнительная страховка по лимиту
                    emoji_max = int(getattr(em_cfg, "max_per_short", 0) or 0)
                    if emoji_max > 0:
                        emoji_list_prepared = [str(x) for x in raw if isinstance(x, str)][:emoji_max]
                    else:
                        emoji_list_prepared = []
            except Exception:
                emoji_enabled = False
                emoji_list_prepared = []

        def _load_emoji_font(sz: int):
            # emoji font loading (best-effort, platform paths)
            candidates = [
                "C:/Windows/Fonts/seguiemj.ttf",                        # Windows
                "/usr/share/fonts/truetype/noto/NotoColorEmoji.ttf",    # Linux
                "/System/Library/Fonts/Apple Color Emoji.ttc",          # macOS
            ]
            for pth in candidates:
                try:
                    if os.path.exists(pth):
                        return ImageFont.truetype(pth, sz)
                except Exception:
                    continue
            return None

        if emoji_enabled:
            try:
                emoji_font = _load_emoji_font(emoji_font_size)
            except Exception:
                emoji_font = None

        def _apply_emojis(base_img, start_x_val, total_w_val, start_y_val):
            """
            Отрисовать эмодзи на отдельном RGBA-слое и скомпозитить поверх кадра.
            - emoji: heuristics and placement
            - emoji font loading (best-effort, platform paths)
            - emoji timing window ~1s from segment start
            - effects: "pulse" / "shiny" в первые 0.8с от старта сегмента, затем статика до 1.0с
            """
            try:
                if not (emoji_enabled and emoji_list_prepared):
                    return base_img

                # easing helpers for emoji
                def _clamp01(v):
                    try:
                        v = float(v)
                    except Exception:
                        return 0.0
                    if v < 0.0:
                        return 0.0
                    if v > 1.0:
                        return 1.0
                    return v

                def _ease_out_cubic(t):
                    # easing: easeOutCubic
                    return 1.0 - (1.0 - float(t)) ** 3

                def _ease_in_out_sine(t):
                    # easing: easeInOutSine
                    return 0.5 * (1.0 - math.cos(math.pi * float(t)))

                def _lerp(a, b, t):
                    return a + (b - a) * float(t)

                # Время сегмента для окна эффекта
                seg_t0 = 0.0
                if isinstance(active_segment, dict):
                    try:
                        seg_t0 = float(active_segment.get("start", 0.0) or 0.0)
                    except Exception:
                        seg_t0 = 0.0

                # Общее окно показа эмодзи ~1.0 c от начала сегмента (совместимость)
                dt = current_time - seg_t0
                if not (0.0 <= dt <= emoji_window_s):
                    # Вне окна показа — ведём себя как раньше (не показываем)
                    return base_img

                # Нормированное время эффекта
                effect_duration = 0.8
                u_raw = dt / (effect_duration if effect_duration > 0 else 1e-6)
                u = _clamp01(u_raw)

                # Чтение стиля эмодзи
                em_style = "none"
                if style_cfg is not None:
                    try:
                        em_cfg = getattr(style_cfg, "emoji", None)
                        em_style = str(getattr(em_cfg, "style", "none") or "none").lower() if em_cfg else "none"
                    except Exception:
                        em_style = "none"

                overlay_em = Image.new("RGBA", (width, height), (0, 0, 0, 0))
                draw_em = ImageDraw.Draw(overlay_em)
                dx = int(font_size * 0.6)
                x_cursor = int(start_x_val + total_w_val + dx)
                y_draw = int(start_y_val)
                used_font = emoji_font if emoji_font else font  # fallback: текущий текстовый шрифт
                gap = max(2, int(emoji_font_size * 0.15))
                any_drawn = False

                for emo in emoji_list_prepared:
                    try:
                        # Базовые измерения
                        bbox = draw_em.textbbox((0, 0), emo, font=used_font)
                        w_emo = (bbox[2] - bbox[0]) if bbox else used_font.getsize(emo)[0]
                        h_emo = (bbox[3] - bbox[1]) if (bbox and len(bbox) >= 4) else font_size

                        # Рендер эмодзи на отдельный слой
                        emoji_layer = Image.new("RGBA", (max(1, w_emo), max(1, h_emo)), (0, 0, 0, 0))
                        emoji_draw = ImageDraw.Draw(emoji_layer)
                        emoji_draw.text((0, 0), emo, font=used_font, fill=(255, 255, 255, 255))

                        scale = 1.0
                        alpha_mult = 1.0

                        if em_style == "pulse":
                            # emoji: pulse effect (scale + alpha)
                            if 0.0 < u < 1.0:
                                u_e = _ease_in_out_sine(u)
                                scale = 1.0 + 0.06 * math.sin(math.pi * u_e)
                                alpha_mult = 0.85 + 0.15 * u
                            else:
                                # Вне окна эффекта, но всё ещё в окне показа (0.8..1.0) — статика
                                scale = 1.0
                                alpha_mult = 1.0

                        elif em_style == "shiny":
                            # emoji: shiny effect (moving gloss stripe)
                            if 0.0 < u < 1.0:
                                # Блик поверх исходного размера
                                gloss_width = max(3, int(0.15 * emoji_layer.width))
                                gloss_alpha = 0.45
                                progress = _ease_out_cubic(u)
                                gloss_x = int(round(_lerp(-gloss_width, emoji_layer.width, progress)))

                                gloss_layer = Image.new("RGBA", emoji_layer.size, (0, 0, 0, 0))
                                gloss_draw = ImageDraw.Draw(gloss_layer)
                                x0 = max(0, gloss_x)
                                x1 = min(emoji_layer.width, gloss_x + gloss_width)
                                if x1 > x0:
                                    gloss_color = (255, 255, 255, int(round(255 * gloss_alpha)))
                                    # Вертикальная полоса блика
                                    gloss_draw.rectangle([x0, 0, x1, emoji_layer.height], fill=gloss_color)

                                emoji_layer = Image.alpha_composite(emoji_layer, gloss_layer)

                                # Лёгкий масштаб
                                scale = 0.98 + 0.02 * u
                                alpha_mult = 1.0
                            else:
                                scale = 1.0
                                alpha_mult = 1.0

                        # Масштабирование слоя эмодзи
                        if abs(scale - 1.0) > 1e-3:
                            new_w = max(1, int(round(emoji_layer.width * scale)))
                            new_h = max(1, int(round(emoji_layer.height * scale)))
                            emoji_layer = emoji_layer.resize((new_w, new_h), resample=Image.BICUBIC)

                        # Применение альфа-множителя
                        if alpha_mult < 0.999:
                            r, g, b, a = emoji_layer.split()
                            a = a.point(lambda v, am=alpha_mult: int(v * am))
                            emoji_layer = Image.merge("RGBA", (r, g, b, a))

                        # Компоновка
                        overlay_em.paste(emoji_layer, (x_cursor, y_draw), mask=emoji_layer)
                        x_cursor += w_emo + gap
                        any_drawn = True

                    except Exception:
                        # Тихий пропуск проблемного эмодзи
                        continue

                if any_drawn:
                    return Image.alpha_composite(base_img, overlay_em)
                return base_img
            except Exception:
                return base_img

        # --- Calculate Fixed Y Position (after getting height) ---
        font_ascent = 0 # Default if font fails
        if font:
            try:
                font_ascent, _ = font.getmetrics()
            except Exception:
                 print("Warning: Could not get font metrics.")
        # legacy defaults
        bottom_margin_legacy = 120
        fixed_top_y_legacy = height - bottom_margin_legacy - font_ascent

        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(temp_animated_video, fourcc, fps, (width, height))
        if not out.isOpened():
            print(f"Error: Could not open video writer for {temp_animated_video}")
            cap.release() # Release resource
            return False

        frame_count = 0
        drawn_any_text = False
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            current_time = frame_count / fps

            # Find the segment and specific word active at this frame's time
            # Use the filtered transcription data
            active_segment, active_word_idx_in_segment = find_active_segment_and_word(transcription_result_filtered, current_time)

            # --- Get Words to Display (Max 2) ---
            words_to_display = ""
            window_words = []  # [(text, word_info)] for per-word animation window
            if active_segment:
                segment_words_list = active_segment.get('words', [])
                num_words_in_segment = len(segment_words_list)

                if 0 <= active_word_idx_in_segment < num_words_in_segment:
                    word1_info = segment_words_list[active_word_idx_in_segment]
                    # Fallback to 'word' if 'text' missing (compat)
                    word1_text = (word1_info.get('text') or word1_info.get('word') or '').strip()

                    # Skip if the primary word is [*]
                    if word1_text != '[*]':
                        # Теперь отображаем только одно слово и в верхнем регистре
                        words_to_display = word1_text.upper()
                        window_words.append((words_to_display, word1_info))

            # --- Drawing Logic (Pillow - Max 2 words) ---
            # Draw only if we have a valid window and an active word within it
            if words_to_display and font: # Only draw if we have words and font loaded
                # Convert frame BGR OpenCV to RGB Pillow
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # При наличии style_cfg — поддерживаем RGBA + тень/альфа/letter-spacing,
                # иначе сохраняем прежнее поведение (совместимость).
                if style_cfg is None:
                    pil_image = Image.fromarray(frame_rgb)
                    draw = ImageDraw.Draw(pil_image)
                    text_bbox = draw.textbbox((0, 0), words_to_display, font=font)
                    text_width = text_bbox[2] - text_bbox[0] if text_bbox else 0
                    start_x = (width - text_width) // 2
                    fixed_top_y = fixed_top_y_legacy
                    # Draw the text (legacy)
                    draw.text(
                        (start_x, fixed_top_y),
                        words_to_display,
                        font=font,
                        fill=(255, 255, 0),
                        stroke_width=stroke_width,
                        stroke_fill=stroke_color_rgb
                    )
                    drawn_any_text = True
                    frame = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
                else:
                    # Расширенный путь с наложением RGBA + опциональная анимация per-word
                    base_rgb = Image.fromarray(frame_rgb).convert("RGBA")
                    anim_cfg = getattr(style_cfg, "animate", None)

                    # Easing helpers + clamp (локальные)
                    def _clamp01(v):
                        try:
                            v = float(v)
                        except Exception:
                            return 0.0
                        if v < 0.0:
                            return 0.0
                        if v > 1.0:
                            return 1.0
                        return v

                    def _ease_out_cubic(t):
                        # easing: easeOutCubic
                        return 1.0 - (1.0 - float(t)) ** 3

                    # Общие измерения
                    tmp_draw = ImageDraw.Draw(Image.new("RGB", (1, 1)))
                    # Высота строки по bbox (вертикаль не зависит от spacing)
                    text_bbox = tmp_draw.textbbox((0, 0), words_to_display, font=font)
                    text_h = (text_bbox[3] - text_bbox[1]) if text_bbox else font_size

                    # Позиционирование
                    if position_mode == "center":
                        # Центрируем по вертикали, затем смещаем ниже центра
                        try:
                            offset_px = int(height * (center_offset_pct / 100.0))
                        except Exception:
                            offset_px = int(height * 0.12)  # 12% fallback
                        start_y = (height - text_h) // 2 + offset_px
                    else:  # safe_bottom
                        try:
                            margin_y = _compute_bottom_margin_px(height, bottom_offset_pct or 22)
                        except Exception:
                            margin_y = 120
                        start_y = height - margin_y - text_h # Используем text_h вместо font_ascent для большей точности

                    # --- Горизонтальное позиционирование и clamping ---
                    
                    # Общая ширина текста для центрирования
                    total_text_w = 0
                    
                    if not anim_cfg and not window_words:
                         total_text_w = _measure_text_width(tmp_draw, words_to_display, font, spacing_px=letter_spacing_px)
                    elif window_words:
                        space_w_bbox = tmp_draw.textbbox((0, 0), " ", font=font)
                        space_w = (space_w_bbox[2] - space_w_bbox[0]) if space_w_bbox else max(1, font_size // 3)
                        word_widths_local = [
                            _measure_text_width(tmp_draw, wt, font, spacing_px=letter_spacing_px)
                            for wt, _ in window_words
                        ]
                        total_text_w = sum(word_widths_local) + (len(word_widths_local) - 1) * space_w if word_widths_local else 0
                    
                    start_x = (width - total_text_w) // 2
                    
                    # Применяем boundary_padding_px для clamping
                    padding = boundary_padding_px or 10
                    start_x = max(padding, min(start_x, width - total_text_w - padding))
                    start_y = max(padding, min(start_y, height - text_h - padding))

                    if not anim_cfg:
                        import re as _re_kc
                        overlay = Image.new("RGBA", (width, height), (0, 0, 0, 0))
                        draw_ov = ImageDraw.Draw(overlay)
                        
                        # Если window_words пуст, отрисовываем всю строку
                        if not window_words:
                            # Shadow
                            if sx != 0 or sy != 0 or (shadow_rgba and shadow_rgba[3] > 0):
                                _draw_text_with_spacing(draw_ov, (start_x + sx, start_y + sy), words_to_display, font, shadow_rgba, 0, None, letter_spacing_px)
                            # Main text
                            _draw_text_with_spacing(draw_ov, (start_x, start_y), words_to_display, font, text_rgba, stroke_width, stroke_color_rgb, letter_spacing_px)
                        else:
                            # Отрисовка по словам (для keyword coloring)
                            space_w_bbox = tmp_draw.textbbox((0, 0), " ", font=font)
                            space_w = (space_w_bbox[2] - space_w_bbox[0]) if space_w_bbox else max(1, font_size // 3)
                            word_widths = [_measure_text_width(tmp_draw, wt, font, spacing_px=letter_spacing_px) for wt, _ in window_words]
                            
                            # Shadow pass
                            if sx != 0 or sy != 0 or (shadow_rgba and shadow_rgba[3] > 0):
                                x_cursor = start_x
                                for idx, (w_text, _) in enumerate(window_words):
                                    _draw_text_with_spacing(draw_ov, (x_cursor + sx, start_y + sy), w_text, font, shadow_rgba, 0, None, letter_spacing_px)
                                    x_cursor += word_widths[idx] + (space_w if idx < len(word_widths) - 1 else 0)
                            
                            # Main pass
                            x_cursor = start_x
                            def _sanitize_kw(token: str) -> str: return _re_kc.sub(r"[^\wа-яё]+", "", token.lower())
                            for idx, (w_text, _) in enumerate(window_words):
                                use_rgba = text_rgba
                                if tone_color_rgba and kw_set and _sanitize_kw(w_text) in kw_set:
                                    use_rgba = tone_color_rgba
                                _draw_text_with_spacing(draw_ov, (x_cursor, start_y), w_text, font, use_rgba, stroke_width, stroke_color_rgb, letter_spacing_px)
                                x_cursor += word_widths[idx] + (space_w if idx < len(word_widths) - 1 else 0)

                        composed = Image.alpha_composite(base_rgb, overlay)
                        composed = _apply_emojis(composed, start_x, total_text_w, start_y)
                        drawn_any_text = True
                        frame = cv2.cvtColor(np.array(composed.convert("RGB")), cv2.COLOR_RGB2BGR)
                    else:
                        # --- Анимация per-word ---
                        try:
                            # Параметры анимации с клиппингом/фолбэками
                            anim_type = str(getattr(anim_cfg, "type", "slide-up") or "slide-up")
                            if anim_type not in ("slide-up", "pop-in"):
                                anim_type = "slide-up"
                            try:
                                duration_s = float(getattr(anim_cfg, "duration_s", 0.35) or 0.35)
                            except Exception:
                                duration_s = 0.35
                            duration_s = max(0.2, min(0.5, duration_s))
                            easing_name = str(getattr(anim_cfg, "easing", "easeOutCubic") or "easeOutCubic")
                            try:
                                stagger_ms = int(getattr(anim_cfg, "per_word_stagger_ms", 0) or 0)
                            except Exception:
                                stagger_ms = 0
                            stagger_s = max(0, stagger_ms) / 1000.0

                            # Выбор функции easing
                            if easing_name == "easeOutCubic":
                                ease = _ease_out_cubic
                            else:
                                ease = lambda t: float(t)  # Линейный фолбэк

                            # Подготовка окна слов; если не собрали — рендер одной строкой
                            if not window_words:
                                if words_to_display:
                                    window_words = [(words_to_display, {"start": current_time})]

                            # Измеряем ширины слов и пробела
                            space_w_bbox = tmp_draw.textbbox((0, 0), " ", font=font)
                            space_w = (space_w_bbox[2] - space_w_bbox[0]) if space_w_bbox else max(1, font_size // 3)
                            word_widths = [
                                _measure_text_width(tmp_draw, wt, font, spacing_px=letter_spacing_px)
                                for wt, _ in window_words
                            ]
                            total_text_w = sum(word_widths) + (len(word_widths) - 1) * space_w if word_widths else 0
                            start_x = (width - total_text_w) // 2

                            # Базовый вертикальный смещающий оффсет для slide-up
                            offsetY0 = int(max(12, min(24, round(0.25 * font_size))))

                            x_cursor = start_x
                            any_drawn_local = False

                            for idx, (w_text, w_info) in enumerate(window_words):
                                # t0 — время начала слова (сек)
                                try:
                                    t0 = float(w_info.get("start", current_time) or current_time)
                                except Exception:
                                    t0 = current_time
                                # Stagger по индексу в текущем окне
                                t0_staggered = t0 + idx * stagger_s

                                # Прогресс анимации per-word
                                progress_raw = (current_time - t0_staggered) / (duration_s if duration_s > 0 else 1e-6)
                                progress = _clamp01(progress_raw)
                                final = ease(progress)  # easing

                                # Преобразования: pop-in / slide-up
                                if anim_type == "pop-in":
                                    # pop-in: scale 0.85->1.0, alpha = final, translateY=0
                                    scale = 0.85 + 0.15 * final
                                    translateY = 0
                                    alpha_mult = final
                                else:
                                    # slide-up: смещение снизу, лёгкий scale 0.98->1.0, alpha = final
                                    translateY = int(round((1.0 - final) * offsetY0))
                                    scale = 0.98 + 0.02 * final
                                    alpha_mult = final

                                # Если слово ещё не началось с учётом stagger — не показываем
                                if progress <= 0.0 or alpha_mult <= 0.0:
                                    x_cursor += word_widths[idx] + (space_w if idx < len(word_widths) - 1 else 0)
                                    continue

                                # Рендер слова на отдельном RGBA-слое
                                word_w = word_widths[idx]
                                # Паддинги под тень, чтобы не обрезать смещённую копию
                                pad_left = max(0, -sx)
                                pad_top = max(0, -sy)
                                pad_right = max(0, sx)
                                pad_bottom = max(0, sy)
                                wl_w = max(1, word_w + pad_left + pad_right)
                                wl_h = max(1, text_h + pad_top + pad_bottom)
                                word_layer = Image.new("RGBA", (wl_w, wl_h), (0, 0, 0, 0))
                                draw_wl = ImageDraw.Draw(word_layer)

                                # Тень (идёт теми же трансформациями)
                                if sx != 0 or sy != 0 or (shadow_rgba and shadow_rgba[3] > 0):
                                    _draw_text_with_spacing(
                                        draw_wl,
                                        (pad_left + sx, pad_top + sy),
                                        w_text,
                                        font=font,
                                        fill=shadow_rgba,
                                        stroke_width=0,
                                        stroke_fill=None,
                                        spacing_px=letter_spacing_px
                                    )

                                # Основной текст с «keyword-based coloring (fallback to base_color)»
                                def _sanitize_kw(token: str) -> str:
                                    import re as _re_kw
                                    return _re_kw.sub(r"[^\wа-яё]+", "", token.lower())

                                use_rgba = text_rgba
                                if tone_color_rgba and kw_set:
                                    token_norm = _sanitize_kw(w_text)
                                    if token_norm in kw_set:
                                        use_rgba = tone_color_rgba

                                _draw_text_with_spacing(
                                    draw_wl,
                                    (pad_left, pad_top),
                                    w_text,
                                    font=font,
                                    fill=use_rgba,
                                    stroke_width=stroke_width,
                                    stroke_fill=stroke_color_rgb,
                                    spacing_px=letter_spacing_px
                                )

                                # Масштабирование слоя
                                if abs(scale - 1.0) > 1e-3:
                                    new_w = max(1, int(round(word_layer.width * scale)))
                                    new_h = max(1, int(round(word_layer.height * scale)))
                                    word_layer = word_layer.resize((new_w, new_h), resample=Image.BICUBIC)

                                # Умножение альфа-канала
                                if alpha_mult < 0.999:
                                    r, g, b, a = word_layer.split()
                                    a = a.point(lambda v, am=alpha_mult: int(v * am))
                                    word_layer = Image.merge("RGBA", (r, g, b, a))

                                # Позиционирование и композитинг (paste с альфа-маской)
                                paste_x = int(round(x_cursor - pad_left * (scale if scale else 1.0)))
                                paste_y = int(round(start_y + translateY - pad_top * (scale if scale else 1.0)))
                                base_rgb.paste(word_layer, (paste_x, paste_y), mask=word_layer)
                                any_drawn_local = True

                                # Сдвиг курсора по X (учитываем пробел между словами)
                                x_cursor += word_w + (space_w if idx < len(word_widths) - 1 else 0)

                            if any_drawn_local:
                                drawn_any_text = True
                                # Apply emojis near text (if enabled)
                                base_rgb = _apply_emojis(base_rgb, start_x, total_text_w, start_y)

                            # Конверсия обратно в OpenCV BGR
                            frame = cv2.cvtColor(np.array(base_rgb.convert("RGB")), cv2.COLOR_RGB2BGR)

                        except Exception as _anim_ex:
                            # Фолбэк: отрисовка без анимации при ошибке
                            overlay = Image.new("RGBA", (width, height), (0, 0, 0, 0))
                            draw_ov = ImageDraw.Draw(overlay)
                            total_text_w = _measure_text_width(tmp_draw, words_to_display, font, spacing_px=letter_spacing_px)
                            start_x = (width - total_text_w) // 2

                            if sx != 0 or sy != 0 or (shadow_rgba and shadow_rgba[3] > 0):
                                _draw_text_with_spacing(
                                    draw_ov,
                                    (start_x + sx, start_y + sy),
                                    words_to_display,
                                    font=font,
                                    fill=shadow_rgba,
                                    stroke_width=0,
                                    stroke_fill=None,
                                    spacing_px=letter_spacing_px
                                )

                            _draw_text_with_spacing(
                                draw_ov,
                                (start_x, start_y),
                                words_to_display,
                                font=font,
                                fill=text_rgba,
                                stroke_width=stroke_width,
                                stroke_fill=stroke_color_rgb,
                                spacing_px=letter_spacing_px
                            )

                            composed = Image.alpha_composite(base_rgb, overlay)
                            # Apply emojis near text (if enabled)
                            composed = _apply_emojis(composed, start_x, total_text_w, start_y)
                            drawn_any_text = True
                            frame = cv2.cvtColor(np.array(composed.convert("RGB")), cv2.COLOR_RGB2BGR)

            # --- Write frame ---
            out.write(frame)
            frame_count += 1
            if frame_count % 100 == 0:
                 print(f"Processed {frame_count}/{total_frames} frames for animation...")

        print("Finished processing frames for animation.")

    except Exception as e:
        print(f"Error during caption animation loop: {e}")
        traceback.print_exc() # Print detailed traceback
        success = False
    finally:
        print("Releasing video resources...")
        if cap and cap.isOpened():
            cap.release()
        if out and out.isOpened():
            out.release()

        # Proceed with muxing only if frames were processed, some text was drawn, and temp file exists
        if drawn_any_text and frame_count > 0 and os.path.exists(temp_animated_video):
             try:
                 print("Muxing audio into animated video...")
                 ffmpeg_mux_command = [
                     'ffmpeg',
                     '-i', temp_animated_video,
                     '-i', audio_source_path,
                     '-map', '0:v:0',
                     '-map', '1:a:0',
                     '-c:v', 'copy',
                     '-c:a', 'aac',
                     '-b:a', '128k',
                     '-shortest',
                     '-y',
                     output_path
                 ]
                 cmd_string = ' '.join([str(arg) for arg in ffmpeg_mux_command])
                 print(f"Mux Command: {cmd_string}")
                 process = subprocess.run(ffmpeg_mux_command, check=True, capture_output=True, text=True, timeout=300)
                 print(f"Successfully created animated caption video: {output_path}")
                 success = True
             except subprocess.TimeoutExpired:
                 print("Error: FFmpeg muxing timed out.")
                 success = False
             except subprocess.CalledProcessError as mux_e:
                  print(f"Error during audio muxing (FFmpeg): {mux_e}")
                  print(f"FFmpeg stdout: {mux_e.stdout}")
                  print(f"FFmpeg stderr: {mux_e.stderr}")
                  success = False
             except Exception as mux_e:
                  print(f"An unexpected error occurred during audio muxing: {mux_e}")
                  success = False
             finally:
                 # Ensure cleanup even if muxing fails
                 if os.path.exists(temp_animated_video):
                     try:
                         os.remove(temp_animated_video)
                         print(f"Removed temporary animated video: {temp_animated_video}")
                     except Exception as e_clean:
                         print(f"Warning: Could not remove temp animated file: {e_clean}")
        elif frame_count > 0 and os.path.exists(temp_animated_video):
             print("No text was drawn on any frame; skipping audio muxing for animated captions.")
             success = False
             # Cleanup temp animated video file
             try:
                 os.remove(temp_animated_video)
                 print(f"Removed temporary animated video: {temp_animated_video}")
             except Exception as e_clean:
                 print(f"Warning: Could not remove temp animated file: {e_clean}")
        elif not os.path.exists(temp_animated_video) and frame_count > 0:
             print(f"Error: Temp animated video file {temp_animated_video} not found, cannot mux audio.")
             success = False
        else: # frame_count == 0 or initial error before loop
             print("Skipping audio muxing due to processing error or no frames processed.")
             success = False # Ensure success is false if animation failed early

    return success
</file>

<file path="Components/config.py">
from dataclasses import dataclass, field
from typing import Optional, Any, Dict
import os
import re
from pathlib import Path

try:
    import yaml  # type: ignore
except Exception:
    yaml = None  # type: ignore


@dataclass
class ProcessingConfig:
    use_animated_captions: bool = True
    shorts_dir: str = "shorts"
    videos_dir: str = "videos"
    transcriptions_dir: str = "transcriptions"
    crop_bottom_percent: float = 0.0
    min_video_dimension_px: int = 100
    log_transcription_preview_len: int = 200


@dataclass
class LLMConfig:
    model_name: str = "gemini-2.5-flash"
    temperature_highlights: float = 0.2
    temperature_metadata: float = 1.0
    max_attempts_highlights: int = 3
    max_attempts_metadata: int = 3
    highlight_min_sec: int = 29
    highlight_max_sec: int = 61
    max_highlights: int = 20


@dataclass
class LoggingConfig:
    # Основные настройки логирования
    log_dir: str = "logs"
    log_level: str = "INFO"
    enable_console_logging: bool = True
    enable_file_logging: bool = True

    # Настройки ротации логов
    log_rotation_max_bytes: int = 10485760  # 10MB
    log_rotation_backup_count: int = 5
    log_rotation_when: str = "midnight"
    log_compression: bool = False

    # Отдельные логгеры для разных типов сообщений
    enable_main_logger: bool = True
    enable_performance_logger: bool = True
    enable_error_logger: bool = True
    enable_debug_logger: bool = False

    # Настройки производительности
    enable_performance_monitoring: bool = True
    performance_log_max_bytes: int = 5242880  # 5MB
    performance_log_backup_count: int = 3
    performance_monitoring_interval: float = 0.5

    # Настройки GPU мониторинга
    enable_gpu_monitoring: bool = True
    gpu_priority_mode: bool = True
    gpu_memory_threshold: float = 0.9
    gpu_temperature_threshold: int = 80

    # Настройки CPU и памяти
    enable_cpu_monitoring: bool = True
    enable_memory_monitoring: bool = True
    memory_threshold: float = 0.85
    cpu_threshold: float = 90.0

    # Настройки прогресс-баров
    enable_progress_bars: bool = True
    progress_bar_update_interval: float = 0.1

    # Системная информация
    enable_system_info_logging: bool = True
    system_info_log_interval: int = 3600

    # Асинхронная обработка
    enable_async_logging: bool = True
    log_queue_size: int = 1000
    log_worker_threads: int = 2

    # Форматирование логов
    log_format: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    log_date_format: str = "%Y-%m-%d %H:%M:%S"
    enable_colors: bool = True

    # Фильтры логирования
    log_filters: list = field(default_factory=lambda: [
        "urllib3.connectionpool",
        "PIL.PngImagePlugin"
    ])

    # Настройки для разных сред
    development_mode: bool = False
    enable_detailed_tracing: bool = False
    enable_function_call_logging: bool = False

    # Ресурсный мониторинг
    resource_monitoring_interval: float = 1.0
    enable_resource_alerts: bool = True
    alert_threshold_duration: float = 30.0


@dataclass
class PathsConfig:
    """
    Путь-ориентированная конфигурация проекта.

    - base_dir: Абсолютный корень проекта/ресурсов.
    - fonts_dir: Каталог со шрифтами (может быть относительным к base_dir или абсолютным).
    """
    base_dir: str = str(Path(".").resolve())
    fonts_dir: str = "fonts"


@dataclass
class ShadowConfig:
    x_px: int = 2
    y_px: int = 2
    blur_px: int = 1
    color: str = "#00000080"


@dataclass
class AccentPalette:
    urgency: str = "#FFD400"
    drama: str = "#FF3B30"
    positive: str = "#34C759"


@dataclass
class AnimateConfig:
    type: str = "slide-up"  # "pop-in" | "slide-up"
    duration_s: float = 0.35  # [0.2, 0.5]
    easing: str = "easeOutCubic"
    per_word_stagger_ms: int = 120  # [0, 500]


@dataclass
class PositionConfig:
    mode: str = "safe_bottom"  # "safe_bottom" | "center"
    bottom_offset_pct: int = 22  # [0, 100]
    center_offset_pct: int = 12
    boundary_padding_px: int = 10


@dataclass
class EmojiConfig:
    enabled: bool = True
    max_per_short: int = 2  # [0, 5]
    style: str = "shiny"  # "shiny" | "pulse" | "none"


@dataclass
class CaptionsConfig:
    font_size_px: int = 38
    letter_spacing_px: float = 1.5
    line_height: float = 1.3
    base_color: str = "#FFFFFF"
    shadow: ShadowConfig = field(default_factory=ShadowConfig)
    accent_palette: AccentPalette = field(default_factory=AccentPalette)
    animate: AnimateConfig = field(default_factory=AnimateConfig)
    position: PositionConfig = field(default_factory=PositionConfig)
    emoji: EmojiConfig = field(default_factory=EmojiConfig)


@dataclass
class AppConfig:
    processing: ProcessingConfig = field(default_factory=ProcessingConfig)
    llm: LLMConfig = field(default_factory=LLMConfig)
    logging: LoggingConfig = field(default_factory=LoggingConfig)
    paths: PathsConfig = field(default_factory=PathsConfig)
    captions: CaptionsConfig = field(default_factory=CaptionsConfig)


def _as_bool(v: Any, default: bool) -> bool:
    try:
        if isinstance(v, bool):
            return v
        if isinstance(v, (int, float)):
            return bool(v)
        if isinstance(v, str):
            s = v.strip().lower()
            if s in ("1", "true", "yes", "y", "on"):
                return True
            if s in ("0", "false", "no", "n", "off"):
                return False
    except Exception:
        pass
    return default


def _as_int(v: Any, default: int) -> int:
    try:
        return int(v)
    except Exception:
        return default


def _as_float(v: Any, default: float) -> float:
    try:
        return float(v)
    except Exception:
        return default


def _as_str(v: Any, default: str) -> str:
    try:
        return str(v)
    except Exception:
        return default


def _clamp(val: float, lo: float, hi: float) -> float:
    try:
        return max(lo, min(hi, float(val)))
    except Exception:
        return lo


def load_config(path: str = "config.yaml") -> AppConfig:
    """
    Загружает конфигурацию приложения из YAML-файла и накладывает её на значения по умолчанию.

    Поведение:
    - Если файл отсутствует или не читается, возвращает значения по умолчанию и печатает:
      "Конфиг не найден. Использую значения по умолчанию."
    - Если файл частично заполнен, недостающие параметры берутся из дефолтов.
    - Выполняется базовая валидация типов и диапазонов (температуры, проценты, интервалы и т.п.).

    Возвращает:
    - Объект AppConfig с заполненными секциями processing, llm и logging.
    """
    defaults = AppConfig()

    if not os.path.exists(path):
        print("Конфиг не найден. Использую значения по умолчанию.")
        return defaults

    data: Dict[str, Any] = {}
    try:
        if yaml is None:
            raise RuntimeError("PyYAML не установлен")
        with open(path, "r", encoding="utf-8") as f:
            loaded = yaml.safe_load(f)  # type: ignore
            if isinstance(loaded, dict):
                data = loaded
    except Exception:
        # В случае любой ошибки парсинга — мягко откатываемся к дефолтам
        return defaults

    p_in = data.get("processing", {}) or {}
    l_in = data.get("llm", {}) or {}
    log_in = data.get("logging", {}) or {}
    paths_in = data.get("paths", {}) or {}
    if not isinstance(p_in, dict):
        p_in = {}
    if not isinstance(l_in, dict):
        l_in = {}
    if not isinstance(log_in, dict):
        log_in = {}
    if not isinstance(paths_in, dict):
        paths_in = {}

    # Processing
    p = ProcessingConfig(
        use_animated_captions=_as_bool(
            p_in.get("use_animated_captions", defaults.processing.use_animated_captions),
            defaults.processing.use_animated_captions,
        ),
        shorts_dir=_as_str(p_in.get("shorts_dir", defaults.processing.shorts_dir), defaults.processing.shorts_dir),
        videos_dir=_as_str(p_in.get("videos_dir", defaults.processing.videos_dir), defaults.processing.videos_dir),
        transcriptions_dir=_as_str(
            p_in.get("transcriptions_dir", defaults.processing.transcriptions_dir),
            defaults.processing.transcriptions_dir
        ),
        crop_bottom_percent=_clamp(
            _as_float(p_in.get("crop_bottom_percent", defaults.processing.crop_bottom_percent),
                      defaults.processing.crop_bottom_percent),
            0.0,
            100.0,
        ),
        min_video_dimension_px=max(
            1,
            _as_int(p_in.get("min_video_dimension_px", defaults.processing.min_video_dimension_px),
                    defaults.processing.min_video_dimension_px),
        ),
        log_transcription_preview_len=max(
            1,
            _as_int(p_in.get("log_transcription_preview_len", defaults.processing.log_transcription_preview_len),
                    defaults.processing.log_transcription_preview_len),
        ),
    )

    # LLM
    t_h = _as_float(l_in.get("temperature_highlights", defaults.llm.temperature_highlights),
                    defaults.llm.temperature_highlights)
    t_m = _as_float(l_in.get("temperature_metadata", defaults.llm.temperature_metadata),
                    defaults.llm.temperature_metadata)
    t_h = _clamp(t_h, 0.0, 2.0)
    t_m = _clamp(t_m, 0.0, 2.0)

    max_att_h = max(1, _as_int(l_in.get("max_attempts_highlights", defaults.llm.max_attempts_highlights),
                               defaults.llm.max_attempts_highlights))
    max_att_m = max(1, _as_int(l_in.get("max_attempts_metadata", defaults.llm.max_attempts_metadata),
                               defaults.llm.max_attempts_metadata))

    h_min = _as_int(l_in.get("highlight_min_sec", defaults.llm.highlight_min_sec),
                    defaults.llm.highlight_min_sec)
    h_max = _as_int(l_in.get("highlight_max_sec", defaults.llm.highlight_max_sec),
                    defaults.llm.highlight_max_sec)
    if h_min < 0:
        h_min = defaults.llm.highlight_min_sec
    if h_max <= h_min:
        h_max = defaults.llm.highlight_max_sec

    max_hls = max(1, _as_int(l_in.get("max_highlights", defaults.llm.max_highlights),
                             defaults.llm.max_highlights))

    l = LLMConfig(
        model_name=_as_str(l_in.get("model_name", defaults.llm.model_name), defaults.llm.model_name),
        temperature_highlights=t_h,
        temperature_metadata=t_m,
        max_attempts_highlights=max_att_h,
        max_attempts_metadata=max_att_m,
        highlight_min_sec=h_min,
        highlight_max_sec=h_max,
        max_highlights=max_hls,
    )

    # Logging
    log = LoggingConfig(
        # Основные настройки логирования
        log_dir=_as_str(log_in.get("log_dir", defaults.logging.log_dir), defaults.logging.log_dir),
        log_level=_as_str(log_in.get("log_level", defaults.logging.log_level), defaults.logging.log_level),
        enable_console_logging=_as_bool(
            log_in.get("enable_console_logging", defaults.logging.enable_console_logging),
            defaults.logging.enable_console_logging,
        ),
        enable_file_logging=_as_bool(
            log_in.get("enable_file_logging", defaults.logging.enable_file_logging),
            defaults.logging.enable_file_logging,
        ),

        # Настройки ротации логов
        log_rotation_max_bytes=max(
            1024,
            _as_int(log_in.get("log_rotation_max_bytes", defaults.logging.log_rotation_max_bytes),
                    defaults.logging.log_rotation_max_bytes),
        ),
        log_rotation_backup_count=max(
            1,
            _as_int(log_in.get("log_rotation_backup_count", defaults.logging.log_rotation_backup_count),
                    defaults.logging.log_rotation_backup_count),
        ),
        log_rotation_when=_as_str(log_in.get("log_rotation_when", defaults.logging.log_rotation_when),
                                  defaults.logging.log_rotation_when),
        log_compression=_as_bool(
            log_in.get("log_compression", defaults.logging.log_compression),
            defaults.logging.log_compression,
        ),

        # Отдельные логгеры для разных типов сообщений
        enable_main_logger=_as_bool(
            log_in.get("enable_main_logger", defaults.logging.enable_main_logger),
            defaults.logging.enable_main_logger,
        ),
        enable_performance_logger=_as_bool(
            log_in.get("enable_performance_logger", defaults.logging.enable_performance_logger),
            defaults.logging.enable_performance_logger,
        ),
        enable_error_logger=_as_bool(
            log_in.get("enable_error_logger", defaults.logging.enable_error_logger),
            defaults.logging.enable_error_logger,
        ),
        enable_debug_logger=_as_bool(
            log_in.get("enable_debug_logger", defaults.logging.enable_debug_logger),
            defaults.logging.enable_debug_logger,
        ),

        # Настройки производительности
        enable_performance_monitoring=_as_bool(
            log_in.get("enable_performance_monitoring", defaults.logging.enable_performance_monitoring),
            defaults.logging.enable_performance_monitoring,
        ),
        performance_log_max_bytes=max(
            1024,
            _as_int(log_in.get("performance_log_max_bytes", defaults.logging.performance_log_max_bytes),
                    defaults.logging.performance_log_max_bytes),
        ),
        performance_log_backup_count=max(
            1,
            _as_int(log_in.get("performance_log_backup_count", defaults.logging.performance_log_backup_count),
                    defaults.logging.performance_log_backup_count),
        ),
        performance_monitoring_interval=max(
            0.1,
            _as_float(log_in.get("performance_monitoring_interval", defaults.logging.performance_monitoring_interval),
                      defaults.logging.performance_monitoring_interval),
        ),

        # Настройки GPU мониторинга
        enable_gpu_monitoring=_as_bool(
            log_in.get("enable_gpu_monitoring", defaults.logging.enable_gpu_monitoring),
            defaults.logging.enable_gpu_monitoring,
        ),
        gpu_priority_mode=_as_bool(
            log_in.get("gpu_priority_mode", defaults.logging.gpu_priority_mode),
            defaults.logging.gpu_priority_mode,
        ),
        gpu_memory_threshold=_clamp(
            _as_float(log_in.get("gpu_memory_threshold", defaults.logging.gpu_memory_threshold),
                      defaults.logging.gpu_memory_threshold),
            0.1, 1.0,
        ),
        gpu_temperature_threshold=max(
            1,
            _as_int(log_in.get("gpu_temperature_threshold", defaults.logging.gpu_temperature_threshold),
                    defaults.logging.gpu_temperature_threshold),
        ),

        # Настройки CPU и памяти
        enable_cpu_monitoring=_as_bool(
            log_in.get("enable_cpu_monitoring", defaults.logging.enable_cpu_monitoring),
            defaults.logging.enable_cpu_monitoring,
        ),
        enable_memory_monitoring=_as_bool(
            log_in.get("enable_memory_monitoring", defaults.logging.enable_memory_monitoring),
            defaults.logging.enable_memory_monitoring,
        ),
        memory_threshold=_clamp(
            _as_float(log_in.get("memory_threshold", defaults.logging.memory_threshold),
                      defaults.logging.memory_threshold),
            0.1, 1.0,
        ),
        cpu_threshold=_clamp(
            _as_float(log_in.get("cpu_threshold", defaults.logging.cpu_threshold),
                      defaults.logging.cpu_threshold),
            1.0, 100.0,
        ),

        # Настройки прогресс-баров
        enable_progress_bars=_as_bool(
            log_in.get("enable_progress_bars", defaults.logging.enable_progress_bars),
            defaults.logging.enable_progress_bars,
        ),
        progress_bar_update_interval=max(
            0.01,
            _as_float(log_in.get("progress_bar_update_interval", defaults.logging.progress_bar_update_interval),
                      defaults.logging.progress_bar_update_interval),
        ),

        # Системная информация
        enable_system_info_logging=_as_bool(
            log_in.get("enable_system_info_logging", defaults.logging.enable_system_info_logging),
            defaults.logging.enable_system_info_logging,
        ),
        system_info_log_interval=max(
            60,
            _as_int(log_in.get("system_info_log_interval", defaults.logging.system_info_log_interval),
                    defaults.logging.system_info_log_interval),
        ),

        # Асинхронная обработка
        enable_async_logging=_as_bool(
            log_in.get("enable_async_logging", defaults.logging.enable_async_logging),
            defaults.logging.enable_async_logging,
        ),
        log_queue_size=max(
            10,
            _as_int(log_in.get("log_queue_size", defaults.logging.log_queue_size),
                    defaults.logging.log_queue_size),
        ),
        log_worker_threads=max(
            1,
            _as_int(log_in.get("log_worker_threads", defaults.logging.log_worker_threads),
                    defaults.logging.log_worker_threads),
        ),

        # Форматирование логов
        log_format=_as_str(log_in.get("log_format", defaults.logging.log_format), defaults.logging.log_format),
        log_date_format=_as_str(log_in.get("log_date_format", defaults.logging.log_date_format),
                                defaults.logging.log_date_format),
        enable_colors=_as_bool(
            log_in.get("enable_colors", defaults.logging.enable_colors),
            defaults.logging.enable_colors,
        ),

        # Фильтры логирования
        log_filters=log_in.get("log_filters", defaults.logging.log_filters) if isinstance(log_in.get("log_filters"), list) else defaults.logging.log_filters,

        # Настройки для разных сред
        development_mode=_as_bool(
            log_in.get("development_mode", defaults.logging.development_mode),
            defaults.logging.development_mode,
        ),
        enable_detailed_tracing=_as_bool(
            log_in.get("enable_detailed_tracing", defaults.logging.enable_detailed_tracing),
            defaults.logging.enable_detailed_tracing,
        ),
        enable_function_call_logging=_as_bool(
            log_in.get("enable_function_call_logging", defaults.logging.enable_function_call_logging),
            defaults.logging.enable_function_call_logging,
        ),

        # Ресурсный мониторинг
        resource_monitoring_interval=max(
            0.1,
            _as_float(log_in.get("resource_monitoring_interval", defaults.logging.resource_monitoring_interval),
                      defaults.logging.resource_monitoring_interval),
        ),
        enable_resource_alerts=_as_bool(
            log_in.get("enable_resource_alerts", defaults.logging.enable_resource_alerts),
            defaults.logging.enable_resource_alerts,
        ),
        alert_threshold_duration=max(
            1.0,
            _as_float(log_in.get("alert_threshold_duration", defaults.logging.alert_threshold_duration),
                      defaults.logging.alert_threshold_duration),
        ),
    )

    # Paths
    base_dir_raw = _as_str(paths_in.get("base_dir", "."), ".")
    fonts_dir_raw = _as_str(paths_in.get("fonts_dir", "fonts"), "fonts")

    try:
        base_abs = Path(base_dir_raw).resolve()
    except Exception:
        base_abs = Path(".").resolve()

    paths = PathsConfig(
        base_dir=str(base_abs),
        fonts_dir=fonts_dir_raw,
    )

    # Captions (with safe defaults and clipping)
    captions_in = data.get("captions", {}) or {}
    if not isinstance(captions_in, dict):
        captions_in = {}

    # Local helpers (scoped to load_config)
    def _is_hex_color(s: str) -> bool:
        if not isinstance(s, str):
            return False
        return bool(re.fullmatch(r"#([0-9A-Fa-f]{6}|[0-9A-Fa-f]{8})", s.strip()))

    def _as_hex_color(v: Any, default: str) -> str:
        s = _as_str(v, default)
        return s if _is_hex_color(s) else default

    def _clamp_int_val(v: Any, default: int, lo: int, hi: int) -> int:
        try:
            iv = int(v)
        except Exception:
            iv = default
        if iv < lo:
            iv = lo
        if iv > hi:
            iv = hi
        return iv

    def _clamp_float_val(v: Any, default: float, lo: float, hi: float) -> float:
        try:
            fv = float(v)
        except Exception:
            fv = default
        return max(lo, min(hi, fv))

    # Top-level caption fields
    fs = _clamp_int_val(
        captions_in.get("font_size_px", defaults.captions.font_size_px),
        defaults.captions.font_size_px, 20, 60
    )
    ls = _clamp_float_val(
        captions_in.get("letter_spacing_px", defaults.captions.letter_spacing_px),
        defaults.captions.letter_spacing_px, 0.0, 10.0
    )
    lh = _clamp_float_val(
        captions_in.get("line_height", defaults.captions.line_height),
        defaults.captions.line_height, 1.0, 2.0
    )
    base_color = _as_hex_color(
        captions_in.get("base_color", defaults.captions.base_color),
        defaults.captions.base_color
    )

    # Shadow
    shadow_in = captions_in.get("shadow", {}) or {}
    if not isinstance(shadow_in, dict):
        shadow_in = {}
    shadow = ShadowConfig(
        x_px=_clamp_int_val(
            shadow_in.get("x_px", defaults.captions.shadow.x_px),
            defaults.captions.shadow.x_px, 0, 20
        ),
        y_px=_clamp_int_val(
            shadow_in.get("y_px", defaults.captions.shadow.y_px),
            defaults.captions.shadow.y_px, 0, 20
        ),
        blur_px=_clamp_int_val(
            shadow_in.get("blur_px", defaults.captions.shadow.blur_px),
            defaults.captions.shadow.blur_px, 0, 16
        ),
        color=_as_hex_color(
            shadow_in.get("color", defaults.captions.shadow.color),
            defaults.captions.shadow.color
        ),
    )

    # Accent palette
    palette_in = captions_in.get("accent_palette", {}) or {}
    if not isinstance(palette_in, dict):
        palette_in = {}
    accent_palette = AccentPalette(
        urgency=_as_hex_color(
            palette_in.get("urgency", defaults.captions.accent_palette.urgency),
            defaults.captions.accent_palette.urgency
        ),
        drama=_as_hex_color(
            palette_in.get("drama", defaults.captions.accent_palette.drama),
            defaults.captions.accent_palette.drama
        ),
        positive=_as_hex_color(
            palette_in.get("positive", defaults.captions.accent_palette.positive),
            defaults.captions.accent_palette.positive
        ),
    )

    # Animate
    animate_in = captions_in.get("animate", {}) or {}
    if not isinstance(animate_in, dict):
        animate_in = {}
    a_type = _as_str(animate_in.get("type", defaults.captions.animate.type), defaults.captions.animate.type)
    if a_type not in ("pop-in", "slide-up"):
        a_type = defaults.captions.animate.type
    duration_s = _clamp_float_val(
        animate_in.get("duration_s", defaults.captions.animate.duration_s),
        defaults.captions.animate.duration_s, 0.2, 0.5
    )
    easing = _as_str(animate_in.get("easing", defaults.captions.animate.easing), defaults.captions.animate.easing)
    per_word_stagger_ms = _clamp_int_val(
        animate_in.get("per_word_stagger_ms", defaults.captions.animate.per_word_stagger_ms),
        defaults.captions.animate.per_word_stagger_ms, 0, 500
    )
    animate = AnimateConfig(
        type=a_type,
        duration_s=duration_s,
        easing=easing,
        per_word_stagger_ms=per_word_stagger_ms,
    )

    # Position
    position_in = captions_in.get("position", {}) or {}
    if not isinstance(position_in, dict):
        position_in = {}
    mode = _as_str(position_in.get("mode", defaults.captions.position.mode), defaults.captions.position.mode)
    if mode not in ("safe_bottom", "center"):
        mode = defaults.captions.position.mode
    bottom_offset_pct = _clamp_int_val(
        position_in.get("bottom_offset_pct", defaults.captions.position.bottom_offset_pct),
        defaults.captions.position.bottom_offset_pct, 0, 100
    )
    center_offset_pct = _clamp_int_val(
        position_in.get("center_offset_pct", defaults.captions.position.center_offset_pct),
        defaults.captions.position.center_offset_pct, -50, 50
    )
    boundary_padding_px = _clamp_int_val(
        position_in.get("boundary_padding_px", defaults.captions.position.boundary_padding_px),
        defaults.captions.position.boundary_padding_px, 0, 100
    )
    position = PositionConfig(
        mode=mode,
        bottom_offset_pct=bottom_offset_pct,
        center_offset_pct=center_offset_pct,
        boundary_padding_px=boundary_padding_px
    )

    # Emoji
    emoji_in = captions_in.get("emoji", {}) or {}
    if not isinstance(emoji_in, dict):
        emoji_in = {}
    enabled = _as_bool(emoji_in.get("enabled", defaults.captions.emoji.enabled), defaults.captions.emoji.enabled)
    max_per_short = _clamp_int_val(
        emoji_in.get("max_per_short", defaults.captions.emoji.max_per_short),
        defaults.captions.emoji.max_per_short, 0, 5
    )
    style = _as_str(emoji_in.get("style", defaults.captions.emoji.style), defaults.captions.emoji.style)
    if style not in ("shiny", "pulse", "none"):
        style = defaults.captions.emoji.style
    emoji = EmojiConfig(enabled=enabled, max_per_short=max_per_short, style=style)

    captions = CaptionsConfig(
        font_size_px=fs,
        letter_spacing_px=ls,
        line_height=lh,
        base_color=base_color,
        shadow=shadow,
        accent_palette=accent_palette,
        animate=animate,
        position=position,
        emoji=emoji,
    )

    return AppConfig(processing=p, llm=l, logging=log, paths=paths, captions=captions)


_CONFIG: Optional[AppConfig] = None


def get_config() -> AppConfig:
    """
    Ленивая кэширующая обёртка над load_config(). Загружает конфигурацию один раз
    из файла (по умолчанию config.yaml), кеширует результат в памяти и возвращает
    один и тот же экземпляр AppConfig при последующих вызовах.
    """
    global _CONFIG
    if _CONFIG is None:
        _CONFIG = load_config()
    return _CONFIG
</file>

<file path="Components/Database.py">
# Components/Database.py
import sqlite3
from typing import Optional, List, Tuple
import json
import os
from datetime import datetime


class VideoDatabase:
    def __init__(self, db_path: str = "video_processing.db"):
        self.db_path = db_path
        self.init_database()

    def init_database(self):
        """Initialize the database with required tables."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # Create videos table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS videos (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    youtube_url TEXT UNIQUE,
                    local_path TEXT,
                    audio_path TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # Create transcriptions table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS transcriptions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    video_id INTEGER,
                    transcription_data TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (video_id) REFERENCES videos (id)
                )
            """)

            # Create highlights table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS highlights (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    video_id INTEGER,
                    start_time FLOAT,
                    end_time FLOAT,
                    output_path TEXT,
                    segment_text TEXT,
                    caption_with_hashtags TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (video_id) REFERENCES videos (id)
                )
            """)

            conn.commit()

    def add_video(
        self, youtube_url: Optional[str], local_path: str, audio_path: str
    ) -> int:
        """Add a new video entry and return its ID."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                INSERT INTO videos (youtube_url, local_path, audio_path)
                VALUES (?, ?, ?)
            """,
                (youtube_url, local_path, audio_path),
            )
            return cursor.lastrowid

    def update_video_audio_path(self, video_id: int, audio_path: str) -> bool:
        """Update the audio path for an existing video."""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    UPDATE videos 
                    SET audio_path = ?
                    WHERE id = ?
                """,
                    (audio_path, video_id),
                )
                return cursor.rowcount > 0
        except Exception as e:
            print(f"Error updating video audio path: {e}")
            return False

    def get_video(
        self, youtube_url: Optional[str] = None, local_path: Optional[str] = None
    ) -> Optional[tuple]:
        """Get video entry by URL or local path."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            if youtube_url:
                cursor.execute(
                    "SELECT * FROM videos WHERE youtube_url = ?", (youtube_url,)
                )
            elif local_path:
                cursor.execute(
                    "SELECT * FROM videos WHERE local_path = ?", (local_path,)
                )
            else:
                return None
            return cursor.fetchone()

    def add_transcription(
        self, video_id: int, transcriptions: List[Tuple[str, float, float]]
    ) -> int:
        """Add transcription data for a video."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            transcription_json = json.dumps(transcriptions)
            cursor.execute(
                """
                INSERT INTO transcriptions (video_id, transcription_data)
                VALUES (?, ?)
            """,
                (video_id, transcription_json),
            )
            return cursor.lastrowid

    def get_transcription(
        self, video_id: int
    ) -> Optional[List[Tuple[str, float, float]]]:
        """Get transcription data for a video."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT transcription_data FROM transcriptions WHERE video_id = ?",
                (video_id,),
            )
            result = cursor.fetchone()
            if result:
                return json.loads(result[0])
            return None

    def add_highlight(
        self, video_id: int, start_time: float, end_time: float, output_path: str,
        segment_text: Optional[str] = None, caption_with_hashtags: Optional[str] = None
    ) -> int:
        """Add a highlight segment for a video with enriched data."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                INSERT INTO highlights (
                    video_id, start_time, end_time, output_path, 
                    segment_text, caption_with_hashtags
                )
                VALUES (?, ?, ?, ?, ?, ?)
            """,
                (video_id, start_time, end_time, output_path, 
                 segment_text, caption_with_hashtags),
            )
            return cursor.lastrowid

    def get_highlights(self, video_id: int) -> List[Tuple[float, float, str, str, str]]:
        """Get all highlights for a video including enriched data."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                SELECT start_time, end_time, output_path, segment_text, caption_with_hashtags
                FROM highlights 
                WHERE video_id = ?
            """,
                (video_id,),
            )
            return cursor.fetchall()

    def video_exists(
        self, youtube_url: Optional[str] = None, local_path: Optional[str] = None
    ) -> bool:
        """Check if a video exists in the database."""
        return self.get_video(youtube_url, local_path) is not None

    def get_cached_processing(
        self, youtube_url: Optional[str] = None, local_path: Optional[str] = None
    ) -> Optional[dict]:
        """Get all cached processing data for a video."""
        video = self.get_video(youtube_url, local_path)
        if not video:
            return None

        video_id = video[0]
        transcription = self.get_transcription(video_id)
        highlights = self.get_highlights(video_id)

        return {
            "video": video,
            "transcription": transcription,
            "highlights": highlights,
        }
</file>

<file path="Components/Edit.py">
from moviepy.video.io.VideoFileClip import VideoFileClip
from moviepy.editor import VideoFileClip
import subprocess
import math
import tempfile
import os
import shlex
import json # For ffprobe output

# Import caption functions from the new module
from .Captions import burn_captions, animate_captions

def extractAudio(video_path):
    try:
        video_clip = VideoFileClip(video_path)
        audio_path = "audio.wav"
        video_clip.audio.write_audiofile(audio_path)
        video_clip.close()
        print(f"Extracted audio to: {audio_path}")
        return audio_path
    except Exception as e:
        print(f"An error occurred while extracting audio: {e}")
        return None


def crop_video(input_file, output_file, start_time, end_time, original_width, original_height):
    """Extracts a video segment using FFmpeg, ensuring original resolution."""
    try:
        duration = end_time - start_time
        if duration <= 0:
            print("Error: End time must be after start time for cropping.")
            return False
            
        ffmpeg_command = [
            'ffmpeg',
            # -ss перед -i: указывает FFmpeg использовать быстрый поиск по ключевым кадрам.
            # Это принципиально для высокой скорости, так как декодируются только необходимые части файла.
            '-ss', str(start_time),
            
            # -i: определяет входной файл.
            '-i', input_file,
            
            # -t: задает длительность сегмента для извлечения.
            '-t', str(duration),
            
            # -map: явно выбирает потоки для включения в выходной файл.
            # 0:v:0 - первый видеопоток из первого входного файла.
            # 0:a:0 - первый аудиопоток из первого входного файла.
            '-map', '0:v:0',
            '-map', '0:a:0',
            
            # -c copy: КЛЮЧЕВОЕ ИЗМЕНЕНИЕ. Эта опция приказывает FFmpeg не перекодировать
            # (decode -> encode), а напрямую копировать данные видео- и аудиопотоков
            # из исходного контейнера в новый. Это операция I/O-bound, быстрая и без потерь качества.
            '-c', 'copy',
            
            # -sn: отключает копирование потоков субтитров, если они присутствуют в исходном файле.
            '-sn',
            
            # -y: перезаписывать выходной файл без интерактивного подтверждения.
            '-y',
            output_file
        ]
        
        print("Running FFmpeg command for segment extraction (crop_video):")
        cmd_string = ' '.join([str(arg) for arg in ffmpeg_command])
        print(f"Command: {cmd_string}")

        process = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)
        print(f"Successfully extracted segment to: {output_file}")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"Error running FFmpeg during segment extraction: {e}")
        print(f"FFmpeg stdout: {e.stdout}")
        print(f"FFmpeg stderr: {e.stderr}")
        return False
    except Exception as e:
        print(f"An error occurred during segment extraction: {e}")
        return False

# Function to format time in SRT format
def format_time(seconds):
    milliseconds = int((seconds - math.floor(seconds)) * 1000)
    seconds = int(math.floor(seconds))
    minutes = seconds // 60
    seconds %= 60
    hours = minutes // 60
    minutes %= 60
    return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"

# Function to format time in ASS format (hours:mm:ss.cc)
def format_time_ass(seconds):
    centiseconds = int((seconds - math.floor(seconds)) * 100)
    seconds = int(math.floor(seconds))
    minutes = seconds // 60
    seconds %= 60
    hours = minutes // 60
    minutes %= 60
    return f"{hours:d}:{minutes:02d}:{seconds:02d}.{centiseconds:02d}"

def crop_bottom_video(input_path, output_path, crop_percentage_bottom):
    """Crops a percentage from the bottom of the video using FFmpeg."""
    try:
        if not 0 < crop_percentage_bottom < 1:
            print("Error: Crop percentage must be between 0 and 1 (exclusive).")
            return False
            
        height_multiplier = 1.0 - crop_percentage_bottom
        
        ffmpeg_command = [
            'ffmpeg',
            '-i', input_path,
            # vf filter: keep original width (iw), calculate new height based on percentage, ensure it's even, crop from top-left (0,0)
            '-vf', f'crop=iw:floor(ih*{height_multiplier}/2)*2:0:0',
            '-c:v', 'libx264', # Re-encode video
            '-preset', 'medium',
            '-crf', '23',
            '-c:a', 'copy',   # Copy existing audio stream
            '-y',           # Overwrite output file
            output_path
        ]

        print("Running FFmpeg command to crop bottom of video:")
        cmd_string = ' '.join([str(arg) for arg in ffmpeg_command])
        print(f"Command: {cmd_string}")

        process = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)
        print(f"Successfully cropped bottom off video to: {output_path}")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"Error running FFmpeg during bottom crop: {e}")
        print(f"FFmpeg stdout: {e.stdout}")
        print(f"FFmpeg stderr: {e.stderr}")
        # Clean up potentially incomplete output file
        if os.path.exists(output_path):
             try: os.remove(output_path) 
             except: pass
        return False
    except Exception as e:
        print(f"An unexpected error occurred during bottom cropping: {e}")
        # Clean up potentially incomplete output file
        if os.path.exists(output_path):
             try: os.remove(output_path) 
             except: pass
        return False

def get_video_dimensions(video_path):
    """Gets the width and height of a video file using ffprobe."""
    try:
        print(f"Checking dimensions for: {video_path}")
        if not os.path.exists(video_path):
            print("  Error: File not found.")
            return None, None
            
        ffprobe_command = [
            'ffprobe',
            '-v', 'error',
            '-select_streams', 'v:0', # Select the first video stream
            '-show_entries', 'stream=width,height',
            '-of', 'json',
            video_path
        ]
        
        result = subprocess.run(ffprobe_command, check=True, capture_output=True, text=True)
        output_json = json.loads(result.stdout)
        
        if output_json and 'streams' in output_json and len(output_json['streams']) > 0:
            width = output_json['streams'][0].get('width')
            height = output_json['streams'][0].get('height')
            if width is not None and height is not None:
                 print(f"  Dimensions found: {width}x{height}")
                 return int(width), int(height)
            else:
                 print("  Error: Could not find width/height in ffprobe stream data.")
                 return None, None
        else:
            print("  Error: No video streams found by ffprobe or invalid JSON output.")
            print(f"  ffprobe output: {output_json}")
            return None, None
            
    except subprocess.CalledProcessError as e:
        print(f"  Error running ffprobe: {e}")
        print(f"  ffprobe stderr: {e.stderr}")
        return None, None
    except json.JSONDecodeError as e:
        print(f"  Error decoding ffprobe JSON output: {e}")
        if 'result' in locals(): print(f"  Raw ffprobe output: {result.stdout}")
        return None, None
    except Exception as e:
        print(f"  An unexpected error occurred getting dimensions: {e}")
        return None, None

# Example usage:
# if __name__ == "__main__":
#    # ... (old example usage)
</file>

<file path="Components/FaceCrop.py">
import cv2
import numpy as np
from moviepy.editor import *
# Note: detect_faces_and_speakers and Frames are no longer used by crop_to_vertical_static
# from Components.Speaker import detect_faces_and_speakers, Frames
global Fps

def crop_to_vertical_static(input_video_path, output_video_path):
    """Crops the video to a 9:16 aspect ratio using a static centered crop."""
    cap = cv2.VideoCapture(input_video_path, cv2.CAP_FFMPEG)
    if not cap.isOpened():
        print("Error: Could not open video.")
        return None # Return None on failure

    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if original_height == 0 or fps == 0:
        print("Error: Video properties (height/fps) are invalid.")
        cap.release()
        return None

    # Calculate target 9:16 width based on original height
    vertical_height = original_height
    vertical_width = int(vertical_height * 9 / 16)

    # Ensure the calculated width is even (required by some codecs)
    if vertical_width % 2 != 0:
        vertical_width -= 1 

    print(f"Original Dims: {original_width}x{original_height} @ {fps:.2f}fps")
    print(f"Target Vertical Dims: {vertical_width}x{vertical_height}")

    if original_width < vertical_width or vertical_width <= 0:
        print("Error: Original video width is less than the calculated vertical width or width is invalid.")
        cap.release()
        return None

    # Calculate static horizontal crop start/end points (centered)
    x_start = (original_width - vertical_width) // 2
    x_end = x_start + vertical_width
    print(f"Static Crop Range (Horizontal): {x_start} to {x_end}")

    # Setup video writer
    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Use 'mp4v' for .mp4 output
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (vertical_width, vertical_height))
    if not out.isOpened():
        print(f"Error: Could not open video writer for {output_video_path}")
        cap.release()
        return None
        
    # Set global Fps (if needed elsewhere, otherwise consider removing global)
    global Fps
    Fps = fps

    processed_frames = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break # End of video

        # Apply the static crop
        cropped_frame = frame[:, x_start:x_end]

        # Basic check in case cropping resulted in unexpected shape
        if cropped_frame.shape[1] != vertical_width or cropped_frame.shape[0] != vertical_height:
             print(f"Warning: Cropped frame shape {cropped_frame.shape} doesn't match target {vertical_width}x{vertical_height}. Adjusting...")
             # Attempt to resize, though this indicates an issue upstream or with calculations
             cropped_frame = cv2.resize(cropped_frame, (vertical_width, vertical_height))

        out.write(cropped_frame)
        processed_frames += 1

    print(f"Processed {processed_frames}/{total_frames} frames.")
    cap.release()
    out.release()
    print(f"Static vertical cropping complete. Video saved to: {output_video_path}")
    return output_video_path # Return path on success


def crop_to_vertical(input_video_path, output_video_path):
    # detect_faces_and_speakers(input_video_path, "DecOut.mp4")
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    cap = cv2.VideoCapture(input_video_path, cv2.CAP_FFMPEG)
    if not cap.isOpened():
        print("Error: Could not open video.")
        return

    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    vertical_height = int(original_height)
    vertical_width = int(vertical_height * 9 / 16)
    print(vertical_height, vertical_width)


    if original_width < vertical_width:
        print("Error: Original video width is less than the desired vertical width.")
        return

    x_start = (original_width - vertical_width) // 2
    x_end = x_start + vertical_width
    print(f"start and end - {x_start} , {x_end}")
    print(x_end-x_start)
    half_width = vertical_width // 2

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (vertical_width, vertical_height))
    global Fps
    Fps = fps
    print(fps)
    count = 0
    for _ in range(total_frames):
        ret, frame = cap.read()
        if not ret:
            print("Error: Could not read frame.")
            break
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
        if len(faces) >-1:
            if len(faces) == 0:
                (x, y, w, h) = Frames[count]

            # (x, y, w, h) = faces[0]  
            try:
                #check if face 1 is active
                (X, Y, W, H) = Frames[count]
            except Exception as e:
                print(e)
                (X, Y, W, H) = Frames[count][0]
                print(Frames[count][0])
            
            for f in faces:
                x1, y1, w1, h1 = f
                center = x1+ w1//2
                if center > X and center < X+W:
                    x = x1
                    y = y1
                    w = w1
                    h = h1
                    break

            # print(faces[0])
            centerX = x+(w//2)
            print(centerX)
            print(x_start - (centerX - half_width))
            if count == 0 or (x_start - (centerX - half_width)) <1 :
                ## IF dif from prev fram is low then no movement is done
                pass #use prev vals
            else:
                x_start = centerX - half_width
                x_end = centerX + half_width


                if int(cropped_frame.shape[1]) != x_end- x_start:
                    if x_end < original_width:
                        x_end += int(cropped_frame.shape[1]) - (x_end-x_start)
                        if x_end > original_width:
                            x_start -= int(cropped_frame.shape[1]) - (x_end-x_start)
                    else:
                        x_start -= int(cropped_frame.shape[1]) - (x_end-x_start)
                        if x_start < 0:
                            x_end += int(cropped_frame.shape[1]) - (x_end-x_start)
                    print("Frame size inconsistant")
                    print(x_end- x_start)

        count += 1
        cropped_frame = frame[:, x_start:x_end]
        if cropped_frame.shape[1] == 0:
            x_start = (original_width - vertical_width) // 2
            x_end = x_start + vertical_width
            cropped_frame = frame[:, x_start:x_end]
        
        print(cropped_frame.shape)

        out.write(cropped_frame)

    cap.release()
    out.release()
    print("Cropping complete. The video has been saved to", output_video_path, count)


# --- New Function: Average Face Centered Crop --- 

def crop_to_vertical_average_face(input_video_path, output_video_path, sample_interval_seconds=0.5):
    """Crops video to 9:16 based on the average horizontal face position sampled periodically."""
    print("Starting average face centered vertical crop...")
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    
    cap = cv2.VideoCapture(input_video_path, cv2.CAP_FFMPEG)
    if not cap.isOpened():
        print(f"Error: Could not open video {input_video_path}")
        return None

    # Get video properties
    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if original_height <= 0 or fps <= 0:
        print("Error: Invalid video properties (height or fps <= 0).")
        cap.release()
        return None
        
    print(f"Input: {original_width}x{original_height} @ {fps:.2f}fps")

    # Calculate target 9:16 width
    vertical_height = original_height
    vertical_width = int(vertical_height * 9 / 16)
    if vertical_width % 2 != 0: vertical_width -= 1

    if original_width < vertical_width or vertical_width <= 0:
        print("Error: Original width too small for vertical crop.")
        cap.release()
        return None

    print(f"Target Vertical Dims: {vertical_width}x{vertical_height}")

    # --- First Pass: Sample face positions --- 
    face_centers_x = []
    frames_to_skip = int(fps * sample_interval_seconds)
    if frames_to_skip < 1: frames_to_skip = 1 # Sample at least every frame if interval is too small
    
    frame_count = 0
    print(f"Sampling face position every {frames_to_skip} frames...")
    while True:
        ret, frame = cap.read()
        if not ret:
            break
            
        if frame_count % frames_to_skip == 0:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            # Adjust detection parameters if needed (e.g., scaleFactor, minNeighbors)
            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60))
            
            if len(faces) > 0:
                # Assume the largest face is the main one if multiple are detected
                faces = sorted(faces, key=lambda f: f[2]*f[3], reverse=True)
                x, y, w, h = faces[0]
                centerX = x + w / 2
                face_centers_x.append(centerX)
                # Optional: Draw box on sample frame for debugging
                # cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                # cv2.imshow('Sample', frame); cv2.waitKey(1)
                
        frame_count += 1
        
    # --- Calculate Average Position --- 
    average_face_center_x = None
    if face_centers_x:
        average_face_center_x = np.mean(face_centers_x)
        print(f"Found {len(face_centers_x)} face samples. Average center X: {average_face_center_x:.2f}")
    else:
        print("Warning: No faces detected during sampling. Falling back to frame center.")
        average_face_center_x = original_width / 2

    # --- Calculate Static Crop Box --- 
    half_vertical_width = vertical_width // 2
    x_start = int(average_face_center_x - half_vertical_width)
    x_end = x_start + vertical_width

    # Clamp crop box to frame boundaries
    x_start = max(0, x_start)
    x_end = min(original_width, x_end)

    # Adjust x_start if clamping x_end changed the width
    if x_end - x_start != vertical_width:
         x_start = x_end - vertical_width
         x_start = max(0, x_start) # Re-clamp x_start just in case
    
    # Final check if width calculation is still correct after clamping
    if x_end - x_start != vertical_width:
        print(f"Error: Could not calculate valid crop window ({x_start}-{x_end}) for width {vertical_width}. Check logic.")
        cap.release()
        return None
        
    print(f"Calculated Static Crop Box: X = {x_start} to {x_end}")

    # --- Second Pass: Apply crop and write video --- 
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0) # Rewind video capture
    
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (vertical_width, vertical_height))
    if not out.isOpened():
        print(f"Error: Could not open video writer for {output_video_path}")
        cap.release()
        return None

    print("Applying static crop and writing output video...")
    written_frames = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
            
        cropped_frame = frame[:, x_start:x_end]
        
        # Sanity check shape before writing (optional but good)
        if cropped_frame.shape[1] != vertical_width or cropped_frame.shape[0] != vertical_height:
            print(f"Warning: Frame {written_frames} cropped shape {cropped_frame.shape} != target {vertical_width}x{vertical_height}. Resizing.")
            cropped_frame = cv2.resize(cropped_frame, (vertical_width, vertical_height))
            
        out.write(cropped_frame)
        written_frames += 1

    print(f"Finished writing {written_frames} frames.")
    cap.release()
    out.release()
    print(f"Average face centered vertical crop complete. Saved to: {output_video_path}")
    return output_video_path
</file>

<file path="Components/LanguageTasks.py">
from google import genai
from typing import TypedDict, List, Optional, Any, Tuple, Dict
import json
import os
import re # Import regex for parsing transcription
import time
from dotenv import load_dotenv
from google.genai import types
from Components.config import get_config

# Optional imports for Google API exceptions (rate limit handling)
try:
    from google.api_core.exceptions import ResourceExhausted as GoogleResourceExhausted  # type: ignore
except Exception:
    GoogleResourceExhausted = None  # type: ignore

try:
    from google.api_core import exceptions as google_exceptions  # type: ignore
except Exception:
    google_exceptions = None  # type: ignore

load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise ValueError(
        "Google API key not found. Make sure it is defined in the .env file."
    )

client = genai.Client(
        api_key=GOOGLE_API_KEY,
    )
# Load configuration (cached)
cfg = get_config()
# Consider using a more capable model if generating descriptions needs more nuance
# model = genai.GenerativeModel("gemini-1.5-flash") # Example alternative
model = cfg.llm.model_name

def build_transcription_prompt(segments: list[dict]) -> str:
    """
    Собирает строку транскрипции для LLM из списка сегментов.

    Вход:
    - segments: список словарей со следующими ключами (минимально необходимые):
        - "start": float
        - "end": float
        - "text": str
        - опционально: "speaker" | "name" | "id" — будет использовано вместо "Speaker", если непусто.

    Формат каждой строки:
    "[{start:.2f}] SpeakerName: {text} [{end:.2f}]"

    Возврат:
    - Одна большая строка с символом новой строки после каждой записи. Побочных эффектов нет.
    """
    lines: list[str] = []
    for seg in (segments or []):
        try:
            if isinstance(seg, dict):
                start = float(seg.get("start", 0.0))
                end = float(seg.get("end", 0.0))
                text = str(seg.get("text", "") or "")
                speaker_val = None
                for key in ("speaker", "name", "id"):
                    v = seg.get(key, None)
                    if v is not None and str(v).strip():
                        speaker_val = str(v).strip()
                        break
            else:
                start = float(getattr(seg, "start", 0.0))
                end = float(getattr(seg, "end", 0.0))
                text = str(getattr(seg, "text", "") or "")
                speaker_val = None
                for key in ("speaker", "name", "id"):
                    if hasattr(seg, key):
                        v = getattr(seg, key)
                        if v is not None and str(v).strip():
                            speaker_val = str(v).strip()
                            break

            speaker_label = speaker_val if speaker_val else "Speaker"
            line = f"[{start:.2f}] {speaker_label}: {text.strip()} [{end:.2f}]"
            lines.append(line)
        except Exception:
            # Любые странности сегмента — пропускаем строку, не прерывая пайплайн
            continue
    return "\n".join(lines) + ("\n" if lines else "")
# --- Rate limit handling utilities and wrapper ---

def parse_retry_delay_seconds(error: Exception | str) -> Optional[int]:
    """
    Пытается извлечь задержку повторной попытки (в секундах) из текста ошибки.
    Поддерживаемые форматы:
    - Retry-After: 28
    - retry-after: 28
    - "retryDelay": "28s"
    - retryDelay: 28s
    Возвращает целое количество секунд или None, если не удалось распарсить.
    """
    text = ""
    try:
        if isinstance(error, Exception):
            parts = [str(error), repr(error)]
            for attr in ("message", "details", "args"):
                val = getattr(error, attr, None)
                if val:
                    parts.append(str(val))
            text = " | ".join(parts)
        else:
            text = str(error)
    except Exception:
        text = str(error)

    patterns = [
        r'(?i)(?:retry[- ]?after|retryDelay)"?:?\s*"?(\d+)\s*s?',  # Retry-After: 28  or retryDelay: "28s"
        r'(?i)"retryDelay"\s*:\s*"?(\d+)\s*s"?'                    # "retryDelay": "28s"
    ]
    for pat in patterns:
        m = re.search(pat, text)
        if m:
            try:
                return int(m.group(1))
            except Exception:
                continue
    return None


def _is_resource_exhausted_error(err: Exception) -> bool:
    """Возвращает True, если ошибка соответствует лимиту API (ResourceExhausted)."""
    try:
        if 'GoogleResourceExhausted' in globals() and GoogleResourceExhausted is not None and isinstance(err, GoogleResourceExhausted):  # type: ignore
            return True
    except Exception:
        pass
    try:
        if 'google_exceptions' in globals() and google_exceptions is not None:
            ResExh = getattr(google_exceptions, "ResourceExhausted", None)
            if ResExh is not None and isinstance(err, ResExh):
                return True
    except Exception:
        pass
    text = f"{type(err).__name__}: {err}"
    return ("ResourceExhausted" in text) or ("RESOURCE_EXHAUSTED" in text) or ("rate limit" in text.lower())


def call_llm_with_retry(
    system_instruction: Optional[str],
    content: List | str,
    generation_config,
    model: Optional[str] = None,
    max_api_attempts: int = 3,
):
    """
    Выполняет вызов client.models.generate_content с централизованной обработкой лимитов API.

    Логирование:
    - При перехвате лимита и наличии retryDelay:
      "Лимит API обработан. Выполняю паузу на X секунд перед попыткой #Y."
    - Если retryDelay извлечь не удалось:
      "Не удалось извлечь retryDelay. Попытки прекращены."

    Стратегия:
    - Повторяет запрос не более max_api_attempts раз, делая паузу X секунд, если retryDelay присутствует.
    - При отсутствии retryDelay немедленно прекращает дальнейшие попытки и пробрасывает исключение.
    - Другие исключения пробрасываются без изменений.
    """
    model_to_use = model or globals().get("model")
    # Нормализуем contents
    if isinstance(content, list):
        contents = content
    else:
        contents = [types.Content(role="user", parts=[types.Part.from_text(text=str(content))])]

    last_err: Optional[Exception] = None

    for api_try in range(1, max_api_attempts + 1):
        try:
            # system_instruction ожидается внутри generation_config; параметр system_instruction оставлен для совместимости.
            return client.models.generate_content(
                model=model_to_use,
                contents=contents,
                config=generation_config,
            )
        except Exception as e:
            last_err = e
            if _is_resource_exhausted_error(e):
                delay = parse_retry_delay_seconds(e)
                if delay is None:
                    print("Не удалось извлечь retryDelay. Попытки прекращены.")
                    raise
                if api_try < max_api_attempts:
                    print(f"Лимит API обработан. Выполняю паузу на {delay} секунд перед попыткой #{api_try+1}.")
                    time.sleep(delay)
                    continue
                # Достигнут лимит попыток API — пробрасываем исключение без дополнительного лога.
                raise
            else:
                raise

    if last_err is not None:
        raise last_err

# Вспомогательная функция: безопасная сборка конфигурации генерации с поддержкой Thinking (если доступно в SDK)
def make_generation_config(system_instruction_text: str, temperature: float = 0.2) -> types.GenerateContentConfig:
    """
    Собирает GenerateContentConfig согласно документации google-genai:
    - system_instruction: строка (может быть Content, но здесь — строка).
    - response_mime_type='application/json' для строгого JSON.
    - thinking_config: types.ThinkingConfig(thinking_budget=-1, include_thoughts=False) для Gemini 2.5.
      Если класс отсутствует или не поддерживается текущей версией SDK — используем конфигурацию без thinking_config.
    """
    base_kwargs = dict(
        temperature=temperature,
        response_mime_type="application/json",
        system_instruction=system_instruction_text,
    )

    # Предпочтительный путь (Python): вложенный thinking_config с корректными snake_case полями.
    ThinkingConfig = getattr(types, "ThinkingConfig", None)
    if ThinkingConfig is not None:
        try:
            return types.GenerateContentConfig(
                **base_kwargs,
                thinking_config=ThinkingConfig(
                    thinking_budget=-1,      # динамическое мышление
                    include_thoughts=False,  # не включать конспекты мыслей в ответ
                ),
            )
        except Exception:
            # Если валидация SDK не пропускает thinking_config, откатываемся на конфиг без мышления.
            print("Предупреждение: ThinkingConfig недоступен/не совместим; продолжаю без thinking.")
            return types.GenerateContentConfig(**base_kwargs)

    # Фолбэк: нет ThinkingConfig в текущем SDK — работаем без мышления.
    return types.GenerateContentConfig(**base_kwargs)


class Message(TypedDict):
    role: str
    content: str


class HighlightSegment(TypedDict):
    start: float
    end: float

# New type for the enriched highlight data
class EnrichedHighlightData(TypedDict):
    start: float
    end: float
    caption_with_hashtags: str
    segment_text: str # Store the text used for generation
    title: Optional[str]
    description: Optional[str]
    hashtags: Optional[List[str]]


def validate_highlight(highlight: HighlightSegment) -> bool:
    """Validate a single highlight segment's time duration and format."""
    try:
        if not all(key in highlight for key in ["start", "end"]):
            print(f"Validation Fail: Missing 'start' or 'end' key in {highlight}")
            return False

        start = float(highlight["start"])
        end = float(highlight["end"])
        duration = end - start

        # Check for valid duration (configured range)
        min_duration = float(cfg.llm.highlight_min_sec)
        max_duration = float(cfg.llm.highlight_max_sec)

        if not (min_duration <= duration <= max_duration):
            print(f"Validation Fail: Duration {duration:.2f}s out of range [~{min_duration:.0f}s, ~{max_duration:.0f}s] for {highlight}")
            return False

        # Check for valid ordering
        if start >= end:
            print(f"Validation Fail: Start time {start} >= end time {end} for {highlight}")
            return False

        return True
    except (ValueError, TypeError) as e:
        print(f"Validation Fail: Invalid type or value in {highlight} - {e}")
        return False


def validate_highlights(highlights: List[HighlightSegment]) -> bool:
    """Validate all highlights and check for overlaps."""
    if not highlights:
        print("Validation: No highlights provided.")
        return False

    # Validate each individual highlight (already checks duration)
    if not all(validate_highlight(h) for h in highlights):
        # Specific errors printed within validate_highlight
        print("Validation: One or more highlights failed individual checks.")
        return False

    # Check for overlapping segments
    sorted_highlights = sorted(highlights, key=lambda x: float(x["start"]))
    for i in range(len(sorted_highlights) - 1):
        if float(sorted_highlights[i]["end"]) > float(
            sorted_highlights[i + 1]["start"]
        ):
            print(f"Validation Fail: Overlap detected between {sorted_highlights[i]} and {sorted_highlights[i+1]}")
            return False

    return True


def extract_highlights(
    transcription: str, max_attempts: int = 3
) -> List[HighlightSegment]:
    """Extracts highlight time segments from transcription, validates, checks overlaps, with retry logic."""
    # System instruction based on Google AI Studio code
    system_instruction_text = f"""
Ты — креатор коротких видео для соцсетей. По предоставленной транскрипции выдели как можно больше непересекающихся отрезков, которые подойдут для увлекательных коротких роликов. Отдавай приоритет разнообразию валидных сегментов.
Верни ТОЛЬКО JSON-массив объектов. Каждый объект обязан содержать ключи "start" и "end" (на английском) с точными временными метками начала и конца сегмента из транскрипта. Никакого текста, объяснений или форматирования вне JSON.

Критерии выбора:
• Ключевые мысли, объяснения, вопросы, выводы и вообще наиболее «цепляющие» места.
• Старайся включать завершённые мысли/предложения.
• Предпочтительны моменты с понятным контекстом, но при необходимости соблюдай ограничение по времени.
• Естественные паузы и переходы — плюс, но не обязательны.

Требования к длительности:
• Длительность каждого сегмента (end - start) СТРОГО ОТ {cfg.llm.highlight_min_sec} ДО {cfg.llm.highlight_max_sec} секунд (включительно).
• Сегменты не должны перекрываться.
• Найди и верни от 10 до {cfg.llm.max_highlights} валидных сегментов — не больше.

Точность таймкодов:
• Используй ИМЕННО те таймкоды, что присутствуют/логически вытекают из транскрипта.
• Нельзя придумывать или править таймкоды.

Пример JSON-вывода:
[
  {{"start": "8.96", "end": "42.20"}},
  {{"start": "115.08", "end": "156.12"}},
  {{"start": "1381.68", "end": "1427.40"}}
]

• Главная цель — найти несколько сегментов со сроком строго {cfg.llm.highlight_min_sec}–{cfg.llm.highlight_max_sec} секунд.
• Убедись, что таймкоды соответствуют фактическим маркерам/границам смысла.
• Сегменты не должны пересекаться.
    """

    # Define generation config based on AI Studio code
    generation_config = make_generation_config(system_instruction_text, temperature=cfg.llm.temperature_highlights)

    effective_attempts = cfg.llm.max_attempts_highlights if max_attempts == 3 else max_attempts
    for attempt in range(effective_attempts):
        print(f"\nПопытка {attempt + 1}: генерация и валидация тайм-сегментов для хайлайтов...")
        try:
            # Structure the prompt and system instruction for generate_content
            user_prompt_text = f"Transcription:\n{transcription}"
            contents = [types.Content(role="user", parts=[types.Part.from_text(text=user_prompt_text)])]
            # Use the global model but with the new config
            response = call_llm_with_retry(
                system_instruction=None,
                content=contents,
                generation_config=generation_config,
                model=model,
            )

            # Basic safety check for response content
            if not response or not response.text:
                 print(f"Неудача на попытке {attempt + 1}: пустой ответ от LLM.")
                 continue

            # Extract JSON from response
            response_text = response.text
            # Handle potential markdown code blocks
            match = re.search(r"```json\s*([\s\S]*?)\s*```", response_text)
            if match:
                json_string = match.group(1).strip()
            else:
                 # Assume the whole text is JSON if no markdown block found
                 json_string = response_text.strip()

            raw_highlights = json.loads(json_string)

            if not isinstance(raw_highlights, list):
                 print(f"Неудача на попытке {attempt + 1}: ответ LLM не является JSON‑массивом.")
                 print(f"Сырой ответ LLM: {response.text}")
                 continue

            # Filter the highlights: Keep only those passing individual validation
            valid_highlights = [h for h in raw_highlights if validate_highlight(h)]

            if not valid_highlights:
                print("No highlights passed individual duration/format validation in this attempt.")
                continue # Try next attempt

            # Check for overlaps ONLY among the valid duration highlights
            # Sort again just to be sure, although validate_highlights also sorts internally for its check
            sorted_highlights = sorted(valid_highlights, key=lambda x: float(x["start"]))
            overlaps_found = False
            for i in range(len(sorted_highlights) - 1):
                if float(sorted_highlights[i]["end"]) > float(sorted_highlights[i + 1]["start"]):
                    print(f"Обнаружено пересечение между {sorted_highlights[i]} и {sorted_highlights[i+1]}")
                    overlaps_found = True
                    break # No need to check further overlaps in this attempt

            if overlaps_found:
                print("Проверка на пересечения провалена для этой попытки.")
                continue # Try next attempt

            # If we reach here, we have a non-empty list of valid, non-overlapping highlights
            print(f"Успех на попытке {attempt + 1}. Найдено валидных сегментов: {len(sorted_highlights)}.")
            # Apply max_highlights cap from config
            try:
                max_h = int(cfg.llm.max_highlights)
                if max_h > 0:
                    return sorted_highlights[:max_h]
            except Exception:
                pass
            return sorted_highlights # Return the validated and sorted list

        except json.JSONDecodeError:
             print(f"Неудача на попытке {attempt + 1}: некорректный JSON от LLM.")
             if 'response_text' in locals(): print(f"Сырой ответ LLM: {response_text}")
             continue
        except Exception as e:
            if _is_resource_exhausted_error(e):
                # Обертка уже залогировала причину; прекращаем дальнейшие попытки этой функции
                break
            print(f"Неудача на попытке {attempt + 1}: непредвиденная ошибка: {str(e)}")
            if 'response_text' in locals():
                print(f"Сырой ответ LLM при ошибке: {response_text}")
            continue

    print("Достигнуто максимальное число попыток извлечения сегментов. Возвращаю пустой список.")
    return []


# --- New Functions ---

def extract_text_for_segment(transcription: str, start_time: float, end_time: float) -> str:
    """Extracts speaker text from transcription within a given time range."""
    segment_text = []
    # Regex to capture timestamp and text, robust to formats like:
    # [0.00] Speaker: Text [8.96]
    # [8.96] Text
    # [12.32] Speaker: Text
    # It captures the start time and the main text content.
    line_pattern = re.compile(r"^\s*\[\s*(\d+\.\d+)\s*\]\s*(.*?)(?:\s*\[\d+\.\d+\s*\])?$")

    lines = transcription.strip().splitlines() # Use splitlines for robustness
    for i, line in enumerate(lines):
        match = line_pattern.match(line)
        if match:
            try:
                timestamp = float(match.group(1))
                text_content = match.group(2).strip()

                # Remove speaker prefix like "Speaker X:" if present
                text_content = re.sub(r"^[Ss]peaker\s*\d*:\s*", "", text_content).strip()

                # Include lines starting within the time range
                if timestamp < end_time and timestamp >= start_time:
                    if text_content: # Avoid adding empty lines
                         segment_text.append(text_content)
            except (ValueError, IndexError):
                # Ignore lines that don't match the expected format
                continue
        # else: Line doesn't match pattern, ignore

    return "\n".join(segment_text)


def generate_description_and_hashtags(segment_text: str, max_attempts: int = 3) -> Optional[str]:
    """Generates a description with appended hashtags for a text segment using LLM."""
    if not segment_text or not segment_text.strip():
        print("Skipping description generation: Empty segment text provided.")
        return None

    system_prompt = """
    Тебе дан текстовый фрагмент короткого видеоролика (обычно 30–60 секунд).
    Твоя задача: вернуть ОДИН JSON-объект со строкой:
    1) Короткое, ёмкое и вовлекающее описание (1–2 предложения) содержимого клипа.
    2) Затем через пробел — 3–5 релевантных хэштегов, слитно, в нижнем регистре, начинаются с #, без пробелов.
    
    Формат ответа (строго, на английском ключе):
    {
        "caption_with_hashtags": "Твоё описание. #пример1 #пример2 #пример3"
    }
    
    Правила:
    - В ответе не должно быть ничего, кроме указанного JSON-объекта.
    - Хэштеги отражают тему клипа, без пробелов, латиницей/кириллицей допустимо.
    
    Пример ввода:
    "Одна из самых интересных областей — обработка видео. Посмотрим, как ИИ может автоматически находить хайлайты."
    
    Пример вывода:
    {
        "caption_with_hashtags": "Как ИИ находит лучшие моменты в видео — кратко и по делу! #ии #видео #хайлайты #машинноевобучение"
    }
    
    Верни ТОЛЬКО JSON-объект.
    """

    # Define generation config based on AI Studio code
    generation_config = make_generation_config(system_prompt, temperature=cfg.llm.temperature_metadata)

    effective_attempts = cfg.llm.max_attempts_metadata if max_attempts == 3 else max_attempts
    for attempt in range(effective_attempts):
        try:
            # Structure the prompt and system instruction for generate_content
            user_prompt_text = f"Segment Text:\n{segment_text}"
            contents = [types.Content(role="user", parts=[types.Part.from_text(text=user_prompt_text)])]
            
            # Use the global model with the new config
            response = call_llm_with_retry(
                system_instruction=None,
                content=contents,
                generation_config=generation_config,
                model=model,
            )

            if not response or not response.text:
                print(f"Неудача на попытке {attempt + 1}: пустой ответ от LLM для описания.")
                continue

            # Extract JSON from response
            response_text = response.text
            match = re.search(r"```json\s*([\s\S]*?)\s*```", response_text)
            if match:
                json_string = match.group(1).strip()
            else:
                json_string = response_text.strip()

            data = json.loads(json_string)

            # Validate the structure and types
            if not isinstance(data, dict) or \
               "caption_with_hashtags" not in data or \
               not isinstance(data["caption_with_hashtags"], str):
                print(f"Неудача на попытке {attempt + 1}: некорректная структура или типы в JSON-ответе.")
                print(f"Сырой ответ LLM: {response_text}")
                continue

            return data["caption_with_hashtags"].strip()

        except json.JSONDecodeError:
            print(f"Неудача на попытке {attempt + 1}: некорректный JSON от LLM для описания.")
            if 'response_text' in locals(): print(f"Сырой ответ LLM: {response_text}")
            continue
        except Exception as e:
            if _is_resource_exhausted_error(e):
                break
            print(f"Неудача на попытке {attempt + 1}: непредвиденная ошибка при генерации описания: {str(e)}")
            if 'response_text' in locals():
                print(f"Сырой ответ LLM при ошибке: {response_text}")
            continue

    print("Достигнуто максимальное число попыток генерации описания/хэштегов. Возвращаю None.")
    return None

# --- Batch metadata generation ---
BATCH_METADATA_SYSTEM_PROMPT = """Ты — эксперт по SMM и продвижению на YouTube, специализирующийся на вирусных Shorts. Тебе на вход подается JSON-массив текстовых фрагментов из видео, каждый с уникальным `id`. Твоя задача — для каждого фрагмента создать оптимальный набор метаданных для максимального вовлечения и охвата.

Правила:
1. Твой ответ должен быть ИСКЛЮЧИТЕЛЬНО одним валидным JSON-массивом. Никакого текста до или после.
2. Для каждого входного объекта с `id` ты должен сгенерировать объект в выходном массиве с тем же `id` и тремя полями: `title`, `description` и `hashtags`.
3. title (заголовок): 40–70 символов, интригующий, задает вопрос или создает предвкушение. Обязательно использовать ключевые слова из текста.
4. description (описание): до 150 символов, кратко раскрывает суть, допускается призыв к действию.
5. hashtags (хэштеги): массив из 3–5 строк; первым ВСЕГДА `#shorts`; остальные — максимально релевантны теме фрагмента.

Пример Входа:
[{"id":"seg_1","text":"Сегодня обсудим, как автоматически находить лучшие моменты в видео..."}]

Пример Выхода:
[{"id":"seg_1","title":"Нейросеть находит лучшие моменты в видео?","description":"Смотрите, как ИИ анализирует ролики для создания шортсов.","hashtags":["#shorts","#ИИ","#нейросети","#видеомонтаж"]}]"""

def _build_retry_prompt(
    validation_tracker: Dict[str, Dict[str, Any]],
    items_to_retry: Optional[List[dict]] = None,
    *,
    max_snippet_len: int = 500
) -> str:
    """
    Строит user prompt для повторной отправки проблемных элементов.

    Параметры:
    - validation_tracker: словарь статусов вида {id: {"status","data","reason","original_item"}}
    - items_to_retry: список исходных элементов {"id","text"} для повтора; если None — берутся со статусами pending/failed
    - max_snippet_len: ограничение длины включаемого текста

    Возвращает:
    - Строку, которую следует передать как пользовательский prompt.
    """
    try:
        if items_to_retry is None:
            items_to_retry = []
            for _id, st in validation_tracker.items():
                if st and st.get("status") in ("pending", "failed"):
                    orig = st.get("original_item") or {}
                    if "id" not in orig:
                        orig = {**orig, "id": _id}
                    items_to_retry.append(orig)
    except Exception:
        items_to_retry = items_to_retry or []

    lines: List[str] = []
    lines.append("Нужно исправить ошибки для указанных id. Верни строго JSON-массив объектов с корректированными данными для этих id.")
    lines.append("")
    lines.append("Требования к каждому объекту ответа:")
    lines.append("• Сохраняй поле id без изменений.")
    lines.append("• Поля: title, description, hashtags.")
    lines.append("• title: 40–70 символов (после тримминга).")
    lines.append("• description: максимум 150 символов.")
    lines.append("• hashtags: массив из 3–5 строк; первый элемент строго '#shorts'; все элементы начинаются с '#'.")
    lines.append("")
    lines.append("Проблемные элементы (id, причина и исходный текст):")
    lines.append("")

    for it in items_to_retry:
        _id = str(it.get("id", ""))
        st = validation_tracker.get(_id, {}) or {}
        reason = st.get("reason") or "Причина не указана — см. требования валидации."
        text = str(it.get("text", "") or "")
        snippet = text.strip()
        if len(snippet) > max_snippet_len:
            snippet = snippet[:max_snippet_len].rstrip() + "..."
        lines.append(f"- id: {_id}")
        lines.append(f"  Причина ошибки: {reason}")
        lines.append("  Исходный текст:")
        lines.append(f'  """{snippet}"""')
        lines.append("")

    lines.append("Верни ТОЛЬКО JSON-массив следующего вида без пояснений:")
    lines.append('[{"id":"<id>","title":"...","description":"...","hashtags":["#shorts","..."]}]')
    return "\n".join(lines)

def generate_metadata_batch(items: list[dict], max_attempts: int = 3) -> list[dict]:
    """
    Пакетная генерация метаданных (title, description, hashtags) для сегментов с таргетированными повторами.

    Параметры:
    - items: список словарей вида {"id": str, "text": str}
    - max_attempts: максимальное число итераций (первая — весь батч, далее — только проблемные)

    Возврат:
    - список объектов {"id": str, "title": str, "description": str, "hashtags": list[str]} в порядке исходных items.

    Валидация (правила НЕ изменены):
    - title: 40–70 символов (после trim)
    - description: длина ≤ 150 символов
    - hashtags: массив длиной 3–5; первый элемент — "#shorts"; все элементы — строки, начинающиеся с "#".
    """
    if not items:
        print("Пакетная генерация метаданных: входных сегментов = 0")
        return []

    print(f"Пакетная генерация метаданных: входных сегментов = {len(items)}")

    # Первая итерация — как и раньше: системный промпт из константы
    generation_config_first = make_generation_config(
        BATCH_METADATA_SYSTEM_PROMPT,
        temperature=cfg.llm.temperature_metadata,
    )

    def _clean_space(s: str) -> str:
        return re.sub(r"\s+", " ", s or "").strip()

    def _extract_json_array(text: str) -> str:
        if not isinstance(text, str):
            return ""
        t = text.strip()
        m = re.search(r"```json\s*([\s\S]*?)\s*```", t)
        if m:
            return m.group(1).strip()
        m = re.search(r"```\s*([\s\S]*?)\s*```", t)
        if m:
            return m.group(1).strip()
        m = re.search(r'"""([\s\S]*?)"""', t)
        if m:
            return m.group(1).strip()
        start = t.find("[")
        end = t.rfind("]")
        if start != -1 and end != -1 and start < end:
            return t[start:end+1].strip()
        return t

    def _validate_item(obj: dict, expected_id: str) -> Tuple[bool, Optional[str], Optional[dict]]:
        """
        Проверяет объект метаданных согласно неизменённым правилам.

        Возвращает:
        - success: bool
        - reason: текст причины ошибки (рус.), если неуспех
        - valid_data: нормализованный объект при успехе
        """
        if not isinstance(obj, dict):
            return False, "Элемент ответа должен быть JSON-объектом (dict).", None
        obj_id = str(obj.get("id", ""))
        if obj_id != expected_id:
            return False, f"Неверный id: ожидалось '{expected_id}', получено '{obj_id}'.", None

        title = _clean_space(str(obj.get("title", "")))
        description = _clean_space(str(obj.get("description", "")))
        hashtags = obj.get("hashtags", [])

        if not (40 <= len(title) <= 70):
            return False, f"Некорректная длина title: {len(title)} символов; требуется 40–70.", None
        if len(description) > 150:
            return False, f"Слишком длинный description: {len(description)} символов; максимум 150.", None
        if not isinstance(hashtags, list):
            return False, "hashtags должен быть списком из 3–5 строк.", None
        if not (3 <= len(hashtags) <= 5):
            return False, f"Некорректное количество хэштегов: {len(hashtags)}; требуется 3–5.", None
        if any(not isinstance(h, str) or not h.startswith("#") for h in hashtags):
            return False, "Все хэштеги должны быть строками и начинаться с '#'.", None
        if len(hashtags) == 0 or hashtags[0] != "#shorts":
            return False, "Первый хэштег должен быть '#shorts'.", None

        return True, None, {
            "id": expected_id,
            "title": title,
            "description": description,
            "hashtags": hashtags,
        }

    # Подготовка входных элементов и трекера валидации
    items_prepared: List[dict] = []
    ordered_ids: List[str] = []
    validation_tracker: Dict[str, Dict[str, Any]] = {}

    for idx, it in enumerate(items):
        safe = dict(it or {})
        id_val = safe.get("id")
        if id_val is None or str(id_val).strip() == "":
            id_val = f"item_{idx+1}"
            safe["id"] = id_val
        id_str = str(id_val)
        if "text" in safe:
            safe["text"] = str(safe["text"])
        items_prepared.append(safe)
        ordered_ids.append(id_str)
        validation_tracker[id_str] = {
            "status": "pending",       # "pending" | "success" | "failed"
            "data": None,              # валидные данные при успехе
            "reason": None,            # последняя причина отказа
            "original_item": safe,     # исходный элемент {"id","text",...}
        }

    N = len(items_prepared)
    effective_attempts = cfg.llm.max_attempts_metadata if max_attempts == 3 else max_attempts

    attempt = 0
    while attempt < effective_attempts:
        attempt += 1

        # Определяем поднабор для текущей попытки
        if attempt == 1:
            to_send = items_prepared
        else:
            to_send = [
                validation_tracker[_id]["original_item"]
                for _id in ordered_ids
                if validation_tracker[_id]["status"] in ("pending", "failed")
            ]

        if not to_send:
            # Все уже успешно провалидированы
            break

        print(f"Попытка {attempt} из {effective_attempts} для batch-метаданных ({len(to_send)} элементов)")
        try:
            if attempt == 1:
                # Первая отправка — как раньше: весь вход и основной модель
                user_payload = json.dumps(to_send, ensure_ascii=False)
                contents = [types.Content(role="user", parts=[types.Part.from_text(text=user_payload)])]
                gen_cfg = generation_config_first
                model_to_use = model
            else:
                # Повторы — только проблемные элементы, со вспомогательным prompt и лёгкой моделью
                retry_prompt = _build_retry_prompt(validation_tracker, to_send)
                contents = [types.Content(role="user", parts=[types.Part.from_text(text=retry_prompt)])]
                # Системный промпт остаётся тем же, чтобы зафиксировать схему ответа
                gen_cfg = make_generation_config(BATCH_METADATA_SYSTEM_PROMPT, temperature=cfg.llm.temperature_metadata)
                model_to_use = "gemini-2.5-flash-lite"

            response = call_llm_with_retry(
                system_instruction=None,  # system_instruction уже в gen_cfg
                content=contents,
                generation_config=gen_cfg,
                model=model_to_use,
            )

            if not response or not getattr(response, "text", None):
                print(f"Неудача на попытке {attempt}: пустой ответ от LLM.")
                continue

            raw_text = response.text
            json_str = _extract_json_array(raw_text)
            data = json.loads(json_str)

            if not isinstance(data, list):
                print(f"Неудача на попытке {attempt}: ответ LLM не является JSON‑массивом.")
                continue

            # Соберём кандидатов по id
            out_by_id: Dict[str, Any] = {}
            for obj in data:
                if isinstance(obj, dict) and "id" in obj:
                    out_by_id[str(obj["id"])] = obj

            validated_this_attempt = 0
            expected_ids = [str(it.get("id")) for it in to_send]

            for expected_id in expected_ids:
                candidate = out_by_id.get(expected_id)
                if candidate is None:
                    validation_tracker[expected_id]["status"] = "failed"
                    validation_tracker[expected_id]["data"] = None
                    validation_tracker[expected_id]["reason"] = f"Модель не вернула объект для id '{expected_id}'."
                    continue

                ok, reason, normalized = _validate_item(candidate, expected_id)
                if ok and normalized:
                    validation_tracker[expected_id]["status"] = "success"
                    validation_tracker[expected_id]["data"] = normalized
                    validation_tracker[expected_id]["reason"] = None
                    validated_this_attempt += 1
                else:
                    validation_tracker[expected_id]["status"] = "failed"
                    validation_tracker[expected_id]["data"] = None
                    validation_tracker[expected_id]["reason"] = reason or "Нарушение правил валидации."

            total_success = sum(1 for st in validation_tracker.values() if st["status"] == "success")
            print(f"Batch-метаданные: успешно сгенерировано за попытку {validated_this_attempt}, всего валидных {total_success} из {N}")

            if total_success == N:
                break

        except json.JSONDecodeError:
            print(f"Неудача на попытке {attempt}: некорректный JSON от LLM для batch-метаданных.")
            if 'raw_text' in locals():
                print(f"Сырой ответ LLM: {raw_text}")
            continue
        except Exception as e:
            if _is_resource_exhausted_error(e):
                # Внутренняя обёртка уже залогировала и управляла паузами; прекращаем дальнейшие итерации
                break
            print(f"Неудача на попытке {attempt}: непредвиденная ошибка при batch-метаданных: {e}")
            if 'raw_text' in locals():
                print(f"Сырой ответ LLM при ошибке: {raw_text}")
            continue

    # Итоговая сборка по исходному порядку; для неуспехов — плейсхолдеры
    if any(st["status"] != "success" for st in validation_tracker.values()):
        print("Batch-метаданные: не все элементы прошли валидацию, для оставшихся будут использованы плейсхолдеры")

    results: List[dict] = []
    for _id in ordered_ids:
        st = validation_tracker[_id]
        if st["status"] == "success" and st["data"]:
            results.append(st["data"])
        else:
            results.append({"id": _id, "title": "", "description": "", "hashtags": ["#shorts"]})

    return results

# --- Updated Main Function ---

def GetHighlights(transcription: str) -> List[EnrichedHighlightData]:
    """
    Main function to get multiple highlight segments from transcription,
    each enriched with LLM-generated metadata (title, description, hashtags).
    Backward-compatible: caption_with_hashtags is preserved.
    """
    enriched_highlights = []
    try:
        # Clean and validate the input transcription
        if not transcription or not transcription.strip():
            print("Ошибка: передана пустая транскрипция.")
            return []

        # 1. Extract highlight time segments
        highlight_segments = extract_highlights(transcription.strip())

        if not highlight_segments:
            print("Не удалось извлечь валидные тайм‑сегменты для хайлайтов.")
            return []

        # 2. Extract text and prepare batch items
        items = []
        mapping = []

        for idx, segment in enumerate(highlight_segments, start=1):
            # Convert string timestamps to floats first
            try:
                start_time = float(segment["start"])
                end_time = float(segment["end"])
            except ValueError:
                print(f"Предупреждение: не удалось преобразовать таймкоды в float для сегмента {segment}. Пропускаю.")
                continue

            # Extract text for this segment
            segment_text = extract_text_for_segment(transcription, start_time, end_time)

            if not segment_text.strip():
                print("Предупреждение: для этого сегмента не извлечён текст. Пропускаю генерацию метаданных.")
                continue

            seg_id = f"seg_{len(items)+1}"
            items.append({"id": seg_id, "text": segment_text})
            mapping.append((seg_id, start_time, end_time, segment_text))

        if not items:
            print("Не удалось подготовить ни одного текстового сегмента для пакетной генерации.")
            return []

        print(f"\nПерехожу к пакетной генерации метаданных для {len(items)} сегментов...")
        batch_meta = generate_metadata_batch(items)
        meta_by_id = {str(m.get("id")): m for m in (batch_meta or []) if isinstance(m, dict)}

        # 3. Assemble enriched highlights
        for seg_id, start_time, end_time, segment_text in mapping:
            meta = meta_by_id.get(seg_id, {})
            title = str(meta.get("title", "") or "").strip()
            description = str(meta.get("description", "") or "").strip()
            hashtags = meta.get("hashtags", None)
            if not isinstance(hashtags, list) or not all(isinstance(h, str) for h in hashtags):
                hashtags = ["#shorts"]

            # Build backward-compatible caption_with_hashtags
            base_caption = ""
            if title and description:
                base_caption = f"{title} — {description}"
            elif title:
                base_caption = title
            elif description:
                base_caption = description
            caption_with_hashtags = base_caption
            if hashtags:
                caption_with_hashtags = f"{base_caption}\n{' '.join(hashtags)}" if base_caption else " ".join(hashtags)

            enriched_data: EnrichedHighlightData = {
                "start": start_time,
                "end": end_time,
                "segment_text": segment_text,
                "caption_with_hashtags": caption_with_hashtags,
                "title": title or None,
                "description": description or None,
                "hashtags": hashtags or None,
            }
            enriched_highlights.append(enriched_data)

        if not enriched_highlights:
            print("Не удалось обогатить ни один хайлайт метаданными.")
            return []

        print(f"\nУспешно обогащено хайлайтов: {len(enriched_highlights)}.")
        return enriched_highlights

    except Exception as e:
        print(f"Ошибка в GetHighlights: {str(e)}")
        import traceback
        traceback.print_exc()
        return []


if __name__ == "__main__":
    example_transcription = """
    [0.0] Speaker 1: Welcome to our discussion about artificial intelligence.
    [15.5] Speaker 1: Today we'll explore the fascinating world of machine learning.
    [30.2] Speaker 2: One of the most exciting applications is in video processing.
    [45.8] Speaker 1: Let's look at how AI can automatically generate video highlights.
    [60.0] Speaker 2: This technology is revolutionizing content creation, making it faster and easier.
    [75.5] Speaker 1: We're seeing it used widely in social media, entertainment, and even education platforms to deliver personalized content.
    [90.2] Speaker 2: The ability for AI to not just cut clips but understand context and find truly engaging moments is key.
    [105.8] Speaker 1: It's changing how we create and consume digital content daily. Think about personalized news feeds.
    [120.0] Speaker 2: Let's dive into some specific examples of tools available now.
    [135.0] Speaker 1: Good idea. Tool number one uses advanced natural language processing...
    """

    final_highlights = GetHighlights(example_transcription)
    if final_highlights:
        print("\n--- Итоговые обогащённые хайлайты ---")
        for i, highlight in enumerate(final_highlights, 1):
            print(f"Хайлайт {i}:")
            print(f"  Время: {highlight['start']:.2f}s - {highlight['end']:.2f}s")
            print(f"  Текст: {highlight['segment_text'][:100]}...") # Фрагмент текста
            print(f"  Подпись: {highlight['caption_with_hashtags']}")
            print("-" * 10)
    else:
        print("\nНе найдено валидных обогащённых хайлайтов.")

# --- Utility: tone/keywords heuristic ---
def compute_tone_and_keywords(text: str) -> dict:
    """
    Простая эвристика для определения «тона» фрагмента и набора ключевых слов.
    Возвращает:
        {
          "tone": "urgency" | "drama" | "positive" | "neutral",
          "keywords": ["слово1", "слово2", ...]  # нижний регистр, без пунктуации
        }
    Правила:
      - Подсчёт вхождений индикаторов по категориям + бонус за '!' для интенсивности.
      - При равенстве/отсутствии индикаторов — tone="neutral".
      - Ключевые слова: 1–4 "жёлуди" (нижний регистр), из найденных индикаторных слов,
        приоритизация по частоте и длине, со стоп-словами.
    """
    try:
        if not text or not str(text).strip():
            return {"tone": "neutral", "keywords": []}

        raw = str(text)
        s = raw.lower()

        # Индикаторы (стеммы/подстроки допустимы)
        indicators = {
            "urgency": {"срочно", "быстро", "скорее", "внимание", "опасн", "атак", "пожар", "беги"},
            "drama": {"плач", "потеря", "больно", "умер", "траг", "слёзы", "страх", "кровь"},
            "positive": {"круто", "супер", "ура", "рад", "люблю", "счаст", "класс", "смешно"},
        }
        emoji_to_tone = {"🔥": "urgency", "❤️": "positive", "😂": "positive", "💀": "drama"}

        # Подсчёт вхождений индикаторов (подстрочные совпадения, чтобы ловить стеммы)
        scores = {"urgency": 0, "drama": 0, "positive": 0}
        for tone, tokens in indicators.items():
            sc = 0
            for tok in tokens:
                try:
                    sc += s.count(tok)  # подстрочные совпадения
                except Exception:
                    continue
            scores[tone] += sc

        # Эмодзи-влияние
        for emo, tone_name in emoji_to_tone.items():
            if emo in raw:
                scores[tone_name] += raw.count(emo)

        # Бонус за '!' как прокси интенсивности -> скорее в сторону urgency
        exclam = raw.count("!")
        if exclam > 0:
            scores["urgency"] += exclam

        # Выбор тона по максимуму; при равенстве или нуле — neutral
        best_tone = "neutral"
        try:
            max_val = max(scores.values()) if scores else 0
            if max_val > 0:
                # если несколько с одинаковым максимумом — neutral
                winners = [k for k, v in scores.items() if v == max_val]
                best_tone = winners[0] if len(winners) == 1 else "neutral"
        except Exception:
            best_tone = "neutral"

        # --- Извлечение ключевых слов ---
        # Токенизация: русские/латинские буквы и цифры, остальное — разделители
        import re
        tokens = re.findall(r"[а-яёa-z0-9]+", s, flags=re.IGNORECASE)
        if not tokens:
            return {"tone": best_tone, "keywords": []}

        stop = {"и", "в", "на", "это", "как", "что", "я", "мы", "они", "он", "она", "а", "но", "или", "да"}
        # Частоты
        freq: dict[str, int] = {}
        for t in tokens:
            if t in stop:
                continue
            freq[t] = freq.get(t, 0) + 1

        # Кандидаты — те токены, которые соприкасаются с любым индикаторным стеммом
        indicator_stems = set().union(*indicators.values())
        candidates = []
        for word, cnt in freq.items():
            if any(stem in word for stem in indicator_stems):
                candidates.append((word, cnt, len(word)))

        # Сортировка по убыванию: частота, длина; затем лексикографически
        candidates.sort(key=lambda x: (-x[1], -x[2], x[0]))

        # Выбрать до 4 уникальных по слову
        kw = []
        seen = set()
        for w, _, _ in candidates:
            if w not in seen:
                seen.add(w)
                kw.append(w)
            if len(kw) >= 4:
                break

        return {"tone": best_tone, "keywords": kw}
    except Exception:
        # На всякий случай — фолбэк: полный бэкомпат
        return {"tone": "neutral", "keywords": []}

# --- Utility: emoji heuristics ---
def compute_emojis_for_segment(text: str, tone: str, max_count: int) -> list[str]:
    """
    emoji: heuristics and placement
    Автономная эвристика выбора эмодзи на основе тона и ключевых слов.

    Правила:
    - Если max_count <= 0 -> [].
    - Карта по тону:
        urgency: ["🔥", "⚠️", "💥"]
        drama: ["💔", "😢", "💀"]
        positive: ["😂", "✨", "🎉", "😊"]
        neutral: []
    - Ключевые слова (рус. стеммы/подстроки):
        смех/юмор -> приоритет "😂" | триггеры: {"смешн","ха","лол","ахаха"}
        экшн/напряжение -> "🔥","⚠️" | триггеры: {"срочно","беги","атак","взрыв","пожар"}
        радость/успех -> "🎉","✨" | триггеры: {"ура","побед","круто","супер","класс"}
    - Сначала кандидаты по ключевым словам (в указанном порядке групп), затем добиваем из карты тона.
    - Удаляем дубликаты, обрезаем до max_count.
    - Никаких внешних вызовов, зависимостей — чистая локальная функция.
    """
    try:
        if max_count is None:
            max_count = 0
        try:
            max_count = int(max_count)
        except Exception:
            max_count = 0

        if max_count <= 0:
            return []

        s = str(text or "")
        s_lower = s.lower()

        tone_map = {
            "urgency": ["🔥", "⚠️", "💥"],
            "drama": ["💔", "😢", "💀"],
            "positive": ["😂", "✨", "🎉", "😊"],
            "neutral": [],
        }

        # Ключевые слова (стеммы/подстроки)
        kw_laughter = {"смешн", "ха", "лол", "ахаха"}
        kw_action   = {"срочно", "беги", "атак", "взрыв", "пожар"}
        kw_joy      = {"ура", "побед", "круто", "супер", "класс"}

        candidates: list[str] = []

        # Смех/юмор — добавляем "😂" один раз, если найдено совпадение
        if any(tok in s_lower for tok in kw_laughter):
            candidates.append("😂")

        # Экшн/напряжение — "🔥" и "⚠️" (в таком порядке)
        if any(tok in s_lower for tok in kw_action):
            candidates.extend(["🔥", "⚠️"])

        # Радость/успех — "🎉" и "✨"
        if any(tok in s_lower for tok in kw_joy):
            candidates.extend(["🎉", "✨"])

        # Добиваем из карты тона
        t = str(tone or "neutral").lower().strip()
        tone_candidates = tone_map.get(t, [])
        candidates.extend(tone_candidates)

        # Уникализация с сохранением порядка
        seen = set()
        result: list[str] = []
        for emo in candidates:
            if emo not in seen:
                seen.add(emo)
                result.append(emo)
            if len(result) >= max_count:
                break

        return result[:max_count]
    except Exception:
        # На всякий случай — "тихий" фолбэк
        return []
</file>

<file path="Components/Logger.py">
"""
Улучшенная система логирования и мониторинга ресурсов для проекта обработки видео.
Включает детальное логирование, мониторинг GPU/CPU, прогресс-бары и ротацию логов.
"""

import time
import logging
import psutil
import GPUtil
from functools import wraps
from typing import Optional, Dict, Any, Callable
from pathlib import Path
import json
from datetime import datetime
import threading
from concurrent.futures import ThreadPoolExecutor
import torch
from dataclasses import dataclass, field
from contextlib import contextmanager
from tqdm import tqdm
import os
from logging.handlers import RotatingFileHandler
import sys


@dataclass
class PerformanceMetrics:
    """Метрики производительности для операций"""
    operation_name: str
    start_time: float
    end_time: Optional[float] = None
    cpu_usage_start: float = 0.0
    cpu_usage_end: float = 0.0
    memory_usage_start: int = 0
    memory_usage_end: int = 0
    gpu_usage_start: Optional[Dict[str, float]] = None
    gpu_usage_end: Optional[Dict[str, float]] = None
    success: bool = True
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def duration(self) -> float:
        """Длительность операции в секундах"""
        if self.end_time is None:
            return time.time() - self.start_time
        return self.end_time - self.start_time

    @property
    def cpu_delta(self) -> float:
        """Изменение CPU использования в процентах"""
        return self.cpu_usage_end - self.cpu_usage_start

    @property
    def memory_delta(self) -> int:
        """Изменение памяти в байтах"""
        return self.memory_usage_end - self.memory_usage_start


class ResourceMonitor:
    """Мониторинг системных ресурсов"""

    def __init__(self):
        self.gpu_available = torch.cuda.is_available()
        self.gpu_count = torch.cuda.device_count() if self.gpu_available else 0

    def get_cpu_usage(self) -> float:
        """Получить текущее использование CPU в процентах"""
        return psutil.cpu_percent(interval=0.1)

    def get_memory_usage(self) -> int:
        """Получить текущее использование памяти в байтах"""
        return psutil.Process().memory_info().rss

    def get_gpu_usage(self) -> Optional[Dict[str, float]]:
        """Получить использование GPU"""
        if not self.gpu_available:
            return None

        try:
            gpus = GPUtil.getGPUs()
            gpu_data = {}
            for i, gpu in enumerate(gpus):
                gpu_data[f'gpu_{i}'] = {
                    'usage': gpu.load * 100,
                    'memory_used': gpu.memoryUsed,
                    'memory_total': gpu.memoryTotal,
                    'temperature': gpu.temperature
                }
            return gpu_data
        except Exception:
            return None

    def get_system_info(self) -> Dict[str, Any]:
        """Получить общую информацию о системе"""
        return {
            'cpu_count': psutil.cpu_count(),
            'cpu_count_logical': psutil.cpu_count(logical=True),
            'memory_total': psutil.virtual_memory().total,
            'gpu_available': self.gpu_available,
            'gpu_count': self.gpu_count,
            'torch_version': torch.__version__,
            'cuda_version': torch.version.cuda if self.gpu_available else None
        }


class AdvancedLogger:
    """Улучшенный логгер с мониторингом ресурсов и таймингами"""

    def __init__(self, name: str = "VideoProcessor", log_dir: str = "logs"):
        self.name = name
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)

        self.monitor = ResourceMonitor()
        self.active_operations: Dict[str, PerformanceMetrics] = {}
        self._lock = threading.Lock()

        # Настройка логгеров
        self._setup_loggers()

        # Thread pool для асинхронных операций
        self.executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="Logger")

        # GPU-first настройки
        self._ensure_gpu_priority()

    def _setup_loggers(self):
        """Настройка системы логирования с ротацией"""
        # Основной логгер
        self.logger = logging.getLogger(f"{self.name}.main")
        self.logger.setLevel(logging.DEBUG)

        # Убираем существующие обработчики
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)

        # Файловый обработчик с ротацией
        log_file = self.log_dir / f"{self.name.lower()}.log"
        file_handler = RotatingFileHandler(
            log_file,
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5,
            encoding='utf-8'
        )
        file_handler.setLevel(logging.DEBUG)

        # Консольный обработчик
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)

        # Форматтеры
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s'
        )

        file_handler.setFormatter(file_formatter)
        console_handler.setFormatter(console_formatter)

        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)

        # Отдельный логгер для производительности
        self.perf_logger = logging.getLogger(f"{self.name}.performance")
        perf_file = self.log_dir / f"{self.name.lower()}_performance.log"
        perf_handler = RotatingFileHandler(
            perf_file,
            maxBytes=5 * 1024 * 1024,  # 5MB
            backupCount=3,
            encoding='utf-8'
        )
        perf_handler.setFormatter(file_formatter)
        self.perf_logger.addHandler(perf_handler)
        self.perf_logger.setLevel(logging.INFO)

        # Отдельный логгер для ошибок
        self.error_logger = logging.getLogger(f"{self.name}.errors")
        error_file = self.log_dir / f"{self.name.lower()}_errors.log"
        error_handler = RotatingFileHandler(
            error_file,
            maxBytes=5 * 1024 * 1024,  # 5MB
            backupCount=3,
            encoding='utf-8'
        )
        error_handler.setFormatter(file_formatter)
        self.error_logger.addHandler(error_handler)
        self.error_logger.setLevel(logging.ERROR)

    def _ensure_gpu_priority(self):
        """Обеспечить GPU-first подход"""
        if self.monitor.gpu_available:
            try:
                # Установить текущий GPU
                torch.cuda.set_device(0)

                # Оптимизации для GPU
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False

                # Очистить кэш GPU
                torch.cuda.empty_cache()

                self.logger.info(f"GPU-first режим активирован. Используется GPU: {torch.cuda.get_device_name(0)}")
            except Exception as e:
                self.logger.warning(f"Не удалось настроить GPU-first режим: {e}")
        else:
            self.logger.info("GPU не доступен, используется CPU режим")

    def start_operation(self, operation_name: str, metadata: Optional[Dict[str, Any]] = None) -> str:
        """Начать отслеживание операции"""
        operation_id = f"{operation_name}_{time.time()}_{threading.get_ident()}"

        with self._lock:
            metrics = PerformanceMetrics(
                operation_name=operation_name,
                start_time=time.time(),
                cpu_usage_start=self.monitor.get_cpu_usage(),
                memory_usage_start=self.monitor.get_memory_usage(),
                gpu_usage_start=self.monitor.get_gpu_usage(),
                metadata=metadata or {}
            )
            self.active_operations[operation_id] = metrics

        self.logger.info(f"🚀 Начата операция: {operation_name} (ID: {operation_id})")
        return operation_id

    def end_operation(self, operation_id: str, success: bool = True, error_message: Optional[str] = None):
        """Завершить отслеживание операции"""
        with self._lock:
            if operation_id not in self.active_operations:
                self.logger.warning(f"Операция {operation_id} не найдена")
                return

            metrics = self.active_operations[operation_id]
            metrics.end_time = time.time()
            metrics.cpu_usage_end = self.monitor.get_cpu_usage()
            metrics.memory_usage_end = self.monitor.get_memory_usage()
            metrics.gpu_usage_end = self.monitor.get_gpu_usage()
            metrics.success = success
            metrics.error_message = error_message

        # Логирование результатов
        self._log_operation_results(metrics)

        # Удаление из активных операций
        with self._lock:
            del self.active_operations[operation_id]

    def _log_operation_results(self, metrics: PerformanceMetrics):
        """Логирование результатов операции"""
        duration = metrics.duration
        cpu_delta = metrics.cpu_delta
        memory_delta = metrics.memory_delta / 1024 / 1024  # MB

        status_icon = "✅" if metrics.success else "❌"
        status_text = "УСПЕХ" if metrics.success else "ОШИБКА"

        # Основное логирование
        self.logger.info(
            ".2f"
            ".1f"
            ".1f"
            f"{status_icon} {status_text}"
        )

        # Детальное логирование производительности
        perf_data = {
            'operation': metrics.operation_name,
            'duration': duration,
            'cpu_usage_start': metrics.cpu_usage_start,
            'cpu_usage_end': metrics.cpu_usage_end,
            'cpu_delta': cpu_delta,
            'memory_start_mb': metrics.memory_usage_start / 1024 / 1024,
            'memory_end_mb': metrics.memory_usage_end / 1024 / 1024,
            'memory_delta_mb': memory_delta,
            'success': metrics.success,
            'timestamp': datetime.now().isoformat(),
            'metadata': metrics.metadata
        }

        if metrics.gpu_usage_start and metrics.gpu_usage_end:
            perf_data['gpu_usage_start'] = metrics.gpu_usage_start
            perf_data['gpu_usage_end'] = metrics.gpu_usage_end

        if not metrics.success and metrics.error_message:
            perf_data['error'] = metrics.error_message

        # Асинхронная запись в файл
        self.executor.submit(self._write_performance_log, perf_data)

        # Логирование ошибок отдельно
        if not metrics.success and metrics.error_message:
            self.error_logger.error(
                f"Ошибка в операции {metrics.operation_name}: {metrics.error_message}"
            )

    def _write_performance_log(self, perf_data: Dict[str, Any]):
        """Асинхронная запись лога производительности"""
        try:
            log_entry = json.dumps(perf_data, ensure_ascii=False, indent=2)
            self.perf_logger.info(f"PERFORMANCE_DATA: {log_entry}")
        except Exception as e:
            self.logger.error(f"Ошибка записи лога производительности: {e}")

    @contextmanager
    def operation_context(self, operation_name: str, metadata: Optional[Dict[str, Any]] = None):
        """Контекстный менеджер для операций"""
        operation_id = self.start_operation(operation_name, metadata)
        try:
            yield operation_id
            self.end_operation(operation_id, success=True)
        except Exception as e:
            self.end_operation(operation_id, success=False, error_message=str(e))
            raise

    def create_progress_bar(self, total: int, desc: str = "", unit: str = "it") -> tqdm:
        """Создать прогресс-бар с мониторингом ресурсов"""
        return tqdm(
            total=total,
            desc=desc,
            unit=unit,
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] '
                      '{postfix}]'
        )

    def log_system_info(self):
        """Логирование информации о системе"""
        system_info = self.monitor.get_system_info()
        self.logger.info("=== ИНФОРМАЦИЯ О СИСТЕМЕ ===")
        for key, value in system_info.items():
            self.logger.info(f"{key}: {value}")
        self.logger.info("=" * 30)

    def get_active_operations_count(self) -> int:
        """Получить количество активных операций"""
        with self._lock:
            return len(self.active_operations)

    def cleanup(self):
        """Очистка ресурсов"""
        self.executor.shutdown(wait=True)
        # Очистка GPU памяти если возможно
        if self.monitor.gpu_available:
            try:
                torch.cuda.empty_cache()
            except Exception:
                pass


# Глобальный экземпляр логгера
logger = AdvancedLogger()


def timed_operation(operation_name: str, metadata: Optional[Dict[str, Any]] = None):
    """Декоратор для отслеживания операций"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            with logger.operation_context(operation_name, metadata):
                return func(*args, **kwargs)
        return wrapper
    return decorator


def log_function_call(func: Callable) -> Callable:
    """Декоратор для логирования вызовов функций"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        func_name = f"{func.__module__}.{func.__name__}" if hasattr(func, '__module__') else func.__name__
        logger.logger.debug(f"Вызов функции: {func_name}")
        start_time = time.time()

        try:
            result = func(*args, **kwargs)
            duration = time.time() - start_time
            logger.logger.debug(".3f")
            return result
        except Exception as e:
            duration = time.time() - start_time
            logger.logger.error(".3f")
            raise
    return wrapper
</file>

<file path="Components/Paths.py">
from typing import Tuple
import os
from pathlib import Path
from Components.config import get_config

def build_short_output_name(base_name: str, idx: int, shorts_dir: str) -> Tuple[str, str]:
    """
    Формирует уникальные пути для итогового short-файла и соответствующего временного файла анимации.

    Итоговый шаблон:
    shorts/{base_name}_highlight_{idx:02d}_final.mp4

    Временный файл для анимации:
    {final_path}_temp_anim.mp4
    """
    file_name = f"{base_name}_highlight_{idx:02d}_final.mp4"
    final_path = os.path.join(shorts_dir, file_name)
    temp_anim_path = f"{final_path}_temp_anim.mp4"
    return final_path, temp_anim_path


def resolve_path(*parts: str) -> str:
    """
    Возвращает абсолютный путь, собранный относительно base_dir из конфига.

    Пример:
      resolve_path("a", "b", "c.txt") -> "<ABS_BASE_DIR>/a/b/c.txt"

    Всегда нормализует и резолвит путь (Path(...).resolve()).
    """
    cfg = get_config()
    base_dir = Path(cfg.paths.base_dir)
    abs_path = (base_dir.joinpath(*[str(p) for p in parts])).resolve()
    return str(abs_path)


def fonts_path(font_filename: str) -> str:
    """
    Возвращает абсолютный путь к шрифту, собранный от base_dir/fonts_dir.

    Пример:
      fonts_path("Montserrat-Bold.ttf") -> "<ABS_BASE_DIR>/<fonts_dir>/Montserrat-Bold.ttf"

    Всегда нормализует и резолвит путь (Path(...).resolve()).
    """
    cfg = get_config()
    base_dir = Path(cfg.paths.base_dir)
    fonts_root = (base_dir / cfg.paths.fonts_dir).resolve()
    return str((fonts_root / font_filename).resolve())
</file>

<file path="Components/Speaker.py">
import cv2
import numpy as np
import webrtcvad
import wave
import contextlib
from pydub import AudioSegment
import os

# Update paths to the model files
prototxt_path = "models/deploy.prototxt"
model_path = "models/res10_300x300_ssd_iter_140000_fp16.caffemodel"
temp_audio_path = "temp_audio.wav"

# Load DNN model
net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)

# Initialize VAD
vad = webrtcvad.Vad(2)  # Aggressiveness mode from 0 to 3

def voice_activity_detection(audio_frame, sample_rate=16000):
    return vad.is_speech(audio_frame, sample_rate)

def extract_audio_from_video(video_path, audio_path):
    audio = AudioSegment.from_file(video_path)
    audio = audio.set_frame_rate(16000).set_channels(1)
    audio.export(audio_path, format="wav")

def process_audio_frame(audio_data, sample_rate=16000, frame_duration_ms=30):
    n = int(sample_rate * frame_duration_ms / 1000) * 2  # 2 bytes per sample
    offset = 0
    while offset + n <= len(audio_data):
        frame = audio_data[offset:offset + n]
        offset += n
        yield frame

global Frames
Frames = [] # [x,y,w,h]

def detect_faces_and_speakers(input_video_path, output_video_path):
    # Return Frams:
    global Frames
    # Extract audio from the video
    extract_audio_from_video(input_video_path, temp_audio_path)

    # Read the extracted audio
    with contextlib.closing(wave.open(temp_audio_path, 'rb')) as wf:
        sample_rate = wf.getframerate()
        audio_data = wf.readframes(wf.getnframes())

    cap = cv2.VideoCapture(input_video_path)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))

    frame_duration_ms = 30  # 30ms frames
    audio_generator = process_audio_frame(audio_data, sample_rate, frame_duration_ms)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        h, w = frame.shape[:2]
        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))
        net.setInput(blob)
        detections = net.forward()

        audio_frame = next(audio_generator, None)
        if audio_frame is None:
            break
        is_speaking_audio = voice_activity_detection(audio_frame, sample_rate)
        MaxDif = 0
        Add = []
        for i in range(detections.shape[2]):
            confidence = detections[0, 0, i, 2]
            if confidence > 0.3:  # Confidence threshold
                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                (x, y, x1, y1) = box.astype("int")
                face_width = x1 - x
                face_height = y1 - y

                # Draw bounding box
                cv2.rectangle(frame, (x, y), (x1, y1), (0, 255, 0), 2)

                # Assuming lips are approximately at the bottom third of the face
                lip_distance = abs((y + 2 * face_height // 3) - (y1))
                Add.append([[x, y, x1, y1], lip_distance])

                MaxDif == max(lip_distance, MaxDif)
        for i in range(detections.shape[2]):
            confidence = detections[0, 0, i, 2]
            if confidence > 0.3:  # Confidence threshold
                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                (x, y, x1, y1) = box.astype("int")
                face_width = x1 - x
                face_height = y1 - y

                # Draw bounding box
                cv2.rectangle(frame, (x, y), (x1, y1), (0, 255, 0), 2)

                # Assuming lips are approximately at the bottom third of the face
                lip_distance = abs((y + 2 * face_height // 3) - (y1))
                print(lip_distance)

                # Combine visual and audio cues
                if lip_distance >= MaxDif and is_speaking_audio:  # Adjust the threshold as needed
                    cv2.putText(frame, "Active Speaker", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                if lip_distance >= MaxDif:
                    break

        Frames.append([x, y, x1, y1])

        out.write(frame)
        cv2.imshow('Frame', frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()
    os.remove(temp_audio_path)



if __name__ == "__main__":
    detect_faces_and_speakers()
    print(Frames)
    print(len(Frames))
    print(Frames[1:5])
</file>

<file path="Components/SpeakerDetection.py">
import cv2
import numpy as np
#Face Detection function
def detect_faces(video_file):
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    # Load the video
    cap = cv2.VideoCapture(video_file)

    faces = []

    # Detect and store unique faces
    while len(faces) < 5:
        ret, frame = cap.read()
        if ret:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            detected_faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

            # Iterate through the detected faces
            for face in detected_faces:
                # Check if the face is already in the list of faces
                if not any(np.array_equal(face, f) for f in faces):
                    faces.append(face)

            # Print the number of unique faces detected so far
            print(f"Number of unique faces detected: {len(faces)}")

    # Release the video capture object
    cap.release()

    # If faces detected, return the list of faces
    if len(faces) > 0:
        return faces
    
def crop_video(faces, input_file, output_file):
    try:
        if len(faces) > 0:
            # Constants for cropping
            CROP_RATIO = 0.9  # Adjust the ratio to control how much of the face is visible in the cropped video
            VERTICAL_RATIO = 9 / 16  # Aspect ratio for the vertical video

            # Read the input video
            cap = cv2.VideoCapture(input_file)

            # Get the frame dimensions
            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

            # Calculate the target width and height for cropping (vertical format)
            target_height = int(frame_height * CROP_RATIO)
            target_width = int(target_height * VERTICAL_RATIO)

            # Create a VideoWriter object to save the output video
            fourcc = cv2.VideoWriter_fourcc(*"mp4v")
            output_video = cv2.VideoWriter(output_file, fourcc, 30.0, (target_width, target_height))

            # Loop through each frame of the input video
            while True:
                ret, frame = cap.read()

                # If no more frames, break out of the loop
                if not ret:
                    break

                # Iterate through each detected face
                for face in faces:
                    # Unpack the face coordinates
                    x, y, w, h = face

                    # Calculate the crop coordinates
                    crop_x = max(0, x + (w - target_width) // 2)  # Adjust the crop region to center the face
                    crop_y = max(0, y + (h - target_height) // 2)
                    crop_x2 = min(crop_x + target_width, frame_width)
                    crop_y2 = min(crop_y + target_height, frame_height)

                    # Crop the frame based on the calculated crop coordinates
                    cropped_frame = frame[crop_y:crop_y2, crop_x:crop_x2]

                    # Resize the cropped frame to the target dimensions
                    resized_frame = cv2.resize(cropped_frame, (target_width, target_height))

                    # Write the resized frame to the output video
                    output_video.write(resized_frame)

            cap.release()
            output_video.release()

            print("Video cropped successfully.")
        else:
            print("No faces detected in the video.")
    except Exception as e:
        print(f"Error during video cropping: {str(e)}")


    return None
if __name__ == "__main__":
    input = r"Short.mp4"
    faces = detect_faces(input)
    print(faces)
    crop_video(faces, input, "Cropped.mp4")
    print("DONE")
</file>

<file path="Components/Transcription.py">
from faster_whisper import WhisperModel
import torch
import os
import psutil
import time
from Components.Logger import logger, timed_operation
import json
from pathlib import Path
from Components.config import get_config


# Старые функции транскрипции (`transcribeAudio`, `transcribe_word_level_full`)
# были удалены и заменены единой функцией `transcribe_unified`.
# Это изменение повышает производительность, избегая повторной загрузки модели
# и двойного прохода по аудиофайлу.

@timed_operation("transcribe_unified")
def transcribe_unified(audio_path, model):
    """
    Выполняет транскрипцию аудио одним вызовом и возвращает две структуры данных:
    сегменты и полную транскрипцию с разбивкой по словам.

    Это единая точка входа для транскрипции, заменяющая двойные вызовы
    transcribeAudio и transcribe_word_level_full.

    Args:
        audio_path (str): Путь к аудиофайлу.
        model (WhisperModel): Загруженная модель faster-whisper.

    Returns:
        tuple[list, dict]: Кортеж, содержащий:
        - list: Список сегментов в формате [[text, start, end], ...].
        - dict: Полная транскрипция на уровне слов в формате
                {'segments': [{'text': str, 'start': float, 'end': float, 'words': [...]}, ...]}.
    """
    logger.logger.info(f"Запуск единой транскрипции аудио: {audio_path}")

    # Логирование начальных системных ресурсов
    _log_system_resources("Начало транскрипции")

    segments_legacy = []
    word_level_transcription = {"segments": []}

    try:
        # Создаем прогресс-бар для транскрипции
        progress_bar = logger.create_progress_bar(
            total=100,  # Процентное представление
            desc="Транскрипция аудио",
            unit="%"
        )

        start_time = time.time()

        # Запуск транскрипции с параметрами
        logger.logger.info("Запуск модели faster-whisper для транскрипции...")
        segments_gen, info = model.transcribe(
            audio=audio_path,
            beam_size=5,
            language="ru",
            condition_on_previous_text=True,
            vad_filter=True,
            word_timestamps=True  # Ключевой параметр для получения слов
        )

        # Получаем информацию о длительности аудио
        audio_duration = getattr(info, 'duration', 0.0)
        logger.logger.info(f"Длительность аудио: {audio_duration:.2f} секунд")

        processed_segments = 0
        total_segments = 0

        # Предварительный подсчет общего количества сегментов для прогресс-бара
        segments_list = list(segments_gen)
        total_segments = len(segments_list)
        segments_gen = iter(segments_list)  # Сбрасываем генератор

        logger.logger.info(f"Обнаружено {total_segments} сегментов для обработки")

        for i, seg in enumerate(segments_gen):
            segment_start = time.time()

            # 1. Формируем структуру для транскрипции на уровне слов
            seg_dict = {
                "start": float(seg.start),
                "end": float(seg.end),
                "text": seg.text
            }
            words_arr = []
            if getattr(seg, "words", None):
                for w in seg.words:
                    words_arr.append({
                        "word": w.word,
                        "start": float(w.start),
                        "end": float(w.end)
                    })
            seg_dict["words"] = words_arr
            word_level_transcription["segments"].append(seg_dict)

            # 2. Формируем "старую" структуру сегментов
            segments_legacy.append([seg.text, float(seg.start), float(seg.end)])

            processed_segments += 1

            # Обновляем прогресс-бар
            progress = int((processed_segments / total_segments) * 100) if total_segments > 0 else 100
            progress_bar.update(max(0, progress - progress_bar.n))
            progress_bar.set_postfix({
                "Сегмент": f"{processed_segments}/{total_segments}",
                "Время": f"{seg.start:.1f}s-{seg.end:.1f}s",
                "Слова": len(words_arr)
            })

            # Логирование обработки сегмента
            segment_time = time.time() - segment_start
            logger.logger.debug(f"Обработан сегмент {processed_segments}/{total_segments}: "
                              f"время={segment_time:.3f}s, слова={len(words_arr)}, "
                              f"текст='{seg.text[:50]}...'")

            # Периодическое логирование системных ресурсов
            if processed_segments % 10 == 0 or processed_segments == total_segments:
                _log_system_resources(f"Обработка сегмента {processed_segments}/{total_segments}")

        progress_bar.close()

        total_time = time.time() - start_time
        logger.logger.info(f"Транскрипция завершена за {total_time:.2f} секунд")
        logger.logger.info(f"Обработано {len(segments_legacy)} сегментов, "
                          f"скорость: {audio_duration/total_time:.2f}x")

        # Финальное логирование ресурсов
        _log_system_resources("Завершение транскрипции")

        # Экспорт артефактов транскрипции на диск
        try:
            cfg = get_config()
            out_dir = getattr(cfg.processing, "transcriptions_dir", "transcriptions")
            base_name = Path(audio_path).stem
            export_transcription_artifacts(base_name, word_level_transcription, out_dir)
        except Exception as ex:
            logger.logger.error(f"Ошибка при сохранении транскрипции на диск: {ex}")
            raise

        return segments_legacy, word_level_transcription

    except Exception as e:
        logger.logger.error(f"Ошибка в единой функции транскрипции: {e}")
        import traceback
        traceback.print_exc()
        # Возвращаем пустые структуры в случае ошибки
        return [], {"segments": []}


def _format_timestamp_srt(seconds: float) -> str:
    try:
        if seconds is None:
            seconds = 0.0
        seconds = float(seconds)
    except Exception:
        seconds = 0.0
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    ms = int(round((seconds - int(seconds)) * 1000))
    if ms == 1000:
        s += 1
        ms = 0
    return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"


def _format_timestamp_vtt(seconds: float) -> str:
    try:
        if seconds is None:
            seconds = 0.0
        seconds = float(seconds)
    except Exception:
        seconds = 0.0
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    ms = int(round((seconds - int(seconds)) * 1000))
    if ms == 1000:
        s += 1
        ms = 0
    return f"{h:02d}:{m:02d}:{s:02d}.{ms:03d}"


def export_transcription_artifacts(base_name: str, word_level_transcription: dict, out_dir: str) -> dict:
    """
    Экспортирует артефакты транскрипции в форматы TXT, JSON, SRT, VTT в указанный каталог.

    Args:
        base_name (str): Базовое имя файла без расширения.
        word_level_transcription (dict): Полная транскрипция с ключом "segments".
        out_dir (str): Каталог для сохранения файлов.

    Returns:
        dict: Словарь путей: {"txt": ..., "json": ..., "srt": ..., "vtt": ...}
    """
    if not isinstance(base_name, str) or not base_name:
        raise ValueError("base_name must be a non-empty string")
    if not isinstance(word_level_transcription, dict):
        raise ValueError("word_level_transcription must be a dict")

    os.makedirs(out_dir, exist_ok=True)

    txt_path = os.path.join(out_dir, f"{base_name}.txt")
    json_path = os.path.join(out_dir, f"{base_name}.json")
    srt_path = os.path.join(out_dir, f"{base_name}.srt")
    vtt_path = os.path.join(out_dir, f"{base_name}.vtt")

    segments = word_level_transcription.get("segments", []) or []

    # TXT: плоский текст
    plain_lines = []
    for seg in segments:
        text = str(seg.get("text", "")).strip()
        if text:
            plain_lines.append(text)
    with open(txt_path, "w", encoding="utf-8") as f_txt:
        f_txt.write("\n".join(plain_lines))
    logger.logger.info(f"Saving transcription to: {txt_path}")

    # JSON: структура транскрипции
    payload = word_level_transcription
    with open(json_path, "w", encoding="utf-8") as f_json:
        json.dump(payload, f_json, ensure_ascii=False, indent=2)
    logger.logger.info(f"Saving transcription to: {json_path}")

    # SRT
    with open(srt_path, "w", encoding="utf-8") as f_srt:
        idx = 1
        for seg in segments:
            text = str(seg.get("text", "")).strip()
            if not text:
                continue
            start = float(seg.get("start", 0.0) or 0.0)
            end = float(seg.get("end", 0.0) or 0.0)
            f_srt.write(f"{idx}\n")
            f_srt.write(f"{_format_timestamp_srt(start)} --> {_format_timestamp_srt(end)}\n")
            f_srt.write(f"{text}\n\n")
            idx += 1
    logger.logger.info(f"Saving transcription to: {srt_path}")

    # VTT
    with open(vtt_path, "w", encoding="utf-8") as f_vtt:
        f_vtt.write("WEBVTT\n\n")
        for seg in segments:
            text = str(seg.get("text", "")).strip()
            if not text:
                continue
            start = float(seg.get("start", 0.0) or 0.0)
            end = float(seg.get("end", 0.0) or 0.0)
            f_vtt.write(f"{_format_timestamp_vtt(start)} --> {_format_timestamp_vtt(end)}\n")
            f_vtt.write(f"{text}\n\n")
    logger.logger.info(f"Saving transcription to: {vtt_path}")

    return {"txt": txt_path, "json": json_path, "srt": srt_path, "vtt": vtt_path}


def _log_system_resources(context: str = ""):
    """
    Логирует текущие системные ресурсы (CPU, память, GPU если доступен).

    Args:
        context (str): Контекст для логирования (например, "Начало транскрипции")
    """
    try:
        # CPU и память
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        memory_used = memory.used / (1024**3)  # GB
        memory_total = memory.total / (1024**3)  # GB

        resource_info = {
            "cpu_percent": f"{cpu_percent:.1f}%",
            "memory_percent": f"{memory_percent:.1f}%",
            "memory_used": f"{memory_used:.2f}GB",
            "memory_total": f"{memory_total:.2f}GB"
        }

        # GPU ресурсы если доступны
        try:
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                resource_info["gpu_count"] = gpu_count

                for i in range(gpu_count):
                    gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)  # GB
                    gpu_allocated = torch.cuda.memory_allocated(i) / (1024**3)  # GB
                    gpu_reserved = torch.cuda.memory_reserved(i) / (1024**3)  # GB

                    resource_info[f"gpu_{i}_memory"] = f"{gpu_allocated:.2f}/{gpu_reserved:.2f}/{gpu_memory:.2f}GB"

                    # Температура GPU если доступна
                    try:
                        import subprocess
                        result = subprocess.run(['nvidia-smi', '--query-gpu=temperature.gpu',
                                               '--format=csv,noheader,nounits'],
                                              capture_output=True, text=True)
                        if result.returncode == 0:
                            temp = result.stdout.strip().split('\n')[i]
                            resource_info[f"gpu_{i}_temp"] = f"{temp}°C"
                    except:
                        pass
        except Exception as e:
            logger.logger.debug(f"Не удалось получить GPU информацию: {e}")

        context_str = f" [{context}]" if context else ""
        logger.logger.info(f"Системные ресурсы{context_str}: {resource_info}")

    except Exception as e:
        logger.logger.warning(f"Ошибка при логировании системных ресурсов: {e}")


# Блок if __name__ == "__main__" был удален, так как он
# использовал устаревшие функции для тестирования.
</file>

<file path="Components/YoutubeDownloader.py">
import os
import subprocess
import glob
import time


def download_youtube_video(url):
    """
    Скачивает видео с YouTube устойчивым способом через yt-dlp (Python API), с автоматическим фолбэком
    на pytubefix/pytube. Гарантируется итоговый MP4 без перекодирования (ffmpeg -c copy).
    Возвращает путь к мастер-файлу MP4 или None при ошибке.
    """
    try:
        out_dir = "videos"
        os.makedirs(out_dir, exist_ok=True)

        def _ffmpeg_merge(v, a, merged_final):
            tmp_out = merged_final + ".tmp"
            print("Автослияние дорожек через ffmpeg (без перекодирования, copy)…")
            merge_cmd = [
                "ffmpeg",
                "-hide_banner",
                "-loglevel", "info",
                "-y",
                "-i", v,
                "-i", a,
                "-map", "0:v:0",
                "-map", "1:a:0",
                "-c:v", "copy",
                "-c:a", "copy",
                "-movflags", "+faststart",
                "-shortest",
                tmp_out,
            ]
            print("Запуск команды слияния:", " ".join(merge_cmd))
            mres = subprocess.run(merge_cmd, capture_output=True, text=True)
            if mres.returncode != 0:
                print("Ошибка: ffmpeg завершился с ненулевым кодом возврата.")
                print(f"Код возврата: {mres.returncode}")
                if mres.stderr:
                    print("stderr ffmpeg:")
                    print(mres.stderr.strip())
                print("Команда:", " ".join(merge_cmd))
                print("Не удалось объединить дорожки. Проверьте ffmpeg в PATH.")
                return None
            try:
                if os.path.exists(merged_final):
                    os.remove(merged_final)
                os.replace(tmp_out, merged_final)
                return merged_final
            except Exception as ren_e:
                print(f"Ошибка при переименовании итогового файла: {ren_e}")
                return None

        def _attempt_local_merge(start_time):
            video_parts = [
                p for p in glob.glob(os.path.join(out_dir, "master-*.mp4"))
                if os.path.getmtime(p) >= (start_time - 1.0)
            ]
            audio_parts = [
                p for p in glob.glob(os.path.join(out_dir, "master-*.m4a"))
                if os.path.getmtime(p) >= (start_time - 1.0)
            ]

            def stem(path):
                base = os.path.basename(path)
                return os.path.splitext(base)[0]  # master-<id>

            video_map = {stem(p): p for p in video_parts}
            audio_map = {stem(p): p for p in audio_parts}
            common = sorted(
                set(video_map.keys()) & set(audio_map.keys()),
                key=lambda s: os.path.getmtime(video_map[s]),
                reverse=True
            )

            if common:
                key = common[0]
                v = video_map[key]
                a = audio_map[key]
                merged_final = os.path.join(out_dir, f"{key}.mp4")
                res = _ffmpeg_merge(v, a, merged_final)
                if res:
                    # Чистим части
                    try:
                        os.remove(v)
                    except Exception:
                        pass
                    try:
                        os.remove(a)
                    except Exception:
                        pass
                    print(f"Загрузка через yt-dlp завершена. Итоговый файл: {res}")
                    return res
            return None

        def _fallback_pytube(src_url):
            try:
                from pytubefix import YouTube
                lib = "pytubefix"
            except Exception:
                try:
                    from pytube import YouTube
                    lib = "pytube"
                except Exception as e_imp:
                    print(f"Фолбэк pytubefix/pytube недоступен: {e_imp}")
                    return None

            try:
                print(f"Фолбэк: {lib} — пытаюсь скачать прогрессивный MP4…")
                # Имитируем поведение WEB‑клиента (для pytubefix) и отключаем OAuth/кэш.
                if lib == "pytubefix":
                    yt = YouTube(src_url, client="WEB", use_oauth=False, allow_oauth_cache=False)
                else:
                    yt = YouTube(src_url, use_oauth=False, allow_oauth_cache=False)
                # Прогрессивный (видео+аудио в одном mp4)
                stream = yt.streams.filter(progressive=True, file_extension="mp4").order_by("resolution").desc().first()
                if stream:
                    filename = f"master-{yt.video_id}.mp4"
                    final_path = stream.download(output_path=out_dir, filename=filename)
                    print(f"Загрузка завершена через {lib}. Итоговый файл: {final_path}")
                    return final_path

                print("Прогрессивный MP4 недоступен. Пытаюсь скачать адаптивные потоки mp4+m4a…")
                vstream = yt.streams.filter(adaptive=True, only_video=True, file_extension="mp4").order_by("resolution").desc().first()
                astream = yt.streams.filter(adaptive=True, only_audio=True, mime_type="audio/mp4").order_by("abr").desc().first()
                if not vstream or not astream:
                    print("Не удалось найти подходящие адаптивные потоки mp4 (видео) и m4a (аудио) для слияния.")
                    return None

                vpath = vstream.download(output_path=out_dir, filename=f"master-{yt.video_id}-video")
                apath = astream.download(output_path=out_dir, filename=f"master-{yt.video_id}-audio")
                merged_final = os.path.join(out_dir, f"master-{yt.video_id}.mp4")
                res = _ffmpeg_merge(vpath, apath, merged_final)
                if res:
                    try:
                        os.remove(vpath)
                    except Exception:
                        pass
                    try:
                        os.remove(apath)
                    except Exception:
                        pass
                    print(f"Загрузка завершена через {lib}. Итоговый файл: {res}")
                    return res
                return None
            except Exception as e_fb:
                print(f"Ошибка фолбэка {lib}: {e_fb}")
                return None

        start_time = time.time()

        # Попытка 1: yt-dlp (Python API)
        try:
            import yt_dlp as ytdlp
        except Exception as imp_err:
            print(f"yt-dlp (Python API) недоступен: {imp_err}")
            print("Перехожу к фолбэку pytubefix/pytube…")
            return _fallback_pytube(url)

        try:
            print("Использую yt-dlp (Python API) для устойчивой загрузки (mp4+m4a, без перекодирования)…")
            format_str = "bv*[ext=mp4][vcodec^=avc1]+ba[ext=m4a]/bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4]/b"
            out_tmpl = os.path.join(out_dir, "master-%(id)s.%(ext)s")
            ydl_opts = {
                "format": format_str,
                "outtmpl": out_tmpl,
                "merge_output_format": "mp4",
                "noplaylist": True,
                "prefer_ffmpeg": True,
                "quiet": True,
                "no_warnings": True,
                "retries": 3,
                # yt-dlp по умолчанию использует -c copy для merge; добавим faststart
                "postprocessor_args": ["-movflags", "+faststart"],
            }
            with ytdlp.YoutubeDL(ydl_opts) as ydl:
                _ = ydl.extract_info(url, download=True)

            # Проверяем итоговый mp4
            mp4s = [
                p for p in glob.glob(os.path.join(out_dir, "master-*.mp4"))
                if os.path.getmtime(p) >= (start_time - 1.0)
            ]
            if mp4s:
                mp4s.sort(key=os.path.getmtime, reverse=True)
                final_path = mp4s[0]
                print(f"Загрузка через yt-dlp завершена. Итоговый файл: {final_path}")
                return final_path

            # Пробуем локальное слияние, если остались отдельные дорожки
            merged = _attempt_local_merge(start_time)
            if merged:
                return merged

            print("yt-dlp не создал финальный MP4 и не найдены пары mp4+m4a для слияния.")
            print("Перехожу к фолбэку pytubefix/pytube…")
            return _fallback_pytube(url)

        except Exception as e_ydl:
            print(f"Ошибка при загрузке через yt-dlp (Python API): {e_ydl}")
            print("Перехожу к фолбэку pytubefix/pytube…")
            return _fallback_pytube(url)

    except Exception as e:
        print(f"Непредвиденная ошибка при скачивании: {e}")
        print("Не удалось скачать видео. Проверьте доступность ffmpeg в PATH.")
        return None


if __name__ == "__main__":
    youtube_url = input("Enter YouTube video URL: ")
    downloaded_file = download_youtube_video(youtube_url)
    if downloaded_file:
        print(f"\nDownload finished. File available at: {downloaded_file}")
    else:
        print("\nDownload failed.")
</file>

<file path="models/deploy.prototxt">
input: "data"
input_shape {
  dim: 1
  dim: 3
  dim: 300
  dim: 300
}

layer {
  name: "data_bn"
  type: "BatchNorm"
  bottom: "data"
  top: "data_bn"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "data_scale"
  type: "Scale"
  bottom: "data_bn"
  top: "data_bn"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1_h"
  type: "Convolution"
  bottom: "data_bn"
  top: "conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 32
    pad: 3
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "msra"
      variance_norm: FAN_OUT
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "conv1_bn_h"
  type: "BatchNorm"
  bottom: "conv1_h"
  top: "conv1_h"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "conv1_scale_h"
  type: "Scale"
  bottom: "conv1_h"
  top: "conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1_relu"
  type: "ReLU"
  bottom: "conv1_h"
  top: "conv1_h"
}
layer {
  name: "conv1_pool"
  type: "Pooling"
  bottom: "conv1_h"
  top: "conv1_pool"
  pooling_param {
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "layer_64_1_conv1_h"
  type: "Convolution"
  bottom: "conv1_pool"
  top: "layer_64_1_conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_bn2_h"
  type: "BatchNorm"
  bottom: "layer_64_1_conv1_h"
  top: "layer_64_1_conv1_h"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "layer_64_1_scale2_h"
  type: "Scale"
  bottom: "layer_64_1_conv1_h"
  top: "layer_64_1_conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_1_relu2"
  type: "ReLU"
  bottom: "layer_64_1_conv1_h"
  top: "layer_64_1_conv1_h"
}
layer {
  name: "layer_64_1_conv2_h"
  type: "Convolution"
  bottom: "layer_64_1_conv1_h"
  top: "layer_64_1_conv2_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_sum"
  type: "Eltwise"
  bottom: "layer_64_1_conv2_h"
  bottom: "conv1_pool"
  top: "layer_64_1_sum"
}
layer {
  name: "layer_128_1_bn1_h"
  type: "BatchNorm"
  bottom: "layer_64_1_sum"
  top: "layer_128_1_bn1_h"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "layer_128_1_scale1_h"
  type: "Scale"
  bottom: "layer_128_1_bn1_h"
  top: "layer_128_1_bn1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_1_relu1"
  type: "ReLU"
  bottom: "layer_128_1_bn1_h"
  top: "layer_128_1_bn1_h"
}
layer {
  name: "layer_128_1_conv1_h"
  type: "Convolution"
  bottom: "layer_128_1_bn1_h"
  top: "layer_128_1_conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_bn2"
  type: "BatchNorm"
  bottom: "layer_128_1_conv1_h"
  top: "layer_128_1_conv1_h"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "layer_128_1_scale2"
  type: "Scale"
  bottom: "layer_128_1_conv1_h"
  top: "layer_128_1_conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_1_relu2"
  type: "ReLU"
  bottom: "layer_128_1_conv1_h"
  top: "layer_128_1_conv1_h"
}
layer {
  name: "layer_128_1_conv2"
  type: "Convolution"
  bottom: "layer_128_1_conv1_h"
  top: "layer_128_1_conv2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_conv_expand_h"
  type: "Convolution"
  bottom: "layer_128_1_bn1_h"
  top: "layer_128_1_conv_expand_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_sum"
  type: "Eltwise"
  bottom: "layer_128_1_conv2"
  bottom: "layer_128_1_conv_expand_h"
  top: "layer_128_1_sum"
}
layer {
  name: "layer_256_1_bn1"
  type: "BatchNorm"
  bottom: "layer_128_1_sum"
  top: "layer_256_1_bn1"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "layer_256_1_scale1"
  type: "Scale"
  bottom: "layer_256_1_bn1"
  top: "layer_256_1_bn1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_1_relu1"
  type: "ReLU"
  bottom: "layer_256_1_bn1"
  top: "layer_256_1_bn1"
}
layer {
  name: "layer_256_1_conv1"
  type: "Convolution"
  bottom: "layer_256_1_bn1"
  top: "layer_256_1_conv1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_bn2"
  type: "BatchNorm"
  bottom: "layer_256_1_conv1"
  top: "layer_256_1_conv1"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "layer_256_1_scale2"
  type: "Scale"
  bottom: "layer_256_1_conv1"
  top: "layer_256_1_conv1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_1_relu2"
  type: "ReLU"
  bottom: "layer_256_1_conv1"
  top: "layer_256_1_conv1"
}
layer {
  name: "layer_256_1_conv2"
  type: "Convolution"
  bottom: "layer_256_1_conv1"
  top: "layer_256_1_conv2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_conv_expand"
  type: "Convolution"
  bottom: "layer_256_1_bn1"
  top: "layer_256_1_conv_expand"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_sum"
  type: "Eltwise"
  bottom: "layer_256_1_conv2"
  bottom: "layer_256_1_conv_expand"
  top: "layer_256_1_sum"
}
layer {
  name: "layer_512_1_bn1"
  type: "BatchNorm"
  bottom: "layer_256_1_sum"
  top: "layer_512_1_bn1"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "layer_512_1_scale1"
  type: "Scale"
  bottom: "layer_512_1_bn1"
  top: "layer_512_1_bn1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_1_relu1"
  type: "ReLU"
  bottom: "layer_512_1_bn1"
  top: "layer_512_1_bn1"
}
layer {
  name: "layer_512_1_conv1_h"
  type: "Convolution"
  bottom: "layer_512_1_bn1"
  top: "layer_512_1_conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1 # 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_bn2_h"
  type: "BatchNorm"
  bottom: "layer_512_1_conv1_h"
  top: "layer_512_1_conv1_h"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "layer_512_1_scale2_h"
  type: "Scale"
  bottom: "layer_512_1_conv1_h"
  top: "layer_512_1_conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_1_relu2"
  type: "ReLU"
  bottom: "layer_512_1_conv1_h"
  top: "layer_512_1_conv1_h"
}
layer {
  name: "layer_512_1_conv2_h"
  type: "Convolution"
  bottom: "layer_512_1_conv1_h"
  top: "layer_512_1_conv2_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 2 # 1
    kernel_size: 3
    stride: 1
    dilation: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_conv_expand_h"
  type: "Convolution"
  bottom: "layer_512_1_bn1"
  top: "layer_512_1_conv_expand_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1 # 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_sum"
  type: "Eltwise"
  bottom: "layer_512_1_conv2_h"
  bottom: "layer_512_1_conv_expand_h"
  top: "layer_512_1_sum"
}
layer {
  name: "last_bn_h"
  type: "BatchNorm"
  bottom: "layer_512_1_sum"
  top: "layer_512_1_sum"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "last_scale_h"
  type: "Scale"
  bottom: "layer_512_1_sum"
  top: "layer_512_1_sum"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "last_relu"
  type: "ReLU"
  bottom: "layer_512_1_sum"
  top: "fc7"
}

layer {
  name: "conv6_1_h"
  type: "Convolution"
  bottom: "fc7"
  top: "conv6_1_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv6_1_relu"
  type: "ReLU"
  bottom: "conv6_1_h"
  top: "conv6_1_h"
}
layer {
  name: "conv6_2_h"
  type: "Convolution"
  bottom: "conv6_1_h"
  top: "conv6_2_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv6_2_relu"
  type: "ReLU"
  bottom: "conv6_2_h"
  top: "conv6_2_h"
}
layer {
  name: "conv7_1_h"
  type: "Convolution"
  bottom: "conv6_2_h"
  top: "conv7_1_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv7_1_relu"
  type: "ReLU"
  bottom: "conv7_1_h"
  top: "conv7_1_h"
}
layer {
  name: "conv7_2_h"
  type: "Convolution"
  bottom: "conv7_1_h"
  top: "conv7_2_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv7_2_relu"
  type: "ReLU"
  bottom: "conv7_2_h"
  top: "conv7_2_h"
}
layer {
  name: "conv8_1_h"
  type: "Convolution"
  bottom: "conv7_2_h"
  top: "conv8_1_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv8_1_relu"
  type: "ReLU"
  bottom: "conv8_1_h"
  top: "conv8_1_h"
}
layer {
  name: "conv8_2_h"
  type: "Convolution"
  bottom: "conv8_1_h"
  top: "conv8_2_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv8_2_relu"
  type: "ReLU"
  bottom: "conv8_2_h"
  top: "conv8_2_h"
}
layer {
  name: "conv9_1_h"
  type: "Convolution"
  bottom: "conv8_2_h"
  top: "conv9_1_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv9_1_relu"
  type: "ReLU"
  bottom: "conv9_1_h"
  top: "conv9_1_h"
}
layer {
  name: "conv9_2_h"
  type: "Convolution"
  bottom: "conv9_1_h"
  top: "conv9_2_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv9_2_relu"
  type: "ReLU"
  bottom: "conv9_2_h"
  top: "conv9_2_h"
}
layer {
  name: "conv4_3_norm"
  type: "Normalize"
  bottom: "layer_256_1_bn1"
  top: "conv4_3_norm"
  norm_param {
    across_spatial: false
    scale_filler {
      type: "constant"
      value: 20
    }
    channel_shared: false
  }
}
layer {
  name: "conv4_3_norm_mbox_loc"
  type: "Convolution"
  bottom: "conv4_3_norm"
  top: "conv4_3_norm_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3_norm_mbox_loc_perm"
  type: "Permute"
  bottom: "conv4_3_norm_mbox_loc"
  top: "conv4_3_norm_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv4_3_norm_mbox_loc_flat"
  type: "Flatten"
  bottom: "conv4_3_norm_mbox_loc_perm"
  top: "conv4_3_norm_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv4_3_norm_mbox_conf"
  type: "Convolution"
  bottom: "conv4_3_norm"
  top: "conv4_3_norm_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8 # 84
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3_norm_mbox_conf_perm"
  type: "Permute"
  bottom: "conv4_3_norm_mbox_conf"
  top: "conv4_3_norm_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv4_3_norm_mbox_conf_flat"
  type: "Flatten"
  bottom: "conv4_3_norm_mbox_conf_perm"
  top: "conv4_3_norm_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv4_3_norm_mbox_priorbox"
  type: "PriorBox"
  bottom: "conv4_3_norm"
  bottom: "data"
  top: "conv4_3_norm_mbox_priorbox"
  prior_box_param {
    min_size: 30.0
    max_size: 60.0
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    step: 8
    offset: 0.5
  }
}
layer {
  name: "fc7_mbox_loc"
  type: "Convolution"
  bottom: "fc7"
  top: "fc7_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc7_mbox_loc_perm"
  type: "Permute"
  bottom: "fc7_mbox_loc"
  top: "fc7_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "fc7_mbox_loc_flat"
  type: "Flatten"
  bottom: "fc7_mbox_loc_perm"
  top: "fc7_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "fc7_mbox_conf"
  type: "Convolution"
  bottom: "fc7"
  top: "fc7_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 12 # 126
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc7_mbox_conf_perm"
  type: "Permute"
  bottom: "fc7_mbox_conf"
  top: "fc7_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "fc7_mbox_conf_flat"
  type: "Flatten"
  bottom: "fc7_mbox_conf_perm"
  top: "fc7_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "fc7_mbox_priorbox"
  type: "PriorBox"
  bottom: "fc7"
  bottom: "data"
  top: "fc7_mbox_priorbox"
  prior_box_param {
    min_size: 60.0
    max_size: 111.0
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    step: 16
    offset: 0.5
  }
}
layer {
  name: "conv6_2_mbox_loc"
  type: "Convolution"
  bottom: "conv6_2_h"
  top: "conv6_2_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv6_2_mbox_loc_perm"
  type: "Permute"
  bottom: "conv6_2_mbox_loc"
  top: "conv6_2_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv6_2_mbox_loc_flat"
  type: "Flatten"
  bottom: "conv6_2_mbox_loc_perm"
  top: "conv6_2_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv6_2_mbox_conf"
  type: "Convolution"
  bottom: "conv6_2_h"
  top: "conv6_2_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 12 # 126
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv6_2_mbox_conf_perm"
  type: "Permute"
  bottom: "conv6_2_mbox_conf"
  top: "conv6_2_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv6_2_mbox_conf_flat"
  type: "Flatten"
  bottom: "conv6_2_mbox_conf_perm"
  top: "conv6_2_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv6_2_mbox_priorbox"
  type: "PriorBox"
  bottom: "conv6_2_h"
  bottom: "data"
  top: "conv6_2_mbox_priorbox"
  prior_box_param {
    min_size: 111.0
    max_size: 162.0
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    step: 32
    offset: 0.5
  }
}
layer {
  name: "conv7_2_mbox_loc"
  type: "Convolution"
  bottom: "conv7_2_h"
  top: "conv7_2_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv7_2_mbox_loc_perm"
  type: "Permute"
  bottom: "conv7_2_mbox_loc"
  top: "conv7_2_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv7_2_mbox_loc_flat"
  type: "Flatten"
  bottom: "conv7_2_mbox_loc_perm"
  top: "conv7_2_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv7_2_mbox_conf"
  type: "Convolution"
  bottom: "conv7_2_h"
  top: "conv7_2_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 12 # 126
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv7_2_mbox_conf_perm"
  type: "Permute"
  bottom: "conv7_2_mbox_conf"
  top: "conv7_2_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv7_2_mbox_conf_flat"
  type: "Flatten"
  bottom: "conv7_2_mbox_conf_perm"
  top: "conv7_2_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv7_2_mbox_priorbox"
  type: "PriorBox"
  bottom: "conv7_2_h"
  bottom: "data"
  top: "conv7_2_mbox_priorbox"
  prior_box_param {
    min_size: 162.0
    max_size: 213.0
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    step: 64
    offset: 0.5
  }
}
layer {
  name: "conv8_2_mbox_loc"
  type: "Convolution"
  bottom: "conv8_2_h"
  top: "conv8_2_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv8_2_mbox_loc_perm"
  type: "Permute"
  bottom: "conv8_2_mbox_loc"
  top: "conv8_2_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv8_2_mbox_loc_flat"
  type: "Flatten"
  bottom: "conv8_2_mbox_loc_perm"
  top: "conv8_2_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv8_2_mbox_conf"
  type: "Convolution"
  bottom: "conv8_2_h"
  top: "conv8_2_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8 # 84
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv8_2_mbox_conf_perm"
  type: "Permute"
  bottom: "conv8_2_mbox_conf"
  top: "conv8_2_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv8_2_mbox_conf_flat"
  type: "Flatten"
  bottom: "conv8_2_mbox_conf_perm"
  top: "conv8_2_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv8_2_mbox_priorbox"
  type: "PriorBox"
  bottom: "conv8_2_h"
  bottom: "data"
  top: "conv8_2_mbox_priorbox"
  prior_box_param {
    min_size: 213.0
    max_size: 264.0
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    step: 100
    offset: 0.5
  }
}
layer {
  name: "conv9_2_mbox_loc"
  type: "Convolution"
  bottom: "conv9_2_h"
  top: "conv9_2_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv9_2_mbox_loc_perm"
  type: "Permute"
  bottom: "conv9_2_mbox_loc"
  top: "conv9_2_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv9_2_mbox_loc_flat"
  type: "Flatten"
  bottom: "conv9_2_mbox_loc_perm"
  top: "conv9_2_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv9_2_mbox_conf"
  type: "Convolution"
  bottom: "conv9_2_h"
  top: "conv9_2_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8 # 84
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv9_2_mbox_conf_perm"
  type: "Permute"
  bottom: "conv9_2_mbox_conf"
  top: "conv9_2_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv9_2_mbox_conf_flat"
  type: "Flatten"
  bottom: "conv9_2_mbox_conf_perm"
  top: "conv9_2_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv9_2_mbox_priorbox"
  type: "PriorBox"
  bottom: "conv9_2_h"
  bottom: "data"
  top: "conv9_2_mbox_priorbox"
  prior_box_param {
    min_size: 264.0
    max_size: 315.0
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    step: 300
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "conv4_3_norm_mbox_loc_flat"
  bottom: "fc7_mbox_loc_flat"
  bottom: "conv6_2_mbox_loc_flat"
  bottom: "conv7_2_mbox_loc_flat"
  bottom: "conv8_2_mbox_loc_flat"
  bottom: "conv9_2_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "conv4_3_norm_mbox_conf_flat"
  bottom: "fc7_mbox_conf_flat"
  bottom: "conv6_2_mbox_conf_flat"
  bottom: "conv7_2_mbox_conf_flat"
  bottom: "conv8_2_mbox_conf_flat"
  bottom: "conv9_2_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "conv4_3_norm_mbox_priorbox"
  bottom: "fc7_mbox_priorbox"
  bottom: "conv6_2_mbox_priorbox"
  bottom: "conv7_2_mbox_priorbox"
  bottom: "conv8_2_mbox_priorbox"
  bottom: "conv9_2_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}

layer {
  name: "mbox_conf_reshape"
  type: "Reshape"
  bottom: "mbox_conf"
  top: "mbox_conf_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: -1
      dim: 2
    }
  }
}
layer {
  name: "mbox_conf_softmax"
  type: "Softmax"
  bottom: "mbox_conf_reshape"
  top: "mbox_conf_softmax"
  softmax_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_flatten"
  type: "Flatten"
  bottom: "mbox_conf_softmax"
  top: "mbox_conf_flatten"
  flatten_param {
    axis: 1
  }
}

layer {
  name: "detection_out"
  type: "DetectionOutput"
  bottom: "mbox_loc"
  bottom: "mbox_conf_flatten"
  bottom: "mbox_priorbox"
  top: "detection_out"
  include {
    phase: TEST
  }
  detection_output_param {
    num_classes: 2
    share_location: true
    background_label_id: 0
    nms_param {
      nms_threshold: 0.45
      top_k: 400
    }
    code_type: CENTER_SIZE
    keep_top_k: 200
    confidence_threshold: 0.01
    clip: 1
  }
}
</file>

<file path=".env.example">
GOOGLE_API_KEY=your_google_api_key_here
</file>

<file path=".gitattributes">
# Auto detect text files and perform LF normalization
* text=auto
</file>

<file path=".gitignore">
# Python-related
*.pyc
**/__pycache__/

# Media Files
*.wav
*.mp3
*.gif
*.mp4
*.webm
*.db
/videos

# Virtual environment
.venv/
.vscode/
env/

# Environment variables
.env
</file>

<file path="clean_url.py">
# clean_url.py
import sqlite3
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
import os

db_path = "video_processing.db"
target_id = 1

if not os.path.exists(db_path):
    print(f"Error: Database file '{db_path}' not found.")
    exit()

try:
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Fetch the current URL
    cursor.execute("SELECT youtube_url FROM videos WHERE id = ?", (target_id,))
    result = cursor.fetchone()

    if result:
        original_url = result[0]
        print(f"Original URL for ID {target_id}: {original_url}")

        if original_url:
            # Simple string manipulation for this specific case
            cleaned_url = original_url
            if '&t' in original_url:
                # Find the position of &t
                t_pos = original_url.find('&t')
                # Check if it's followed by '=' or just the end of the string
                if t_pos != -1:
                    # Check if it's the last parameter or followed by another &
                    next_amp_pos = original_url.find('&', t_pos + 1)
                    if next_amp_pos == -1:
                        # &t is the last part, remove it
                        cleaned_url = original_url[:t_pos]
                        print("Removed trailing '&t' parameter.")
                    else:
                        # &t=... is in the middle, need proper parsing (or more complex string logic)
                        # Fall back to original parsing method just in case
                        print("'&t' found, but potentially with a value or followed by other params. Using URL parsing...")
                        parsed_url = urlparse(original_url)
                        query_params = parse_qs(parsed_url.query)
                        if 't' in query_params:
                            del query_params['t']
                            new_query_string = urlencode(query_params, doseq=True)
                            cleaned_url_parts = parsed_url._replace(query=new_query_string)
                            cleaned_url = urlunparse(cleaned_url_parts)
                            print("Removed 't' parameter using URL parsing.")
                        else:
                            # If parsing didn't find t=, maybe it was just &t. Try removing the segment again.
                            cleaned_url = original_url[:t_pos] + original_url[next_amp_pos:]
                            print("Removed '&t' segment between other params.")
                else: # &t not found by find()
                     print("'&t' not found in the URL string. No update needed.")       

            if cleaned_url != original_url:
                print(f"Cleaned URL: {cleaned_url}")
                cursor.execute("UPDATE videos SET youtube_url = ? WHERE id = ?", (cleaned_url, target_id))
                conn.commit()
                print(f"Database updated successfully for ID {target_id}.")
            else:
                print("URL was not changed. No update needed.")
        else:
             print("URL is empty or null in the database. No update needed.")

    else:
        print(f"Error: No entry found for ID {target_id}.")

except sqlite3.Error as e:
    print(f"SQLite error: {e}")
except Exception as e:
    print(f"An unexpected error occurred: {e}")
finally:
    if conn:
        conn.close()
</file>

<file path="clear_highlights.py">
# clear_highlights.py
import sqlite3
import os

db_path = "video_processing.db"

if not os.path.exists(db_path):
    print(f"Error: Database file '{db_path}' not found.")
    exit()

try:
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Confirm with the user (optional but recommended for delete operations)
    confirm = input(f"Are you sure you want to delete ALL entries from the 'highlights' table in {db_path}? (yes/no): ")
    if confirm.lower() != 'yes':
        print("Operation cancelled.")
        exit()

    # Delete all rows from the highlights table
    cursor.execute("DELETE FROM highlights")
    deleted_rows = cursor.rowcount # Get the number of deleted rows
    conn.commit()

    print(f"Successfully deleted {deleted_rows} rows from the 'highlights' table.")

except sqlite3.Error as e:
    print(f"SQLite error: {e}")
    if conn:
        conn.rollback() # Rollback changes on error
except Exception as e:
    print(f"An unexpected error occurred: {e}")
finally:
    if conn:
        conn.close()
        print("Database connection closed.")
</file>

<file path="config.yaml">
# --- Paths & processing hints (comments only, safe to keep as-is) ---
# Все пути теперь резолвятся относительно paths.base_dir.
# Для Colab/контейнеров можно указать base_dir как рабочую директорию окружения;
# fonts_dir и прочие каталоги будут резолвиться относительно base_dir.
#
# Примерные значения по умолчанию (РАЗКОММЕНТИРУЙТЕ при необходимости переопределения):
# paths:
#   base_dir: .
#   fonts_dir: fonts
# processing:
#   transcriptions_dir: "transcriptions"  # экспорт .txt/.json/.srt/.vtt
#   shorts_dir: "shorts"                   # каталог итоговых шортов
processing:
  use_animated_captions: true
  shorts_dir: "shorts"
  videos_dir: "videos"
  transcriptions_dir: "transcriptions"
  crop_bottom_percent: 0.0
  min_video_dimension_px: 100
  log_transcription_preview_len: 200

llm:
  model_name: "gemini-2.5-flash"
  temperature_highlights: 0.2
  temperature_metadata: 1.0
  max_attempts_highlights: 3
  max_attempts_metadata: 3
  highlight_min_sec: 29
  highlight_max_sec: 61
  max_highlights: 20

logging:
  # Основные настройки логирования
  log_dir: "logs"
  log_level: "INFO"
  enable_console_logging: true
  enable_file_logging: true

  # Настройки ротации логов
  log_rotation_max_bytes: 10485760  # 10MB
  log_rotation_backup_count: 5
  log_rotation_when: "midnight"  # daily rotation at midnight
  log_compression: false

  # Отдельные логгеры для разных типов сообщений
  enable_main_logger: true
  enable_performance_logger: true
  enable_error_logger: true
  enable_debug_logger: false

  # Настройки производительности
  enable_performance_monitoring: true
  performance_log_max_bytes: 5242880  # 5MB
  performance_log_backup_count: 3
  performance_monitoring_interval: 0.5  # seconds

  # Настройки GPU мониторинга
  enable_gpu_monitoring: true
  gpu_priority_mode: true
  gpu_memory_threshold: 0.9  # 90% memory usage warning
  gpu_temperature_threshold: 80  # Celsius

  # Настройки CPU и памяти
  enable_cpu_monitoring: true
  enable_memory_monitoring: true
  memory_threshold: 0.85  # 85% memory usage warning
  cpu_threshold: 90.0  # 90% CPU usage warning

  # Настройки прогресс-баров
  enable_progress_bars: true
  progress_bar_update_interval: 0.1  # seconds

  # Системная информация
  enable_system_info_logging: true
  system_info_log_interval: 3600  # 1 hour in seconds

  # Асинхронная обработка
  enable_async_logging: true
  log_queue_size: 1000
  log_worker_threads: 2

  # Форматирование логов
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_date_format: "%Y-%m-%d %H:%M:%S"
  enable_colors: true

  # Фильтры логирования
  log_filters:
    - "urllib3.connectionpool"  # Filter noisy HTTP logs
    - "PIL.PngImagePlugin"      # Filter PIL debug logs

  # Настройки для разных сред
  development_mode: false
  enable_detailed_tracing: false
  enable_function_call_logging: false

  # Ресурсный мониторинг
  resource_monitoring_interval: 1.0  # seconds
  enable_resource_alerts: true
  alert_threshold_duration: 30.0  # seconds for long operations
paths:
  base_dir: .
  fonts_dir: fonts
captions:
   font_size_px: 38
  letter_spacing_px: 1.5
  line_height: 1.3
  base_color: "#FFFFFF"
  shadow:
    x_px: 2
    y_px: 2
    blur_px: 1
    color: "#00000080"
  accent_palette:
    urgency: "#FFD400"
    drama: "#FF3B30"
    positive: "#34C759"
  animate:
    type: "slide-up"         # возможные: "pop-in" | "slide-up"
    duration_s: 0.35         # диапазон 0.2–0.5
    easing: "easeOutCubic"
    per_word_stagger_ms: 120
  position:
    mode: "safe_bottom"      # возможные: "safe_bottom" | "center"
    bottom_offset_pct: 22    # 20–28% рекомендовано, по умолчанию 22
    center_offset_pct: 12
    boundary_padding_px: 10
  emoji:
    enabled: false
    max_per_short: 2
    style: "shiny"           # возможные: "shiny" | "pulse" | "none"
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2024 SamurAIGPT

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="main.py">
from Components.YoutubeDownloader import download_youtube_video
from Components.Edit import extractAudio, crop_video, burn_captions, crop_bottom_video, animate_captions, get_video_dimensions
from Components.Transcription import transcribe_unified
from faster_whisper import WhisperModel
import torch
import json
from Components.LanguageTasks import GetHighlights, build_transcription_prompt, compute_tone_and_keywords, compute_emojis_for_segment
from Components.FaceCrop import crop_to_vertical_average_face
from Components.Database import VideoDatabase
from dataclasses import dataclass, field
from typing import Optional, List
import os
import traceback
import time
from Components.config import get_config, AppConfig
from Components.Logger import logger, timed_operation
from Components.Paths import build_short_output_name

# Load config once
cfg = get_config()
print(f"Конфиг загружен: shorts_dir={cfg.processing.shorts_dir}, model={cfg.llm.model_name}")

# Инициализация системы логирования
if cfg.logging.enable_system_info_logging:
    logger.log_system_info()

# --- Configuration Flags ---
# Set to True to use two words-level animated captions (slower but nicer)
# Set to False to use the default, faster ASS subtitle burning
USE_ANIMATED_CAPTIONS = cfg.processing.use_animated_captions

# Define the output directory for final shorts
SHORTS_DIR = cfg.processing.shorts_dir

# Define the crop percentage for the bottom of the video (useful when there are integrated captions in the original video)
CROP_PERCENTAGE_BOTTOM = cfg.processing.crop_bottom_percent

# --- Transcriptions JSON helpers (non-blocking) ---
def build_transcriptions_dir():
    from pathlib import Path
    return Path(__file__).resolve().parent / "transcriptions"


def ensure_dir(path_like):
    from pathlib import Path
    p = Path(path_like)
    try:
        p.parent.mkdir(parents=True, exist_ok=True)
    except Exception:
        try:
            p.mkdir(parents=True, exist_ok=True)
        except Exception:
            pass


def sanitize_base_name(name: str) -> str:
    import re
    from pathlib import Path
    try:
        base = Path(str(name)).stem
    except Exception:
        base = str(name)
    base = base.replace(" ", "_").lower()
    base = re.sub(r"[^A-Za-z0-9_-]", "", base)
    return base


def save_json_safely(data, path):
    import json
    from pathlib import Path
    p = Path(path)
    try:
        ensure_dir(p)
        with p.open("w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        try:
            resolved = p.resolve()
        except Exception:
            resolved = str(p)
        print(f"[WARN] Ошибка при сохранении файла: {resolved} – {e}")
        return False


def _to_float(val, default=None):
    try:
        return float(val)
    except Exception:
        return default


def to_full_segments_payload(segments):
    result = []
    for seg in (segments or []):
        start = 0.0
        end = 0.0
        text = ""
        speaker = None
        confidence = None
        if isinstance(seg, dict):
            start = _to_float(seg.get("start"), 0.0)
            end = _to_float(seg.get("end"), 0.0)
            text = str(seg.get("text", ""))
            speaker_val = seg.get("speaker", None)
            speaker = str(speaker_val) if speaker_val is not None else None
            conf_val = seg.get("confidence", seg.get("prob", seg.get("probability", seg.get("score", None))))
            confidence = _to_float(conf_val, None) if conf_val is not None else None
        elif isinstance(seg, (list, tuple)) and len(seg) >= 3:
            text = str(seg[0]) if seg[0] is not None else ""
            start = _to_float(seg[1], 0.0)
            end = _to_float(seg[2], 0.0)
            speaker = None
            confidence = None
        else:
            start = _to_float(getattr(seg, "start", 0.0), 0.0)
            end = _to_float(getattr(seg, "end", 0.0), 0.0)
            t = getattr(seg, "text", "")
            text = str(t) if t is not None else ""
            sp = getattr(seg, "speaker", None)
            speaker = str(sp) if sp is not None else None
            conf_val = getattr(seg, "confidence", None)
            if conf_val is None:
                for k in ("prob", "probability", "score"):
                    if hasattr(seg, k):
                        conf_val = getattr(seg, k)
                        break
            confidence = _to_float(conf_val, None) if conf_val is not None else None
        result.append({"start": start, "end": end, "text": text, "speaker": speaker, "confidence": confidence})
    return {"segments": result}


def to_words_payload(word_level_result):
    words = []
    segments = []
    if word_level_result is None:
        return {"words": words}
    if isinstance(word_level_result, dict):
        segments = word_level_result.get("segments", [])
        if not segments and "words" in word_level_result:
            segments = [{"words": word_level_result.get("words", [])}]
    else:
        segments = getattr(word_level_result, "segments", []) or []
    for seg in segments:
        seg_words = seg.get("words", []) if isinstance(seg, dict) else getattr(seg, "words", []) or []
        # Wait: indentation alignment; ensure 8 spaces? We'll keep 8 spaces. We'll correct.
        for w in seg_words:
            if isinstance(w, dict):
                start = _to_float(w.get("start", 0.0), 0.0)
                end = _to_float(w.get("end", 0.0), 0.0)
                word_val = w.get("word", w.get("text", ""))
                conf_val = w.get("confidence", w.get("prob", w.get("probability", w.get("score", None))))
            else:
                start = _to_float(getattr(w, "start", 0.0), 0.0)
                end = _to_float(getattr(w, "end", 0.0), 0.0)
                word_val = getattr(w, "word", getattr(w, "text", ""))
                conf_val = getattr(w, "confidence", None)
                if conf_val is None:
                    for k in ("prob", "probability", "score"):
                        if hasattr(w, k):
                            conf_val = getattr(w, k)
                            break
            words.append({
                "start": start,
                "end": end,
                "word": str(word_val) if word_val is not None else "",
                "confidence": _to_float(conf_val, None) if conf_val is not None else None
            })
    return {"words": words}

@dataclass
class ProcessingContext:
    """
    Контекст обработки видео для пайплайна.

    Ключевые поля:
    - cfg: AppConfig
    - db: VideoDatabase
    - url/local_path: входной источник
    - video_path: путь к исходному видео
    - video_id: идентификатор в БД
    - audio_path: путь к извлеченному аудио
    - base_name: базовое имя файла видео
    - initial_width/initial_height: исходные размеры видео
    - transcription_segments: список сегментов транскрипции (dict: start, end, text, speaker?)
    - transcription_text: форматированный текст транскрипции для LLM
    - outputs: список путей к финальным клипам
    """
    cfg: AppConfig
    db: VideoDatabase
    url: Optional[str] = None
    local_path: Optional[str] = None
    video_path: Optional[str] = None
    video_id: Optional[str] = None
    audio_path: Optional[str] = None
    base_name: Optional[str] = None
    initial_width: int = 0
    initial_height: int = 0
    transcription_segments: List[dict] = field(default_factory=list)
    transcription_text: Optional[str] = None
    outputs: List[str] = field(default_factory=list)
    word_level_transcription: Optional[dict] = None


def init_context(url: Optional[str], local_path: Optional[str]) -> ProcessingContext:
    """Инициализирует контекст: загружает конфиг, создаёт БД и гарантирует наличие выходных директорий."""
    cfg_local = get_config()
    # Ensure directories exist
    os.makedirs(cfg_local.processing.shorts_dir, exist_ok=True)
    os.makedirs(cfg_local.processing.videos_dir, exist_ok=True)
    ctx = ProcessingContext(cfg=cfg_local, db=VideoDatabase(), url=url, local_path=local_path)
    return ctx


@timed_operation("resolve_video_source")
def resolve_video_source(ctx: ProcessingContext) -> bool:
    """Определяет источник видео (URL/локальный), учитывает кэш БД, выставляет ctx.video_path и ctx.video_id."""
    if not ctx.url and not ctx.local_path:
        logger.logger.error("Error: Must provide either URL or local path")
        return False

    video_path = None
    video_id = None

    if ctx.url:
        logger.logger.info(f"Processing YouTube URL: {ctx.url}")
        cached_data = ctx.db.get_cached_processing(youtube_url=ctx.url)
        if cached_data:
            logger.logger.info("Found cached video from URL!")
            video_path = cached_data["video"][2]
            video_id = cached_data["video"][0]
            if not os.path.exists(video_path):
                logger.logger.warning(f"Cached video path not found: {video_path}. Re-downloading.")
                video_path = None
                video_id = None
        if not video_path:
            with logger.operation_context("download_youtube_video", {"url": ctx.url}):
                video_path = download_youtube_video(ctx.url)
                if not video_path:
                    logger.logger.error("Failed to download video")
                    return False
                if not video_path.lower().endswith('.mp4'):
                    base, _ = os.path.splitext(video_path)
                    new_path = base + ".mp4"
                    try:
                        os.rename(video_path, new_path)
                        video_path = new_path
                        logger.logger.info(f"Renamed downloaded file to: {video_path}")
                    except OSError as e:
                        logger.logger.warning(f"Error renaming file to mp4: {e}. Trying conversion.")
                        pass
    else:
        logger.logger.info(f"Processing local file: {ctx.local_path}")
        if not os.path.exists(ctx.local_path):
            logger.logger.error("Error: Local file does not exist")
            return False
        video_path = ctx.local_path
        cached_data = ctx.db.get_cached_processing(local_path=ctx.local_path)
        if cached_data:
            logger.logger.info("Found cached local video!")
            video_id = cached_data["video"][0]

    if not video_path or not os.path.exists(video_path):
        logger.logger.error("No valid video path obtained or file does not exist.")
        return False

    ctx.video_path = video_path
    ctx.video_id = video_id
    ctx.base_name = os.path.splitext(os.path.basename(video_path))[0]
    return True


def validate_dimensions(ctx: ProcessingContext) -> bool:
    """Проверяет исходные размеры видео и сохраняет их в контекст."""
    print("\n--- Checking Initial Video Dimensions ---")
    w, h = get_video_dimensions(ctx.video_path)
    if w is None or h is None:
        print("Error: Could not determine initial video dimensions. Aborting.")
        return False
    ctx.initial_width, ctx.initial_height = int(w), int(h)
    if ctx.initial_width < ctx.cfg.processing.min_video_dimension_px or ctx.initial_height < ctx.cfg.processing.min_video_dimension_px:
        print(f"Warning: Initial video dimensions ({ctx.initial_width}x{ctx.initial_height}) seem very small.")
    print("--- Initial Check Done ---")
    return True


def ensure_audio(ctx: ProcessingContext) -> bool:
    """Извлекает или загружает из кэша аудио. Обновляет БД и ctx.audio_path/ctx.video_id."""
    audio_path = None
    cached_data = None
    if ctx.url:
        cached_data = ctx.db.get_cached_processing(youtube_url=ctx.url)
    else:
        cached_data = ctx.db.get_cached_processing(local_path=ctx.local_path or ctx.video_path)

    if cached_data and cached_data["video"][3]:
        print("Using cached audio file reference")
        audio_path = cached_data["video"][3]
        if not os.path.exists(audio_path):
            print("Cached audio file not found, extracting again")
            audio_path = None

    if not audio_path:
        print(f"Extracting audio from video: {ctx.video_path}")
        audio_path = extractAudio(ctx.video_path)
        if not audio_path:
            print("Failed to extract audio")
            return False
        if ctx.video_id:
            ctx.db.update_video_audio_path(ctx.video_id, audio_path)
        else:
            ctx.video_id = ctx.db.add_video(ctx.url, ctx.video_path, audio_path)

    if not ctx.video_id:
        video_entry = ctx.db.get_video(youtube_url=ctx.url, local_path=ctx.video_path)
        if video_entry:
            ctx.video_id = video_entry[0]
        else:
            print("Error: Video ID could not be determined after processing.")
            return False

    ctx.audio_path = audio_path
    return True


def run_unified_transcription(ctx: ProcessingContext, model: WhisperModel) -> bool:
    """
    Выполняет или загружает из кэша транскрипцию (сегменты и слова).
    Использует JSON-файлы как основной кэш для обоих типов данных.
    Сохраняет результаты в контекст.
    """
    base_name = sanitize_base_name(os.path.splitext(os.path.basename(ctx.video_path))[0])
    segments_cache_path = build_transcriptions_dir() / f"{base_name}_full_segments.json"
    words_cache_path = build_transcriptions_dir() / f"{base_name}_word_level.json"

    segments_loaded = False
    words_loaded = False

    # Попытка загрузить из JSON кэша
    if segments_cache_path.exists():
        try:
            with segments_cache_path.open("r", encoding="utf-8") as f:
                data = json.load(f)
                ctx.transcription_segments = data.get("segments", [])
                segments_loaded = True
                print("Loaded segment transcription from JSON cache.")
        except Exception as e:
            print(f"[WARN] Could not load segments from JSON cache: {e}")

    if words_cache_path.exists():
        try:
            with words_cache_path.open("r", encoding="utf-8") as f:
                ctx.word_level_transcription = json.load(f)
                words_loaded = True
                print("Loaded word-level transcription from JSON cache.")
        except Exception as e:
            print(f"[WARN] Could not load words from JSON cache: {e}")

    if segments_loaded and words_loaded:
        print("Both transcriptions loaded from cache. Skipping transcription.")
        # Убедимся, что в контексте не None, а пустой dict, если что-то пошло не так
        if ctx.word_level_transcription is None:
            ctx.word_level_transcription = {"segments": []}
        return True

    # Если чего-то нет, запускаем унифицированную транскрипцию
    print("Cache incomplete. Running unified transcription for segments and words...")
    
    # Используется унифицированный подход для повышения эффективности
    segments_legacy, word_level_transcription = transcribe_unified(ctx.audio_path, model)

    if not segments_legacy:
        print("Unified transcription failed. Cannot proceed.")
        return False

    # Преобразуем и сохраняем результаты в контекст
    full_segments_payload = to_full_segments_payload(segments_legacy)
    ctx.transcription_segments = full_segments_payload.get("segments", [])
    ctx.word_level_transcription = word_level_transcription

    # Сохраняем в БД (как и раньше) и в JSON-кэш
    if ctx.video_id:
        ctx.db.add_transcription(ctx.video_id, segments_legacy)
    
    save_json_safely(full_segments_payload, segments_cache_path)
    save_json_safely(word_level_transcription, words_cache_path)
    
    print("Unified transcription complete. Results saved to context and cache.")
    return True


def prepare_transcript_text(ctx: ProcessingContext) -> None:
    """Формирует текст транскрипции через LanguageTasks.build_transcription_prompt и логирует превью."""
    TransText = build_transcription_prompt(ctx.transcription_segments)
    ctx.transcription_text = TransText
    print(f"\nFirst {cfg.processing.log_transcription_preview_len} characters of transcription:")
    print(TransText[:cfg.processing.log_transcription_preview_len] + "...")


def fetch_highlights(ctx: ProcessingContext) -> list:
    """Запрашивает у LLM список обогащённых хайлайтов по готовому тексту транскрипции."""
    print("Generating new highlights")
    return GetHighlights(ctx.transcription_text or "")


@timed_operation("process_highlight")
def process_highlight(ctx: ProcessingContext, item) -> Optional[str]:
    """Обрабатывает один хайлайт: кропы, субтитры, сохранение; возвращает путь финального файла либо None."""
    temp_segment = None
    cropped_vertical_temp = None
    cropped_vertical_final = None
    segment_audio_path = None
    transcription_result = None

    seq = int(item.get("_seq", 1)) if isinstance(item, dict) else 1
    total = int(item.get("_total", 1)) if isinstance(item, dict) else 1

    try:
        start = float(item["start"])
        stop = float(item["end"])

        # Корректировка stop по фактическому окончанию последнего полного слова (по start < stop)
        adjusted_stop = stop
        if getattr(ctx, "word_level_transcription", None):
            last_word_end = find_last_word_end_time(ctx.word_level_transcription, stop)
            if last_word_end is not None and last_word_end > stop:
                prev_stop = adjusted_stop
                adjusted_stop = last_word_end
                logger.logger.info(f"[WordLevel] Adjusted stop from {prev_stop:.2f}s to {adjusted_stop:.2f}s based on last word end")
        # Гарантия: stop > start
        if adjusted_stop <= start:
            adjusted_stop = start + 0.1

        logger.logger.info(f"\n--- Processing Highlight {seq}/{total}: Start={start:.2f}s, End={stop:.2f}s (effective end {adjusted_stop:.2f}s) ---")
        if isinstance(item, dict) and "caption_with_hashtags" in item:
            logger.logger.info(f"Caption: {item['caption_with_hashtags']}")

        # --- Define File Paths ---
        base_name = os.path.splitext(os.path.basename(ctx.video_path))[0]
        output_base = f"{base_name}_highlight_{seq}"
        temp_segment = os.path.join(ctx.cfg.processing.videos_dir, f"{output_base}_temp_segment.mp4")
        cropped_vertical_temp = os.path.join(ctx.cfg.processing.videos_dir, f"{output_base}_vertical_temp.mp4")
        cropped_vertical_final = os.path.join(ctx.cfg.processing.videos_dir, f"{output_base}_vertical_final.mp4")
        # Use unified naming with zero-padded index for final output (and derived temp anim path)
        final_output_with_captions, _unused_temp_anim = build_short_output_name(base_name, seq, SHORTS_DIR)
        if USE_ANIMATED_CAPTIONS:
            segment_audio_path = os.path.join(ctx.cfg.processing.videos_dir, f"{output_base}_temp_audio.wav")

        # 1. Extract Segment (Video + Audio, Original Aspect Ratio)
        with logger.operation_context("extract_segment", {"start": start, "end": adjusted_stop}):
            logger.logger.info("1. Extracting segment...")
            extract_success = crop_video(ctx.video_path, temp_segment, start, adjusted_stop, ctx.initial_width, ctx.initial_height)
            if not extract_success:
                logger.logger.error(f"Failed step 1 for highlight {seq}. Skipping.")
                if os.path.exists(temp_segment):
                    try:
                        os.remove(temp_segment)
                    except Exception as clean_e:
                        logger.logger.warning(f"Warning: Could not remove temp segment file: {clean_e}")
                return None

        # --- CHECK DIMENSIONS: Segment ---
        with logger.operation_context("check_segment_dimensions", {"segment_path": temp_segment}):
            logger.logger.info("\n--- Checking Segment Video Dimensions ---")
            segment_width, segment_height = get_video_dimensions(temp_segment)
            if segment_width is None or segment_height is None:
                logger.logger.error("Error: Could not determine segment video dimensions. Skipping highlight.")
                if os.path.exists(temp_segment):
                    try:
                        os.remove(temp_segment)
                    except Exception as clean_e:
                        logger.logger.warning(f"Warning: Could not remove temp segment file: {clean_e}")
                return None
            if segment_width != ctx.initial_width or segment_height != ctx.initial_height:
                logger.logger.warning(f"Warning: Segment dimensions ({segment_width}x{segment_height}) differ from initial ({ctx.initial_width}x{ctx.initial_height}).")
            logger.logger.info("--- Segment Check Done ---")

        # 2. Create Vertical Crop (Based on Average Face Position)
        with logger.operation_context("create_vertical_crop", {"segment_path": temp_segment}):
            logger.logger.info("2. Creating average face centered vertical crop...")
            vert_crop_path = crop_to_vertical_average_face(temp_segment, cropped_vertical_temp)
            if not vert_crop_path:
                logger.logger.error(f"Failed step 2 (average face crop) for highlight {seq}. Skipping.")
                if os.path.exists(temp_segment):
                    try:
                        os.remove(temp_segment)
                    except Exception as clean_e:
                        logger.logger.warning(f"Warning: Could not remove temp segment file: {clean_e}")
                if os.path.exists(cropped_vertical_temp):
                    try:
                        os.remove(cropped_vertical_temp)
                    except Exception as clean_e:
                        logger.logger.warning(f"Warning: Could not remove temp vertical crop file: {clean_e}")
                return None
            cropped_vertical_temp = vert_crop_path

        # 3. Crop Bottom Off Vertical Video (Temporary Fix)
        if CROP_PERCENTAGE_BOTTOM > 0:
            print("3. Applying bottom crop to vertical video...")
            bottom_crop_success = crop_bottom_video(cropped_vertical_temp, cropped_vertical_final, CROP_PERCENTAGE_BOTTOM)
            if not bottom_crop_success:
                print(f"Failed step 3 for highlight {seq}. Skipping.")
                if os.path.exists(temp_segment):
                    try:
                        os.remove(temp_segment)
                    except Exception:
                        pass
                if os.path.exists(cropped_vertical_temp):
                    try:
                        os.remove(cropped_vertical_temp)
                    except Exception:
                        pass
                if os.path.exists(cropped_vertical_final):
                    try:
                        os.remove(cropped_vertical_final)
                    except Exception:
                        pass
                return None
        else:
            print("No bottom crop applied")
            cropped_vertical_final = cropped_vertical_temp

        # 4. Choose Captioning Method
        captioning_success = False
        if USE_ANIMATED_CAPTIONS:
            print("Attempting Word-Level Animated Captions (reusing global word-level transcription)...")
            transcription_result = None
            if getattr(ctx, "word_level_transcription", None):
                # Подготовка слов через чистый хелпер
                try:
                    transcription_result = prepare_words_for_segment(ctx.word_level_transcription, start, adjusted_stop)
                    # Логирование количества слов после фильтрации
                    words_count = 0
                    try:
                        segs = transcription_result.get("segments", []) or []
                        if segs:
                            words_count = len(segs[0].get("words", []))
                    except Exception:
                        words_count = 0
                    print(f"[WordLevel] Prepared {words_count} words for animated captions from global transcription.")
                    # Опционально сохраним JSON слов для дебага/просмотра
                    try:
                        base_sanitized = sanitize_base_name(os.path.splitext(os.path.basename(ctx.video_path))[0])
                        words_payload = to_words_payload(transcription_result)
                        target_words = build_transcriptions_dir() / f"{base_sanitized}_highlight_{seq}_words.json"
                        save_json_safely(words_payload, target_words)
                    except Exception:
                        pass
                except Exception as e:
                    print(f"[WordLevel][WARN] Failed to prepare words subset: {e}")
                    transcription_result = None
            else:
                print("[WordLevel][WARN] No global word-level transcription available; cannot animate captions for this highlight.")
    
            if transcription_result and transcription_result.get("segments", []) and transcription_result["segments"][0].get("words"):
                # tone/keywords heuristic — compute meta based on segment text used for captions
                text_for_segment = ""
                try:
                    # Prefer enriched text from highlight item (LLM-extracted for this segment)
                    text_for_segment = (item.get("segment_text", "") if isinstance(item, dict) else "") or ""
                except Exception:
                    text_for_segment = ""
                if not text_for_segment:
                    # Fallback: reconstruct from prepared words list
                    try:
                        segs = transcription_result.get("segments", []) or []
                        if segs:
                            words = segs[0].get("words", []) or []
                            text_for_segment = " ".join(
                                (w.get("text") or w.get("word") or "").strip()
                                for w in words if isinstance(w, dict) and (w.get("text") or w.get("word"))
                            ).strip()
                    except Exception:
                        text_for_segment = ""
                meta = compute_tone_and_keywords(text_for_segment) if text_for_segment else None

                # emoji: heuristics and placement — propagate emoji metadata (backward compatible)
                highlight_meta = meta or {}
                try:
                    cfg_emoji = getattr(ctx.cfg.captions, "emoji", None)
                    if cfg_emoji and getattr(cfg_emoji, "enabled", False) and text_for_segment:
                        tone_val = (highlight_meta.get("tone") if isinstance(highlight_meta, dict) else None) or "neutral"
                        max_per = int(getattr(cfg_emoji, "max_per_short", 0) or 0)
                        emojis = compute_emojis_for_segment(text_for_segment, tone_val, max_per)
                        if isinstance(highlight_meta, dict):
                            highlight_meta = {**highlight_meta, "emojis": list(emojis or [])[:max_per]}
                except Exception:
                    # Полная обратная совместимость: любые ошибки с эмодзи не должны ломать рендер
                    pass

                captioning_success = animate_captions(
                    cropped_vertical_final,
                    temp_segment,
                    transcription_result,
                    final_output_with_captions,
                    style_cfg=ctx.cfg.captions,
                    highlight_meta=highlight_meta
                )
            else:
                print("Word-level data for this segment is empty. Skipping animation.")
                captioning_success = False
        else:
            print("Using Standard ASS Caption Burning...")
            transcriptions_legacy = [[
                str(seg.get("text", "")),
                float(seg.get("start", 0.0)),
                float(seg.get("end", 0.0)),
            ] for seg in (ctx.transcription_segments or [])]
            captioning_success = burn_captions(cropped_vertical_final, temp_segment, transcriptions_legacy, start, adjusted_stop, final_output_with_captions, style_cfg=ctx.cfg.captions)

        # 5. Handle Captioning Result
        if not captioning_success:
            print(f"Animated caption generation failed for highlight {seq}. Attempting ASS burn fallback...")
            transcriptions_legacy = [[
                str(seg.get("text", "")),
                float(seg.get("start", 0.0)),
                float(seg.get("end", 0.0)),
            ] for seg in (ctx.transcription_segments or [])]
            fallback_success = burn_captions(cropped_vertical_final, temp_segment, transcriptions_legacy, start, adjusted_stop, final_output_with_captions, style_cfg=ctx.cfg.captions)
            if not fallback_success:
                print(f"ASS fallback failed for highlight {seq}. Skipping.")
                if os.path.exists(temp_segment):
                    try:
                        os.remove(temp_segment)
                    except Exception as clean_e:
                        print(f"Warning: Could not remove temp segment file: {clean_e}")
                if os.path.exists(cropped_vertical_temp):
                    try:
                        os.remove(cropped_vertical_temp)
                    except Exception as clean_e:
                        print(f"Warning: Could not remove temp vertical file: {clean_e}")
                if os.path.exists(cropped_vertical_final):
                    try:
                        os.remove(cropped_vertical_final)
                    except Exception as clean_e:
                        print(f"Warning: Could not remove final vertical file: {clean_e}")
                if segment_audio_path and os.path.exists(segment_audio_path):
                    try:
                        os.remove(segment_audio_path)
                    except Exception as clean_e:
                        print(f"Warning: Could not remove segment audio file: {clean_e}")
                return None
            else:
                captioning_success = True

        logger.logger.info(f"Successfully processed highlight {seq}.")
        ctx.outputs.append(final_output_with_captions)
        logger.logger.info(f"Saving highlight {seq} info to database: {final_output_with_captions}")

        segment_text = item.get('segment_text', '') if isinstance(item, dict) else ''
        caption = item.get('caption_with_hashtags', '') if isinstance(item, dict) else ''

        with logger.operation_context("save_to_database", {"video_id": ctx.video_id, "highlight_path": final_output_with_captions}):
            ctx.db.add_highlight(
                ctx.video_id,
                start,
                adjusted_stop,
                final_output_with_captions,
                segment_text=segment_text,
                caption_with_hashtags=caption
            )

        # --- Cleanup Intermediate Files ---
        with logger.operation_context("cleanup_intermediate_files", {"highlight_seq": seq}):
            logger.logger.info("Cleaning up intermediate files for this highlight...")
            if os.path.exists(temp_segment):
                try:
                    os.remove(temp_segment)
                except Exception as clean_e:
                    logger.logger.warning(f"Warning: Could not remove temp segment file: {clean_e}")
            if os.path.exists(cropped_vertical_temp):
                try:
                    os.remove(cropped_vertical_temp)
                except Exception as clean_e:
                    logger.logger.warning(f"Warning: Could not remove temp vertical file: {clean_e}")
            if os.path.exists(cropped_vertical_final):
                try:
                    os.remove(cropped_vertical_final)
                except Exception as clean_e:
                    logger.logger.warning(f"Warning: Could not remove final vertical file: {clean_e}")
            if segment_audio_path and os.path.exists(segment_audio_path):
                try:
                    os.remove(segment_audio_path)
                except Exception as clean_e:
                    logger.logger.warning(f"Warning: Could not remove segment audio file: {clean_e}")

        return final_output_with_captions

    except Exception:
        print(f"\n--- Error processing highlight {seq} --- ")
        traceback.print_exc()
        print("Continuing to next highlight if available.")
        if temp_segment and os.path.exists(temp_segment):
            try:
                os.remove(temp_segment)
            except Exception as clean_e:
                print(f"Warning: Could not remove temp segment file: {clean_e}")
        if cropped_vertical_temp and os.path.exists(cropped_vertical_temp):
            try:
                os.remove(cropped_vertical_temp)
            except Exception as clean_e:
                print(f"Warning: Could not remove temp vertical file: {clean_e}")
        if cropped_vertical_final and os.path.exists(cropped_vertical_final):
            try:
                os.remove(cropped_vertical_final)
            except Exception as clean_e:
                print(f"Warning: Could not remove final vertical file: {clean_e}")
        if segment_audio_path and os.path.exists(segment_audio_path):
            try:
                os.remove(segment_audio_path)
            except Exception as clean_e:
                print(f"Warning: Could not remove segment audio file: {clean_e}")
        return None


def process_all_highlights(ctx: ProcessingContext, items: list) -> List[str]:
    """Итерирует по сегментам, вызывает process_highlight, накапливает выходы и печатает итоги/ошибки."""
    try:
        final_output_paths: List[str] = []
        total = len(items or [])
        for i, raw_item in enumerate(items or []):
            item = dict(raw_item) if isinstance(raw_item, dict) else raw_item
            if isinstance(item, dict):
                item["_seq"] = i + 1
                item["_total"] = total
            out_path = process_highlight(ctx, item)
            if out_path:
                final_output_paths.append(out_path)

        if not final_output_paths:
            print("\nProcessing finished, but no highlight segments were successfully converted.")
            return []
        else:
            print(f"\nProcessing finished. Generated {len(final_output_paths)} shorts in '{SHORTS_DIR}' directory.")
            return final_output_paths
    except Exception as e:
        print(f"Error in overall highlight processing: {str(e)}")
        traceback.print_exc()
        return []


def _select_whisper_runtime():
    """Выбирает оптимальные параметры для модели Whisper с GPU-first подходом."""
    try:
        has_cuda = torch.cuda.is_available()
        gpu_count = torch.cuda.device_count() if has_cuda else 0
    except Exception:
        has_cuda = False
        gpu_count = 0

    # GPU-first подход
    if has_cuda and cfg.logging.gpu_priority_mode:
        device = "cuda"
        # Используем лучшую доступную модель для GPU
        model_size = "large-v3"
        compute_type = "float16"
        cpu_threads = 0  # Для GPU используем 0 CPU threads
        logger.logger.info(f"GPU-first режим: Используем {gpu_count} GPU(s), модель {model_size}")
    else:
        device = "cpu"
        model_size = "small"
        compute_type = "int8"
        cpu_threads = max(1, os.cpu_count() - 2) if os.cpu_count() else 4
        logger.logger.info(f"CPU режим: Используем {cpu_threads} потоков, модель {model_size}")

    return model_size, device, compute_type, cpu_threads


@timed_operation("video_processing_pipeline")
def process_video(url: str = None, local_path: str = None):
    """
    Координатор пайплайна. Публичная сигнатура сохранена.
    Возвращает список путей к сгенерированным клипам или None при ошибке/пустом результате.
    """
    with logger.operation_context("initialize_context", {"url": url, "local_path": local_path}):
        ctx = init_context(url, local_path)

    with logger.operation_context("resolve_video_source", {"url": url, "local_path": local_path}):
        if not resolve_video_source(ctx):
            return None

    with logger.operation_context("validate_dimensions", {"video_path": ctx.video_path}):
        if not validate_dimensions(ctx):
            return None

    with logger.operation_context("ensure_audio", {"video_path": ctx.video_path}):
        if not ensure_audio(ctx):
            return None

    # --- Загрузка модели Whisper ---
    with logger.operation_context("load_whisper_model", {"model_size": "auto", "device": "auto"}):
        print("\n--- Loading Whisper Model ---")
        model_size, device, compute_type, cpu_threads = _select_whisper_runtime()
        os.environ["OMP_NUM_THREADS"] = str(cpu_threads)
        os.environ["MKL_NUM_THREADS"] = str(cpu_threads)
        try:
            model = WhisperModel(
                model_size,
                device=device,
                compute_type=compute_type,
                cpu_threads=cpu_threads if device == "cpu" else 0,
                num_workers=2,
            )
            print(f"Faster-Whisper model loaded: {model_size} on {device} ({compute_type})")
        except Exception as e:
            print(f"FATAL: Could not load Whisper model. Error: {e}")
            return None

    # --- Транскрипция (унифицированный вызов) ---
    with logger.operation_context("transcription", {"audio_path": ctx.audio_path}):
        logger.logger.info(f"Начало транскрипции аудио: {ctx.audio_path}")
        transcription_start = time.time()

        if not run_unified_transcription(ctx, model):
            logger.logger.error("Транскрипция завершилась неудачей")
            return None

        transcription_time = time.time() - transcription_start
        logger.logger.info(f"Транскрипция завершена за {transcription_time:.2f} секунд")

    with logger.operation_context("prepare_transcript_text", {"transcription_length": len(ctx.transcription_text or "")}):
        prepare_transcript_text(ctx)

    try:
        with logger.operation_context("fetch_highlights", {"transcription_length": len(ctx.transcription_text or "")}):
            highlights = fetch_highlights(ctx)
            if not highlights or len(highlights) == 0:
                print("No valid highlights found")
                return None

        with logger.operation_context("process_highlights", {"highlights_count": len(highlights)}):
            # Создаем прогресс-бар для обработки хайлайтов
            progress_bar = logger.create_progress_bar(
                total=len(highlights),
                desc="Обработка хайлайтов",
                unit="highlight"
            )

            outputs = []
            for i, highlight in enumerate(highlights):
                # Ensure proper sequencing for unique filenames and logging
                payload = dict(highlight) if isinstance(highlight, dict) else highlight
                if isinstance(payload, dict):
                    payload["_seq"] = i + 1
                    payload["_total"] = len(highlights)
                with logger.operation_context(
                    "process_single_highlight",
                    {"highlight_index": i, "highlight_text": (payload.get("caption_with_hashtags", "")[:50] if isinstance(payload, dict) else "")}
                ):
                    output = process_highlight(ctx, payload)
                    if output:
                        outputs.append(output)

                progress_bar.update(1)
                progress_bar.set_postfix({
                    "Обработано": f"{i+1}/{len(highlights)}",
                    "Успешно": len(outputs)
                })

            progress_bar.close()

        if not outputs:
            return None
        return outputs
    except Exception as e:
        print(f"Error in overall highlight processing: {str(e)}")
        traceback.print_exc()
        return None


def prepare_words_for_segment(full_word_level_transcription: dict, start: float, stop: float) -> dict:
    """
    Подготавливает подмножество слов для одного сегмента [start, stop] из глобальной
    словной транскрипции и нормализует их таймкоды к координатам сегмента.

    Вход:
    - full_word_level_transcription: dict в формате
      {
        "segments": [
          {
            "words": [
              {"start": float, "end": float, "text"/"word": str, ...},
              ...
            ],
            ...
          },
          ...
        ]
      }
    - start: абсолютное время начала сегмента, секунды (float)
    - stop:  абсолютное время конца сегмента, секунды (float)

    Логика отбора и нормализации:
    - Выбираются только те слова, которые пересекаются с интервалом [start, stop]:
      word.start < stop и word.end > start.
    - Нормализация в координаты сегмента:
      start_rel = max(0.0, word.start - start)
      end_rel   = max(start_rel, word.end - start)
    - end_rel дополнительно ограничивается длительностью сегмента (stop - start), чтобы
      «хвост» слова за границей сегмента был обрезан.
    - Слова без числовых start/end пропускаются.
    - Результат сортируется по (start_rel, end_rel).

    Возврат:
    Структура совместимая с animate_captions:
    {
      "segments": [{
        "start": 0.0,
        "end": stop - start,
        "text": "segment",
        "words": [
          {"start": start_rel, "end": end_rel, "text": word_text}, ...
        ]
      }]}
    
    Предположения:
    - Функция чистая: не изменяет входные данные, не имеет побочных эффектов.
    - При некорректном входе возвращается сегмент с пустым списком слов и корректной длительностью.
    """
    try:
        seg_duration = max(0.0, float(stop) - float(start))
    except Exception:
        # На всякий случай, если приведение типов не удалось
        seg_duration = max(0.0, (stop or 0.0) - (start or 0.0))

    words_out = []
    try:
        segments_wl = []
        if isinstance(full_word_level_transcription, dict):
            segments_wl = full_word_level_transcription.get("segments", []) or []
        for seg_wl in segments_wl:
            # Достаём список слов из dict или объекта с атрибутом .words
            seg_words = seg_wl.get("words", []) if isinstance(seg_wl, dict) else getattr(seg_wl, "words", []) or []
            if not seg_words:
                continue
            for w in seg_words:
                if isinstance(w, dict):
                    s = _to_float(w.get("start", None), None)
                    e = _to_float(w.get("end", None), None)
                    txt = w.get("text", w.get("word", ""))
                else:
                    s = _to_float(getattr(w, "start", None), None)
                    e = _to_float(getattr(w, "end", None), None)
                    txt = getattr(w, "text", getattr(w, "word", ""))
                # Пропускаем некорректные таймкоды
                if s is None or e is None:
                    continue
                # Фильтр пересечения [start, stop]
                if e > start and s < stop:
                    start_rel = max(0.0, s - start)
                    end_rel = max(start_rel, e - start)
                    # Обрезка по длительности сегмента
                    if end_rel > seg_duration:
                        end_rel = seg_duration
                        if end_rel < start_rel:
                            start_rel = end_rel
                    words_out.append({"start": start_rel, "end": end_rel, "text": str(txt)})
        # Сортировка стабильна для предсказуемости
        words_out.sort(key=lambda x: (x["start"], x["end"]))
    except Exception:
        # В случае неожиданных проблем вернём пустой список слов, сохранив длительность
        words_out = []

    return {
        "segments": [{
            "start": 0.0,
            "end": seg_duration,
            "text": "segment",
            "words": words_out
        }]}
    

def find_last_word_end_time(word_level_transcription: dict, segment_end_time: float) -> Optional[float]:
    """
    Определяет фактическое время окончания «последнего слова до segment_end_time».

    Определение «последнего слова до segment_end_time»:
    - Рассматриваются только слова, у которых start < segment_end_time (start — абсолютный).
    - Возвращается максимальный end среди таких слов.

    Почему функция может вернуть end > segment_end_time:
    - Слово может начинаться до segment_end_time, а заканчиваться ПОСЛЕ него. Это важно для
      корректировки stop вправо, чтобы не обрывать слово в анимации (используется max(original_stop, last_word_end)).

    Граничные случаи:
    - Отсутствуют сегменты/слова — возвращается None.
    - Нечисловые или отсутствующие start/end у слова — такие слова пропускаются.
    - Порядок слов/сегментов может быть произвольным — берётся максимум end среди подходящих.

    Безопасность:
    - Функция устойчиво обрабатывает некорректные структуры (не dict / пустые поля), возвращая None.
    """
    try:
        if not isinstance(word_level_transcription, dict):
            return None

        segments = word_level_transcription.get("segments", []) or []
        if not isinstance(segments, list) or not segments:
            return None

        last_end: Optional[float] = None
        for seg in segments:
            # Безопасно достаём список слов
            words = seg.get("words", []) if isinstance(seg, dict) else getattr(seg, "words", []) or []
            if not words:
                continue

            for w in words:
                if isinstance(w, dict):
                    s = _to_float(w.get("start", None), None)
                    e = _to_float(w.get("end", None), None)
                else:
                    s = _to_float(getattr(w, "start", None), None)
                    e = _to_float(getattr(w, "end", None), None)

                # Пропускаем слова без числовых таймкодов
                if s is None or e is None:
                    continue

                # Критерий отбора — слово началось до segment_end_time
                if s < segment_end_time:
                    if last_end is None or e > last_end:
                        last_end = e

        return last_end
    except Exception:
        return None


if __name__ == "__main__":
    print("\nVideo Processing Options:")
    print("1. Process YouTube URL")
    print("2. Process Local File")
    choice = input("Enter your choice (1 or 2): ")

    if choice == "1":
        url = input("Enter YouTube URL: ")
        output = process_video(url=url)
    elif choice == "2":
        local_file = input("Enter path to local video file: ")
        output = process_video(local_path=local_file)
    else:
        print("Invalid choice")
        output = None

    if output:
        # If output is a list (multiple shorts generated)
        if isinstance(output, list):
            print(f"\nSuccess! Output saved to:")
            for path in output:
                print(f"- {path}")
        else: # Should not happen with current logic, but handle just in case
            print(f"\nSuccess! Output saved to: {output}")
    else:
        print("\nProcessing failed or no shorts generated!")
</file>

<file path="README.md">
# AI-Youtube-Shorts-Generator-Gemini

An AI-powered tool that automatically generates engaging short-form videos from longer YouTube content, optimized for platforms like YouTube Shorts, Instagram Reels, and TikTok and for static videos with a 1 person speaking.

## Key Features

- **Smart Video Download**: 
  - Downloads videos from YouTube URLs with quality selection
  - Supports both progressive and adaptive streams
  - Automatically merges video and audio for best quality
  - Handles local video files as input

- **Advanced Transcription**:
  - Uses `faster-whisper` (base.en model) for efficient transcription
  - Provides both segment-level and word-level timestamps
  - CPU-optimized processing with int8 quantization
  - Multi-threaded performance for faster processing

- **AI-Powered Highlight Detection**:
  - Leverages Google's Gemini-2.0-flash model for content analysis
  - Identifies the most engaging segments from transcriptions
  - Generates relevant hashtags and captions
  - Smart content selection based on engagement potential

- **Intelligent Video Processing**:
  - Multiple vertical cropping strategies:
    - Static centered crop
    - Face-detection based dynamic cropping
    - Average face position based cropping
  - Maintains optimal 9:16 aspect ratio for shorts
  - Automatic bottom margin cropping for better framing
  - Supports both static and animated captions

- **Robust Caching System**:
  - SQLite database for efficient data management
  - Caches processed videos, audio, and transcriptions
  - Prevents redundant processing of previously handled content
  - Easy cache management and cleanup

## Prerequisites

- Python 3.10 or higher
- FFmpeg (latest version recommended)
- CUDA-compatible GPU (optional, for faster processing)
- 4GB+ RAM recommended

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/AI-Youtube-Shorts-Generator.git
   cd AI-Youtube-Shorts-Generator
   ```

2. Create and activate a virtual environment:
   ```bash
   # Windows
   python -m venv venv
   venv\Scripts\activate

   # Linux/MacOS
   python3 -m venv venv
   source venv/bin/activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Set up environment variables:
   Create a `.env` file in the project root:
   ```bash
   GOOGLE_API_KEY=your_google_ai_studio_key_here
   ```

## Usage

1. Start the tool:
   ```bash
   python main.py
   ```

2. Input either:
   - A YouTube URL
   - A path to a local video file

3. Select video quality when prompted (for YouTube downloads)

4. The tool will process your video through several stages:
   - Download/import video
   - Extract and transcribe audio
   - Identify engaging segments
   - Create vertical crops
   - Add captions
   - Generate final shorts

5. Find your processed shorts in the `shorts` directory

## Configuration Options

- `USE_ANIMATED_CAPTIONS`: Toggle between static and animated captions (in main.py) (reccomended)
- `SHORTS_DIR`: Customize output directory for processed videos
- CPU thread optimization in `Components/Transcription.py`

## Project Structure

```
AI-Youtube-Shorts-Generator/
├── Components/
│   ├── Captions.py       # Caption generation and rendering
│   ├── Database.py       # SQLite database management
│   ├── Edit.py          # Video editing and processing
│   ├── FaceCrop.py      # Vertical cropping algorithms
│   ├── LanguageTasks.py # AI content analysis
│   ├── Speaker.py       # Speaker detection (experimental)
│   ├── Transcription.py # Audio transcription
│   └── YoutubeDownloader.py # Video download handling
├── main.py              # Main execution script
├── requirements.txt     # Python dependencies
└── .env                # Environment variables
```

## Database Schema

The SQLite database (`video_processing.db`) contains three main tables:

1. **videos**:
   - id (PRIMARY KEY)
   - youtube_url
   - local_path
   - audio_path
   - created_at

2. **transcriptions**:
   - id (PRIMARY KEY)
   - video_id (FOREIGN KEY)
   - transcription_data
   - created_at

3. **highlights**:
   - id (PRIMARY KEY)
   - video_id (FOREIGN KEY)
   - start_time
   - end_time
   - output_path
   - segment_text
   - caption_with_hashtags
   - created_at

## Known Issues & Limitations

1. **Face Detection**:
   - The face-based cropping can be inconsistent with multiple faces
   - May need manual adjustment for optimal framing in some cases

2. **Speaker Detection**:
   - Current implementation uses basic voice activity detection
   - Full speaker diarization not yet implemented

3. **Resource Usage**:
   - Processing long videos can be memory-intensive
   - GPU acceleration limited to specific components

## Troubleshooting

1. If facing cache-related issues:
   - Delete `video_processing.db` to clear the cache
   - Remove temporary files in the `videos` directory

2. For video processing errors:
   - Ensure FFmpeg is properly installed and accessible
   - Check available disk space for temporary files
   - Verify input video format compatibility

3. For AI-related issues:
   - Confirm Google API key is valid and has sufficient quota
   - Check internet connectivity for API calls

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to your branch
5. Create a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments
- SQL integration made by [YassineKADER](https://github.com/YassineKADER/AI-Youtube-Shorts-Generator-)
- Original project by [SamurAIGPT](https://github.com/SamurAIGPT/AI-Youtube-Shorts-Generator)
- Uses Google's Gemini AI for content analysis
- Powered by faster-whisper for transcription

## Batch‑метаданные

- Цель: пакетно сгенерировать для каждого текстового фрагмента видео метаданные: `title`, `description`, `hashtags`.
- Вход: JSON‑массив объектов вида `{"id": string, "text": string}`.
- Выход: JSON‑массив объектов вида `{"id": string, "title": string, "description": string, "hashtags": string[]}`.
  - Требования:
    - title: 40–70 символов
    - description: ≤ 150 символов
    - hashtags: 3–5 шт., первый элемент строго `#shorts`
    - строго один валидный JSON‑массив без текста вокруг
- Реализация: [generate_metadata_batch()](Components/LanguageTasks.py:576)
- Место интеграции в пайплайн: [GetHighlights()](Components/LanguageTasks.py:711) — после выделения тайм‑сегментов и извлечения текста для каждого сегмента.

Новый системный промпт (точно как в ТЗ), применяемый при пакетной генерации:
```
Ты — эксперт по SMM и продвижению на YouTube, специализирующийся на вирусных Shorts. Тебе на вход подается JSON-массив текстовых фрагментов из видео, каждый с уникальным `id`. Твоя задача — для каждого фрагмента создать оптимальный набор метаданных для максимального вовлечения и охвата.

Правила:
1. Твой ответ должен быть ИСКЛЮЧИТЕЛЬНО одним валидным JSON-массивом. Никакого текста до или после.
2. Для каждого входного объекта с `id` ты должен сгенерировать объект в выходном массиве с тем же `id` и тремя полями: `title`, `description` и `hashtags`.
3. title (заголовок): 40–70 символов, интригующий, задает вопрос или создает предвкушение. Обязательно использовать ключевые слова из текста.
4. description (описание): до 150 символов, кратко раскрывает суть, допускается призыв к действию.
5. hashtags (хэштеги): массив из 3–5 строк; первым ВСЕГДА `#shorts`; остальные — максимально релевантны теме фрагмента.

Пример Входа:
[{"id":"seg_1","text":"Сегодня обсудим, как автоматически находить лучшие моменты в видео..."}]

Пример Выхода:
[{"id":"seg_1","title":"Нейросеть находит лучшие моменты в видео?","description":"Смотрите, как ИИ анализирует ролики для создания шортсов.","hashtags":["#shorts","#ИИ","#нейросети","#видеомонтаж"]}]
```

## Rate‑limiting

- Централизованная обёртка для вызова LLM: [call_llm_with_retry()](Components/LanguageTasks.py:145)
  - Пытается повторить запрос при лимитах API и парсит задержку повтора.
  - Разбор задержки: [parse_retry_delay_seconds()](Components/LanguageTasks.py:89)
- Поддерживаемые форматы retryDelay:
  - `Retry-After: 28`
  - `retry-after: 28`
  - `"retryDelay": "28s"`
  - `retryDelay: 28s`
- Логи (точные формулировки):
  - "Лимит API обработан. Выполняю паузу на X секунд перед попыткой #Y."
  - "Не удалось извлечь retryDelay. Попытки прекращены."
- Поведение:
  - При наличии корректного `retryDelay` выполняется `sleep(X)` и повтор запроса.
  - При отсутствии `retryDelay` попытки прекращаются и исключение пробрасывается дальше.

## Конфигурация (config.yaml)

- Путь: [config.yaml](config.yaml)
- Структура секций и ключи:
  - `processing`:
    - `use_animated_captions`: bool — использовать ли анимированные субтитры
    - `shorts_dir`: str — каталог для итоговых шортов
    - `videos_dir`: str — каталог для промежуточных файлов/видео
    - `crop_bottom_percent`: float — нижний кроп вертикального видео (в процентах)
    - `min_video_dimension_px`: int — минимальный размер видео
    - `log_transcription_preview_len`: int — длина превью транскрипции в логах
  - `llm`:
    - `model_name`: str — модель Gemini
    - `temperature_highlights`: float — температура для поиска хайлайтов
    - `temperature_metadata`: float — температура для метаданных
    - `max_attempts_highlights`: int — попытки при извлечении сегментов
    - `max_attempts_metadata`: int — попытки при генерации метаданных
    - `highlight_min_sec` / `highlight_max_sec`: границы длительности сегмента
    - `max_highlights`: максимум сегментов
- Минимальный пример:
```yaml
processing:
  use_animated_captions: true
  shorts_dir: "shorts"
  videos_dir: "videos"

llm:
  model_name: "gemini-2.5-flash"
  temperature_highlights: 0.2
  temperature_metadata: 1.0
  highlight_min_sec: 29
  highlight_max_sec: 61
  max_highlights: 20
```
- Поведение при отсутствии файла: используются значения по умолчанию; при этом выводится сообщение
  "Конфиг не найден. Использую значения по умолчанию." — см. [Components/config.py](Components/config.py:100).
- Логирование факта загрузки и активной модели — см. [main.py](main.py:15).

## Централизованные пути и конфигурация

Начиная с актуальной версии, все ресурсы и каталоги резолвятся относительно базовой директории из конфига. Ключевые параметры задаются в секции `paths` и `processing`.

Пример фрагмента `config.yaml` (минимальный):
```yaml
paths:
  base_dir: .
  fonts_dir: fonts
processing:
  transcriptions_dir: transcriptions
  shorts_dir: shorts
```

Назначение ключей:
- `base_dir` — корень проекта, относительно которого резолвятся внутренние пути.
- `fonts_dir` — подкаталог со шрифтами (по умолчанию `fonts`), резолвится относительно `base_dir`.
- `transcriptions_dir` — директория, куда сохраняются транскрипции (`.txt/.json/.srt/.vtt`).
- `shorts_dir` — директория, куда сохраняются итоговые шорт‑видео.

Центральные функции резолва путей:
- Ресурсы: [resolve_path()](Components/Paths.py:22)
- Шрифты: [fonts_path()](Components/Paths.py:37)

Пример резолва пути к шрифту «Montserrat-Bold.ttf» с настройками по умолчанию:
- Конфиг: `paths.base_dir: .`, `paths.fonts_dir: fonts`
- Вызов: [fonts_path()](Components/Paths.py:37) для `"Montserrat-Bold.ttf"` даст путь вида: `<ABS_BASE_DIR>/fonts/Montserrat-Bold.ttf`

## Именование выходных short‑файлов

Уникальные имена итоговых и временных short‑файлов формируются функцией [build_short_output_name()](Components/Paths.py:6).

Шаблоны:
- Итоговый: `shorts/{base_name}_highlight_{idx:02d}_final.mp4`  
  Пример: `shorts/master-ABC_highlight_01_final.mp4`
- Временный (анимация): `{final_path}_temp_anim.mp4`

Индекс `idx` — это порядковый номер хайлайта в текущей сессии; он пробрасывается из цикла и логируется (см. [main.py](main.py)).

## Экспорт транскрипций

После завершения унифицированной транскрипции ("Unified transcription complete…") экспорт выполняется автоматически, код — [Components/Transcription.py](Components/Transcription.py).

Создаются файлы:
- `{transcriptions_dir}/{base_name}.txt`
- `{transcriptions_dir}/{base_name}.json`
- `{transcriptions_dir}/{base_name}.srt`
- `{transcriptions_dir}/{base_name}.vtt`

Особенности:
- JSON сохраняет сегменты и слова в UTF‑8 (`ensure_ascii=False`).
- Папка назначения задаётся через `processing.transcriptions_dir` в `config.yaml`.

## Пути и совместимость окружений

- Абсолютные пути вида `/content/uol/*` больше не используются — все ресурсы резолвятся относительно `paths.base_dir`.
- Для Google Colab/контейнеров можно установить `paths.base_dir` на рабочую директорию окружения — остальные относительные пути (`fonts_dir`, `transcriptions_dir`, `shorts_dir`) подхватятся корректно.

### Релевантные тесты

- Проверка уникальности имён short‑файлов: [tests/test_output_naming.py](tests/test_output_naming.py)
- Экспорт транскрипции в 4 формата: [tests/test_transcription_export.py](tests/test_transcription_export.py)
- Резолв путей для ресурсов и шрифта: [tests/test_resource_paths.py](tests/test_resource_paths.py)
## Тесты

- Запуск всех тестов:
  - `python -m unittest -v`
- Покрытие:
  - Форматирование транскрипции [build_transcription_prompt()](Components/LanguageTasks.py:38) —
    [tests/test_language_tasks_prompt_formatting.py](tests/test_language_tasks_prompt_formatting.py)
  - Пакетная генерация метаданных [generate_metadata_batch()](Components/LanguageTasks.py:576) —
    [tests/test_language_tasks_batch_metadata.py](tests/test_language_tasks_batch_metadata.py)
  - Обработка лимитов API [call_llm_with_retry()](Components/LanguageTasks.py:145) —
    [tests/test_language_tasks_rate_limit.py](tests/test_language_tasks_rate_limit.py)
- Для запуска не требуется реальный API/ключ: в тестах используется monkeypatch/mock.
- Опциональный онлайновый сценарий (при наличии GOOGLE_API_KEY): [tests/smoke_gemini_and_whisper.py](tests/smoke_gemini_and_whisper.py)
</file>

</files>
