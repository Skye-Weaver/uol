This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: haarcascade_frontalface_default.xml, readme.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
Components/
  Captions.py
  config.py
  Database.py
  Edit.py
  FaceCrop.py
  FilmMode.py
  LanguageTasks.py
  Logger.py
  Paths.py
  PauseAnalysis.py
  Speaker.py
  SpeakerDetection.py
  Transcription.py
  YoutubeDownloader.py
models/
  deploy.prototxt
.env.example
.gitattributes
.gitignore
clean_url.py
clear_highlights.py
config.yaml
LICENSE
main.py
MOVIE_SELECTION_README.md
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="Components/Captions.py">
import math
import cv2
import numpy as np # Add numpy import
import os
import subprocess
import traceback # For detailed error printing in animate_captions
import unicodedata
from PIL import Image, ImageDraw, ImageFont # Pillow imports for custom font
from Components.Paths import fonts_path
from Components.config import get_config
from Components.Logger import logger
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    import urllib.request
import tempfile
import shutil

# Кэш для пути к шрифту Montserrat-Bold
_FONT_CACHE = None

def find_or_install_font():
    """
    Универсальный поиск и установка шрифта Montserrat-Bold.ttf для Google Colab.

    Поиск осуществляется в следующих директориях:
    - Локальная директория fonts проекта
    - Системные директории Google Colab (/usr/share/fonts/, /usr/local/share/fonts/, ~/.fonts/)

    Если шрифт не найден, выполняется автоматическое скачивание из Google Fonts
    и установка в подходящую системную директорию.

    Returns:
        str: Путь к найденному или установленному шрифту Montserrat-Bold.ttf
    """
    global _FONT_CACHE

    # Возвращаем кэшированный результат, если он есть
    if _FONT_CACHE and os.path.exists(_FONT_CACHE):
        logger.logger.debug(f"[CAPTIONS] Используется кэшированный шрифт: {_FONT_CACHE}")
        return _FONT_CACHE

    font_name = "Montserrat-Bold.ttf"
    font_url = "https://github.com/JulietaUla/Montserrat/raw/master/fonts/ttf/Montserrat-Bold.ttf"

    # Директории для поиска (в порядке приоритета)
    search_dirs = [
        fonts_path(font_name),  # Локальная директория проекта
        "/usr/share/fonts/truetype/montserrat/Montserrat-Bold.ttf",  # Системная директория
        "/usr/local/share/fonts/truetype/montserrat/Montserrat-Bold.ttf",
        "/usr/share/fonts/Montserrat-Bold.ttf",  # Общие системные директории
        "/usr/local/share/fonts/Montserrat-Bold.ttf",
        os.path.expanduser("~/.fonts/Montserrat-Bold.ttf"),  # Пользовательская директория
    ]

    # 1. Поиск существующего шрифта
    for font_path in search_dirs:
        if os.path.exists(font_path):
            logger.logger.info(f"[CAPTIONS] Найден шрифт Montserrat-Bold: {font_path}")
            _FONT_CACHE = font_path
            return font_path

    logger.logger.info("[CAPTIONS] Шрифт Montserrat-Bold не найден в системных директориях. Выполняю установку...")

    # 2. Попытка автоматической установки
    try:
        # Создаем директорию для шрифтов, если её нет
        user_fonts_dir = os.path.expanduser("~/.fonts")
        os.makedirs(user_fonts_dir, exist_ok=True)

        font_install_path = os.path.join(user_fonts_dir, font_name)

        # Скачиваем шрифт
        logger.logger.info(f"[CAPTIONS] Скачиваю шрифт из: {font_url}")
        if REQUESTS_AVAILABLE:
            response = requests.get(font_url, timeout=30)
            response.raise_for_status()
            font_data = response.content
        else:
            # Fallback to urllib
            with urllib.request.urlopen(font_url, timeout=30) as response:
                font_data = response.read()

        # Сохраняем шрифт
        with open(font_install_path, 'wb') as f:
            f.write(font_data)

        logger.logger.info(f"[CAPTIONS] Шрифт успешно установлен: {font_install_path}")

        # Обновляем кэш системных шрифтов (для Linux систем)
        try:
            subprocess.run(['fc-cache', '-f', '-v'], check=True, capture_output=True)
            logger.logger.debug("[CAPTIONS] Кэш системных шрифтов обновлен")
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.logger.warning("[CAPTIONS] Предупреждение: не удалось обновить кэш системных шрифтов (fc-cache)")

        _FONT_CACHE = font_install_path
        return font_install_path

    except Exception as e:
        logger.logger.error(f"[CAPTIONS] Ошибка при установке шрифта: {e}")
        logger.logger.warning("[CAPTIONS] Будет использован шрифт по умолчанию")
        return None
# Epsilon for time comparisons (inclusive start, exclusive end)
EPS_TIME = 1e-6
# pure helper for unit tests and reuse
def _compute_bottom_margin_px(frame_h: int, bottom_offset_pct: int) -> int:
    try:
        # positioning via bottom_offset_pct
        pct = int(bottom_offset_pct)
    except Exception:
        pct = 0
    pct = max(0, min(pct, 100))
    try:
        fh = int(frame_h)
    except Exception:
        fh = 0
    return int(fh * pct / 100)
# Function to format time in SRT format
def format_time(seconds):
    milliseconds = int((seconds - math.floor(seconds)) * 1000)
    seconds = int(math.floor(seconds))
    minutes = seconds // 60
    seconds %= 60
    hours = minutes // 60
    minutes %= 60
    return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"

# Function to format time in ASS format (hours:mm:ss.cc)
def format_time_ass(seconds):
    centiseconds = int((seconds - math.floor(seconds)) * 100)
    seconds = int(math.floor(seconds))
    minutes = seconds // 60
    seconds %= 60
    hours = minutes // 60
    minutes %= 60
    return f"{hours:d}:{minutes:02d}:{seconds:02d}.{centiseconds:02d}"

# Display text sanitizer: remove punctuation for visual layer only
def sanitize_display_text(s: str) -> str:
    """
    Remove all Unicode punctuation characters from the given string for display purposes.
    Explicitly removes the ellipsis character '…' and any sequences of two or more dots
    ('..', '...', etc.). Does not alter spaces, letters, digits, emoji or the case of
    characters. Whitespace is not normalized (no trimming/collapsing).
    """
    try:
        if not isinstance(s, str):
            return str(s)
        if s == "":
            return ""
    except Exception:
        return str(s)

    # 1) Explicit removals: ellipsis and runs of >=2 dots (keep surrounding spaces intact)
    tmp = s.replace("…", "")
    out_chars = []
    i = 0
    n = len(tmp)
    while i < n:
        ch = tmp[i]
        if ch == ".":
            j = i
            while j < n and tmp[j] == ".":
                j += 1
            if (j - i) >= 2:
                # Skip entire run of dots
                i = j
                continue
            # Single dot will be removed by punctuation filtering below
        out_chars.append(ch)
        i += 1
    tmp2 = "".join(out_chars)

    # 2) Filter out any Unicode punctuation (categories starting with 'P')
    cleaned = "".join(ch for ch in tmp2 if not unicodedata.category(ch).startswith("P"))
    return cleaned

# Function to escape special characters for ASS format
def escape_ass_text(text: str) -> str:
    """
    Escape special characters in ASS subtitle text.
    ASS requires escaping of \, {, } characters.
    """
    if not isinstance(text, str):
        text = str(text)
    return text.replace('\\', '\\\\').replace('{', '\\{').replace('}', '\\}')

# Function to generate ASS content (more compatible with ffmpeg filter)
def generate_ass_content(transcriptions, start_time, end_time, style_cfg=None, video_w=None, video_h=None):
    """
    Генерирует ASS-контент с использованием найденного шрифта Montserrat-Bold.
    """
    logger.logger.debug("[CAPTIONS] Начало генерации ASS-контента субтитров")

    # Получаем путь к шрифту Montserrat-Bold
    font_path = find_or_install_font()
    font_family = "Montserrat-Bold"  # Имя семейства шрифта для ASS

    if font_path:
        logger.logger.debug(f"[CAPTIONS] ASS генерация использует шрифт: {font_path}")
    else:
        logger.logger.warning("[CAPTIONS] Предупреждение: шрифт Montserrat-Bold не найден, ASS может использовать системный шрифт по умолчанию")
    """
    Генерирует ASS-контент. Обратная совместимость сохранена:
    - При style_cfg is None — используются прежние константы (PlayRes 384x720, Fontsize=36, цвета/отступы как раньше).
    - При наличии style_cfg — применяются параметры из конфигурации:
      font_size, base_color, shadow (offset -> Shadow, color -> BackColour), letter_spacing -> Spacing,
      позиционирование Alignment/MarginV через position.mode и bottom_offset_pct.
    """

    # Локальный helper: HEX (#RRGGBB или #RRGGBBAA) -> &HAABBGGRR
    def _hex_to_ass_color(hex_str: str, default: str = "#FFFFFFFF") -> str:
        try:
            s = str(hex_str).strip()
            if not s.startswith("#"):
                raise ValueError("No #")
            s = s[1:]
            if len(s) == 6:
                rr, gg, bb = s[0:2], s[2:4], s[4:6]
                aa = "00"  # 00 — непрозрачно в ASS
            elif len(s) == 8:
                rr, gg, bb, aa = s[0:2], s[2:4], s[4:6], s[6:8]
            else:
                raise ValueError("Bad length")
            # Формат &HAABBGGRR
            return f"&H{aa}{bb}{gg}{rr}"
        except Exception:
            # На некорректный ввод — вернуть default
            if default != hex_str:
                return _hex_to_ass_color(default, "#FFFFFFFF")
            # Подстраховка (белый непрозрачный)
            return "&H00FFFFFF"

    # Дефолтные значения (как в предыдущей реализации)
    use_style = style_cfg is not None
    default_play_x, default_play_y = 384, 720
    play_res_x = default_play_x
    play_res_y = default_play_y
    if use_style and isinstance(video_w, (int, float)) and isinstance(video_h, (int, float)) and video_w > 0 and video_h > 0:
        # Используем реальный размер видео только если style_cfg задан (чтобы не менять старое поведение)
        play_res_x = int(video_w)
        play_res_y = int(video_h)

    font_size_default = 38
    font_size = font_size_default
    primary_colour = "&H00FFFFFF"  # белый
    outline_colour = "&H00000000"  # чёрный
    back_colour = "&H70000000"     # как было в коде
    outline = 2
    shadow_val = 1
    spacing_val = 0
    alignment = 2  # bottom-center
    margin_l = 10
    margin_r = 10
    margin_v = 30  # как было

    if use_style:
        # Font size
        try:
            fs = int(getattr(style_cfg, "font_size_px", font_size_default))
            font_size = max(20, min(60, fs))
        except Exception:
            font_size = font_size_default

        # Primary colour (base_color)
        try:
            primary_colour = _hex_to_ass_color(getattr(style_cfg, "base_color", "#FFFFFF"))
        except Exception:
            primary_colour = "&H00FFFFFF"

        # Back colour from shadow.color
        try:
            shadow_cfg = getattr(style_cfg, "shadow", None)
            if shadow_cfg:
                back_colour = _hex_to_ass_color(getattr(shadow_cfg, "color", "#00000080"))
                # Shadow offset (ASS поддерживает только интенсивность)
                sx = int(getattr(shadow_cfg, "x_px", 2) or 0)
                sy = int(getattr(shadow_cfg, "y_px", 2) or 0)
                shadow_int = max(sx, sy)
                try:
                    shadow_val = max(1, min(4, int(round(shadow_int))))
                except Exception:
                    shadow_val = 1
        except Exception:
            pass

        # Letter spacing
        try:
            spacing_val = int(round(float(getattr(style_cfg, "letter_spacing_px", 0) or 0)))
        except Exception:
            spacing_val = 0

        # Alignment/MarginV
        # positioning via bottom_offset_pct and center_offset_pct
        # Alignment mapping: safe_bottom→2, center→5
        try:
            position = getattr(style_cfg, "position", None)
            mode = getattr(position, "mode", "safe_bottom") if position else "safe_bottom"
            bottom_offset_pct = int(getattr(position, "bottom_offset_pct", 22)) if position else 22
            center_offset_pct = int(getattr(position, "center_offset_pct", 12)) if position else 12
            if mode == "center":
                alignment = 5  # middle-center
                # Для режима center рассчитываем margin_v как смещение ниже центра
                try:
                    center_y = play_res_y // 2
                    offset_px = int(play_res_y * center_offset_pct / 100.0)
                    margin_v = play_res_y - (center_y + offset_px)  # Расстояние от низа до позиции текста
                except Exception:
                    margin_v = play_res_y // 2 - 50  # Резервное значение
            else:
                alignment = 2  # bottom-center
                try:
                    margin_v = int(play_res_y * float(bottom_offset_pct) / 100.0)
                except Exception:
                    margin_v = 30
        except Exception:
            pass

    ass_content = (
        f"[Script Info]\n"
        f"Title: Auto-generated by AI-Youtube-Shorts-Generator\n"
        f"ScriptType: v4.00+\n"
        f"PlayResX: {play_res_x}\n"
        f"PlayResY: {play_res_y}\n"
        f"WrapStyle: 0\n"
        f"ScaledBorderAndShadow: yes\n"
        f"\n"
        f"[V4+ Styles]\n"
        f"Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, "
        f"OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, "
        f"ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, "
        f"MarginL, MarginR, MarginV, Encoding\n"
        f"Style: Default,{font_family},{font_size},{primary_colour},&H000000FF,{outline_colour},{back_colour},"
        f"-1,0,0,0,100,100,{spacing_val},0,1,{outline},{shadow_val},{alignment},{margin_l},{margin_r},{margin_v},1\n"
        f"Style: Fallback,{font_family},{font_size},{primary_colour},&H000000FF,{outline_colour},{back_colour},"
        f"-1,0,0,0,100,100,{spacing_val},0,1,{outline},{shadow_val},{alignment},{margin_l},{margin_r},{margin_v},1\n"
        f"\n"
        f"[Events]\n"
        f"Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n"
    )
    # Determine whether to strip punctuation for display (default True if absent)
    try:
        _cfg = get_config()
        _strip_punct = bool(getattr(_cfg.captions, "strip_punctuation", True))
        _align_to_audio = bool(getattr(_cfg.captions, "align_to_audio", True))
    except Exception:
        _strip_punct = True
        _align_to_audio = True

    processed_segments = 0
    for segment in transcriptions:
        text, seg_start, seg_end = segment
        if str(text).strip() == '[*]':
            continue

        # Include if there is any overlap with [start_time, end_time) using EPS
        try:
            s0 = float(seg_start)
            s1 = float(seg_end)
        except Exception:
            s0 = seg_start
            s1 = seg_end

        if (s1 > start_time + EPS_TIME) and (s0 < end_time - EPS_TIME):
            # Clip to window
            clip_start = max(s0, start_time)
            clip_end = min(s1, end_time)

            # Relative times from window start, enforcing [start, end)
            relative_start = max(0.0, clip_start - start_time)
            relative_end = max(relative_start, clip_end - start_time)

            t_raw = str(text)
            sanitized = sanitize_display_text(t_raw) if _strip_punct else t_raw
            line_text = sanitized.strip().upper()
            ass_content += (
                f"Dialogue: 0,{format_time_ass(relative_start)},{format_time_ass(relative_end)},"
                f"Default,,0,0,0,,{escape_ass_text(line_text)}\\N"
            )
            processed_segments += 1

    logger.logger.debug(f"[CAPTIONS] Обработано {processed_segments} сегментов для ASS-контента")
    logger.logger.debug(f"[CAPTIONS] Завершена генерация ASS-контента, размер: {len(ass_content)} символов")
    return ass_content

# Function to burn captions using FFmpeg
def burn_captions(vertical_video_path, audio_source_path, transcriptions, start_time, end_time, output_path, style_cfg=None):
    """Burns captions onto the vertical video using audio from the source segment."""
    logger.logger.info(f"[CAPTIONS] Начало наложения субтитров на видео: {os.path.basename(output_path)}")

    # Create temp ASS file in the same directory as output to avoid path issues
    output_dir = os.path.dirname(os.path.abspath(output_path))
    temp_ass_filename = f"temp_subtitles_{os.path.basename(output_path).replace('.', '_')}.ass"
    temp_ass_path = os.path.join(output_dir, temp_ass_filename)

    # Пытаемся получить реальные размеры видео (для PlayRes при наличии style_cfg)
    vw, vh = None, None
    try:
        cap = cv2.VideoCapture(vertical_video_path)
        if cap.isOpened():
            vw = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            vh = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            logger.logger.debug(f"[CAPTIONS] Размеры входного видео: {vw}x{vh}")
        if cap:
            cap.release()
    except Exception as e:
        vw, vh = None, None
        logger.logger.warning(f"[CAPTIONS] Не удалось получить размеры видео {vertical_video_path}: {e}")

    try:
        # Create an ASS subtitle file (more compatible than SRT for styling)
        logger.logger.info("[CAPTIONS] Генерация ASS-контента субтитров")
        ass_content = generate_ass_content(transcriptions, start_time, end_time, style_cfg=style_cfg, video_w=vw, video_h=vh)

        dialogue_count = ass_content.count("Dialogue:")
        logger.logger.debug(f"[CAPTIONS] Сгенерировано {dialogue_count} диалогов в ASS-контенте")

        if not dialogue_count:
            logger.logger.warning("[CAPTIONS] Не найдено релевантных транскрипций для выделенного фрагмента. Создание видео без субтитров.")
            # Need to add audio even if no captions are burned
            ffmpeg_command_no_subs = [
                'ffmpeg',
                '-i', vertical_video_path, # Silent video input
                '-i', audio_source_path,  # Audio source input
                '-map', '0:v:0', # Video from input 0
                '-map', '1:a:0', # Audio from input 1
                '-c:v', 'copy',  # Copy video stream (faster if no filter applied)
                '-c:a', 'aac',   # Re-encode audio
                '-b:a', '128k',
                '-shortest',    # Ensure output duration matches shortest input
                '-y',
                output_path
            ]
            logger.logger.info("[CAPTIONS] Запуск FFmpeg команды (без субтитров, добавление аудио)")
            logger.logger.debug(f"[CAPTIONS] Команда FFmpeg: {' '.join(ffmpeg_command_no_subs)}")
            process = subprocess.run(ffmpeg_command_no_subs, check=True, capture_output=True, text=True)
            logger.logger.info(f"[CAPTIONS] Успешно объединено аудио в: {output_path}")
            return True # Return true as the operation (adding audio) succeeded

        # Write the ASS content to the output directory with UTF-8-BOM for FFmpeg compatibility
        with open(temp_ass_path, 'w', encoding='utf-8-sig') as f:
            f.write(ass_content)

        logger.logger.info(f"[CAPTIONS] Сгенерирован файл субтитров: {temp_ass_path}")

        # Verify ASS file was created and has content
        if not os.path.exists(temp_ass_path):
            logger.logger.error(f"[CAPTIONS] Ошибка: файл ASS не существует: {temp_ass_path}")
            return False
        file_size = os.path.getsize(temp_ass_path)
        if file_size == 0:
            logger.logger.error(f"[CAPTIONS] Ошибка: файл ASS пустой: {temp_ass_path}")
            return False
        logger.logger.debug(f"[CAPTIONS] Файл ASS создан успешно: {temp_ass_path} (размер: {file_size} байт, кодировка: UTF-8-BOM)")
        # Log first few lines of ASS content for debugging
        try:
            with open(temp_ass_path, 'r', encoding='utf-8-sig') as f:
                lines = f.readlines()[:10]  # First 10 lines
                logger.logger.debug("[CAPTIONS] Предварительный просмотр содержимого ASS файла:")
                for i, line in enumerate(lines, 1):
                    logger.logger.debug(f"[CAPTIONS]   {i}: {line.strip()}")
                if len(lines) == 10:
                    logger.logger.debug("[CAPTIONS]   ... (обрезано)")
        except Exception as e:
            logger.logger.warning(f"[CAPTIONS] Предупреждение: не удалось прочитать ASS файл для логирования: {e}")

        # Use absolute path for better compatibility with FFmpeg
        ass_path_escaped = temp_ass_path.replace("'", "\\'")

        # FFmpeg command using two inputs and mapping streams
        ffmpeg_command = [
            'ffmpeg',
            '-i', vertical_video_path,  # Input 0: Vertically cropped video (silent)
            '-i', audio_source_path,   # Input 1: Original segment (with audio)
            # Use properly escaped path for subtitles file
            '-filter_complex', f"[0:v]ass='{ass_path_escaped}'[video_out]",
            '-map', '[video_out]',     # Map the filtered video stream
            '-map', '1:a:0',           # Map the audio stream from input 1
            '-c:v', 'libx264',
            '-crf', '23',
            '-preset', 'medium',
            '-c:a', 'aac',           # Re-encode audio (required when filtering/mapping)
            '-b:a', '128k',
            '-shortest',             # Finish encoding when the shortest input ends
            '-y',
            output_path
        ]

        # Print the command for debugging
        logger.logger.info("[CAPTIONS] Запуск FFmpeg команды (наложение субтитров и добавление аудио)")
        logger.logger.debug(f"[CAPTIONS] Путь к ASS файлу для FFmpeg: {ass_path_escaped}")
        cmd_string = ' '.join([str(arg) for arg in ffmpeg_command])
        logger.logger.debug(f"[CAPTIONS] Команда FFmpeg: {cmd_string}")

        # Run FFmpeg with the new command
        try:
            process = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)
            logger.logger.info(f"[CAPTIONS] Успешно наложены субтитры и добавлено аудио в: {output_path}")
            return True
        except subprocess.CalledProcessError as e:
            # Check if error is related to ASS processing
            stderr_str = str(e.stderr).lower()
            if "ass=" in stderr_str or "subtitle" in stderr_str or "fopen" in stderr_str:
                logger.logger.warning("[CAPTIONS] Предупреждение: обработка ASS субтитров не удалась. Попытка фолбэка без субтитров.")
                logger.logger.debug(f"[CAPTIONS] FFmpeg stderr: {e.stderr}")
                # Fallback: create video without subtitles
                fallback_command = [
                    'ffmpeg',
                    '-i', vertical_video_path,
                    '-i', audio_source_path,
                    '-map', '0:v:0',
                    '-map', '1:a:0',
                    '-c:v', 'copy',
                    '-c:a', 'aac',
                    '-b:a', '128k',
                    '-shortest',
                    '-y',
                    output_path
                ]
                try:
                    subprocess.run(fallback_command, check=True, capture_output=True, text=True)
                    logger.logger.info(f"[CAPTIONS] Успешно создано видео без субтитров: {output_path}")
                    return True
                except subprocess.CalledProcessError as fallback_e:
                    logger.logger.error(f"[CAPTIONS] Фолбэк также не удался: {fallback_e}")
                    logger.logger.debug(f"[CAPTIONS] Fallback stderr: {fallback_e.stderr}")
                    return False
            else:
                # Re-raise if not ASS-related
                raise

    except subprocess.CalledProcessError as e:
        logger.logger.error(f"[CAPTIONS] Ошибка запуска FFmpeg: {e}")
        logger.logger.debug(f"[CAPTIONS] FFmpeg stdout: {e.stdout}")
        logger.logger.debug(f"[CAPTIONS] FFmpeg stderr: {e.stderr}")
        # Check if error is related to ASS processing
        if "ass=" in str(e.stderr) or "subtitle" in str(e.stderr).lower():
            logger.logger.warning("[CAPTIONS] Предупреждение: обработка ASS субтитров не удалась. Возможные проблемы с шрифтами или форматом ASS.")
        return False
    except Exception as e:
        logger.logger.error(f"[CAPTIONS] Произошла ошибка при наложении субтитров: {e}")
        return False
    finally:
        # Always clean up the subtitle file
        if os.path.exists(temp_ass_path):
            try:
                os.remove(temp_ass_path)
                logger.logger.debug(f"[CAPTIONS] Удален временный файл субтитров: {temp_ass_path}")
            except Exception as e:
                logger.logger.warning(f"[CAPTIONS] Предупреждение: не удалось удалить временный файл субтитров: {e}")


# --- Word-Level Animation Helpers ---

def find_active_segment_and_word(transcription_result, current_time):
    """Finds the segment and word active at the current time."""
    active_segment = None
    active_word_index_in_segment = -1 # Index within the segment's word list

    for segment in transcription_result.get("segments", []):
        # Use segment boundaries with EPS: start inclusive, end exclusive
        try:
            seg_start = float(segment['start'])
            seg_end = float(segment['end'])
        except Exception:
            seg_start = segment['start']
            seg_end = segment['end']
        if (seg_start - EPS_TIME) <= current_time < (seg_end - EPS_TIME):
            active_segment = segment
            # Find the specific word within this segment based on word timings
            for i, word_info in enumerate(segment.get("words", [])):
                # Ensure word timings exist before comparing
                if 'start' in word_info and 'end' in word_info:
                    try:
                        w_start = float(word_info['start'])
                        w_end = float(word_info['end'])
                    except Exception:
                        w_start = word_info['start']
                        w_end = word_info['end']
                    if (w_start - EPS_TIME) <= current_time < (w_end - EPS_TIME):
                        active_word_index_in_segment = i
                        break # Found the active word
            # If no specific word is active but the segment is, keep active_segment
            # active_word_index_in_segment will remain -1 or the found index
            break # Found the active segment

    return active_segment, active_word_index_in_segment


# --- Main Animation Function ---

def animate_captions(vertical_video_path, audio_source_path, transcription_result, output_path, style_cfg=None, highlight_meta=None):
    """Creates a video with word-by-word highlighted captions based on segments.
    - tone/keywords heuristic via optional highlight_meta for accent coloring
    """
    logger.logger.info(f"[CAPTIONS] Начало анимации субтитров для: {os.path.basename(output_path)}")

    temp_animated_video = output_path + "_temp_anim.mp4"
    success = False
    cap = None
    out = None

    # Локальный helper: HEX -> RGBA (tuple)
    def _hex_to_rgba(hex_str: str, default=(255, 255, 0, 255)):
        try:
            s = str(hex_str).strip()
            if not s.startswith("#"):
                raise ValueError("No #")
            s = s[1:]
            if len(s) == 6:
                rr, gg, bb = int(s[0:2], 16), int(s[2:4], 16), int(s[4:6], 16)
                aa = 255
            elif len(s) == 8:
                rr, gg, bb, aa = int(s[0:2], 16), int(s[2:4], 16), int(s[4:6], 16), int(s[6:8], 16)
            else:
                raise ValueError("Bad length")
            return (rr, gg, bb, aa)
        except Exception:
            return default

    # Посимвольная отрисовка с letter-spacing (упрощение для одной строки)
    def _draw_text_with_spacing(draw_obj, start_xy, text, font, fill, stroke_width=0, stroke_fill=None, spacing_px=0):
        x, y = start_xy
        for idx, ch in enumerate(text):
            draw_obj.text((x, y), ch, font=font, fill=fill, stroke_width=stroke_width, stroke_fill=stroke_fill)
            # Используем getlength для более точного измерения ширины символа
            try:
                ch_w = font.getlength(ch)
            except AttributeError:  # Fallback for older/different PIL versions
                bbox = font.getbbox(ch) if hasattr(font, 'getbbox') else draw_obj.textbbox((0, 0), ch, font=font)
                ch_w = (bbox[2] - bbox[0]) if bbox else font.getsize(ch)[0]
            x += ch_w + (spacing_px if idx < len(text) - 1 else 0)

    # Оценка ширины текста с letter-spacing
    def _measure_text_width(draw_obj, text, font, spacing_px=0):
        if not text:
            return 0
        try:
            # Предпочтительный, более точный метод
            base_width = font.getlength(text)
            total_spacing = (len(text) - 1) * spacing_px if len(text) > 1 else 0
            return base_width + total_spacing
        except AttributeError:
            # Fallback для старых версий PIL или шрифтов без getlength
            total = 0
            for idx, ch in enumerate(text):
                bbox = font.getbbox(ch) if hasattr(font, 'getbbox') else draw_obj.textbbox((0, 0), ch, font=font)
                ch_w = (bbox[2] - bbox[0]) if bbox else font.getsize(ch)[0]
                total += ch_w
            total += (len(text) - 1) * spacing_px if len(text) > 1 else 0
            return total

    try:
        # --- Font Setup (Pillow) ---
        font_size = 38  # default legacy
        if style_cfg is not None:
            try:
                font_size = int(getattr(style_cfg, "font_size_px", font_size) or font_size)
            except Exception:
                pass

        # Используем универсальную функцию поиска шрифта
        font_path = find_or_install_font()
        font = None
        try:
            if font_path and os.path.exists(font_path):
                font = ImageFont.truetype(font_path, font_size)
                logger.logger.debug(f"[CAPTIONS] Успешно загружен шрифт: {font_path}")
            else:
                logger.logger.warning("[CAPTIONS] Файл шрифта не найден. Используется шрифт PIL по умолчанию.")
                font = ImageFont.load_default()
        except Exception as e:
            logger.logger.warning(f"[CAPTIONS] Предупреждение: не удалось загрузить TTF шрифт: {e}. Используется шрифт PIL по умолчанию.")
            font = ImageFont.load_default()

        # --- Pre-filter segments ---
        original_segments = transcription_result.get("segments", [])
        filtered_segments = [seg for seg in original_segments if seg.get('text', '').strip() != '[*]']
        if not filtered_segments:
            print("Warning: No non-[*] segments found in transcription. Captions might be empty.")

        # Update the transcription_result to use filtered segments for further processing
        transcription_result_filtered = transcription_result.copy() # Avoid modifying original dict directly if reused
        transcription_result_filtered['segments'] = filtered_segments

        logger.logger.info("[CAPTIONS] Запуск генерации анимированных субтитров (статическое окно/подсветка)")
        logger.logger.debug(f"[CAPTIONS] Отфильтровано сегментов: {len(filtered_segments)} из {len(original_segments)}")

        cap = cv2.VideoCapture(vertical_video_path)
        if not cap.isOpened():
            logger.logger.error(f"[CAPTIONS] Ошибка: невозможно открыть видеофайл {vertical_video_path}")
            return False

        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps <= 0: # Handle zero or negative fps
              logger.logger.error(f"[CAPTIONS] Ошибка: некорректный FPS видео ({fps}), невозможно рассчитать время")
              cap.release() # Release resource
              return False
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        logger.logger.debug(f"[CAPTIONS] Свойства входного видео: {width}x{height} @ {fps:.2f}fps, кадров: {total_frames}")

        # --- Text Styling (Pillow) ---
        # Base color
        text_rgba = _hex_to_rgba(getattr(style_cfg, "base_color", "#FFFF00FF") if style_cfg else "#FFFF00FF", (255, 255, 0, 255))
        stroke_color_rgb = (0, 0, 0)    # Black outline (legacy)
        stroke_width = 1                # Outline width in pixels

        # accent color mapping (palette from style_cfg or defaults)
        tone_color_rgba = None
        kw_set = set()
        if style_cfg is not None and highlight_meta:
            # «accent color mapping»
            try:
                # Prepare palette
                default_palette = {"urgency": "#FFD400", "drama": "#FF3B30", "positive": "#34C759"}
                pal_obj = getattr(style_cfg, "accent_palette", None)
                if pal_obj:
                    palette = {
                        "urgency": getattr(pal_obj, "urgency", default_palette["urgency"]),
                        "drama": getattr(pal_obj, "drama", default_palette["drama"]),
                        "positive": getattr(pal_obj, "positive", default_palette["positive"]),
                    }
                else:
                    palette = default_palette
                tone = str(highlight_meta.get("tone", "neutral") or "neutral")
                if tone in ("urgency", "drama", "positive"):
                    tone_hex = palette.get(tone)
                    if isinstance(tone_hex, str):
                        tone_color_rgba = _hex_to_rgba(tone_hex, text_rgba)
                # Build keyword set (lowercase)
                kws = highlight_meta.get("keywords", []) or []
                if isinstance(kws, (list, tuple)):
                    kw_set = {str(k).lower() for k in kws if isinstance(k, str)}
            except Exception:
                tone_color_rgba = None
                kw_set = set()

        # Shadow config
        sx = sy = 0
        shadow_rgba = (0, 0, 0, 128)
        if style_cfg is not None:
            sh = getattr(style_cfg, "shadow", None)
            if sh:
                try:
                    sx = int(getattr(sh, "x_px", 2) or 0)
                    sy = int(getattr(sh, "y_px", 2) or 0)
                except Exception:
                    sx = sy = 0
                shadow_rgba = _hex_to_rgba(getattr(sh, "color", "#00000080"), (0, 0, 0, 128))

        # Letter-spacing
        letter_spacing_px = 0
        if style_cfg is not None:
            try:
                letter_spacing_px = int(round(float(getattr(style_cfg, "letter_spacing_px", 0) or 0)))
            except Exception:
                letter_spacing_px = 0

        # Fade config from AppConfig.captions
        try:
            _cfg_fade = get_config()
            _cap_cfg = getattr(_cfg_fade, "captions", None)
            fade_in_seconds = float(getattr(_cap_cfg, "fade_in_seconds", 0.15) or 0.15) if _cap_cfg else 0.15
            fade_out_seconds = float(getattr(_cap_cfg, "fade_out_seconds", 0.12) or 0.12) if _cap_cfg else 0.12
        except Exception:
            fade_in_seconds = 0.15
            fade_out_seconds = 0.12

        # Positioning
        position_mode = "safe_bottom"
        bottom_offset_pct = 22
        center_offset_pct = 12
        boundary_padding_px = 10
        if style_cfg is not None:
            pos = getattr(style_cfg, "position", None)
            if pos:
                position_mode = getattr(pos, "mode", "safe_bottom")
                try:
                    bottom_offset_pct = int(getattr(pos, "bottom_offset_pct", 22))
                except (ValueError, TypeError):
                    bottom_offset_pct = 22
                try:
                    center_offset_pct = int(getattr(pos, "center_offset_pct", 12))
                except (ValueError, TypeError):
                    center_offset_pct = 12
                try:
                    boundary_padding_px = int(getattr(pos, "boundary_padding_px", 10))
                except (ValueError, TypeError):
                    boundary_padding_px = 10

        # Emoji config and font (best-effort)
        emoji_enabled = False
        emoji_list_prepared = []
        emoji_max = 0
        emoji_window_s = 1.0  # emoji timing window ~1s from segment start
        emoji_font_size = max(8, int(font_size * 0.9))
        emoji_font = None

        if style_cfg is not None and isinstance(highlight_meta, dict):
            try:
                em_cfg = getattr(style_cfg, "emoji", None)
                emoji_enabled = bool(getattr(em_cfg, "enabled", False)) if em_cfg else False
                if emoji_enabled:
                    raw = list(highlight_meta.get("emojis", []) or [])
                    # дополнительная страховка по лимиту
                    emoji_max = int(getattr(em_cfg, "max_per_short", 0) or 0)
                    if emoji_max > 0:
                        emoji_list_prepared = [str(x) for x in raw if isinstance(x, str)][:emoji_max]
                    else:
                        emoji_list_prepared = []
            except Exception:
                emoji_enabled = False
                emoji_list_prepared = []

        def _load_emoji_font(sz: int):
            # emoji font loading (best-effort, platform paths)
            candidates = [
                "C:/Windows/Fonts/seguiemj.ttf",                        # Windows
                "/usr/share/fonts/truetype/noto/NotoColorEmoji.ttf",    # Linux
                "/System/Library/Fonts/Apple Color Emoji.ttc",          # macOS
            ]
            for pth in candidates:
                try:
                    if os.path.exists(pth):
                        return ImageFont.truetype(pth, sz)
                except Exception:
                    continue
            return None

        if emoji_enabled:
            try:
                emoji_font = _load_emoji_font(emoji_font_size)
            except Exception:
                emoji_font = None

        def _apply_emojis(base_img, start_x_val, total_w_val, start_y_val):
            """
            Отрисовать эмодзи на отдельном RGBA-слое и скомпозитить поверх кадра.
            - emoji: heuristics and placement
            - emoji font loading (best-effort, platform paths)
            - emoji timing window ~1s from segment start
            - effects: "pulse" / "shiny" в первые 0.8с от старта сегмента, затем статика до 1.0с
            """
            try:
                if not (emoji_enabled and emoji_list_prepared):
                    return base_img

                # easing helpers for emoji
                def _clamp01(v):
                    try:
                        v = float(v)
                    except Exception:
                        return 0.0
                    if v < 0.0:
                        return 0.0
                    if v > 1.0:
                        return 1.0
                    return v

                def _ease_out_cubic(t):
                    # easing: easeOutCubic
                    return 1.0 - (1.0 - float(t)) ** 3

                def _ease_in_out_sine(t):
                    # easing: easeInOutSine
                    return 0.5 * (1.0 - math.cos(math.pi * float(t)))

                def _lerp(a, b, t):
                    return a + (b - a) * float(t)

                # Время сегмента для окна эффекта
                seg_t0 = 0.0
                if isinstance(active_segment, dict):
                    try:
                        seg_t0 = float(active_segment.get("start", 0.0) or 0.0)
                    except Exception:
                        seg_t0 = 0.0

                # Учитываем границы сегмента с EPS и масштабируем окно показа
                seg_t1 = seg_t0
                if isinstance(active_segment, dict):
                    try:
                        seg_t1 = float(active_segment.get("end", seg_t0) or seg_t0)
                    except Exception:
                        seg_t1 = seg_t0
                # Ограничение по границам сегмента [start, end)
                if not (seg_t0 - EPS_TIME <= current_time < seg_t1 - EPS_TIME):
                    return base_img

                seg_dur = max(0.0, seg_t1 - seg_t0)
                emoji_window_dyn = min(1.0, 0.3 + 0.4 * seg_dur)

                dt = current_time - seg_t0
                if not (0.0 <= dt <= emoji_window_dyn):
                    # Вне окна показа — не отображаем эмодзи
                    return base_img

                # Нормированное время эффекта
                effect_duration = 0.8
                u_raw = dt / (effect_duration if effect_duration > 0 else 1e-6)
                u = _clamp01(u_raw)

                # Чтение стиля эмодзи
                em_style = "none"
                if style_cfg is not None:
                    try:
                        em_cfg = getattr(style_cfg, "emoji", None)
                        em_style = str(getattr(em_cfg, "style", "none") or "none").lower() if em_cfg else "none"
                    except Exception:
                        em_style = "none"

                overlay_em = Image.new("RGBA", (width, height), (0, 0, 0, 0))
                draw_em = ImageDraw.Draw(overlay_em)
                dx = int(font_size * 0.6)
                x_cursor = int(start_x_val + total_w_val + dx)
                y_draw = int(start_y_val)
                used_font = emoji_font if emoji_font else font  # fallback: текущий текстовый шрифт
                gap = max(2, int(emoji_font_size * 0.15))
                any_drawn = False

                for emo in emoji_list_prepared:
                    try:
                        # Базовые измерения
                        bbox = draw_em.textbbox((0, 0), emo, font=used_font)
                        w_emo = (bbox[2] - bbox[0]) if bbox else used_font.getsize(emo)[0]
                        h_emo = (bbox[3] - bbox[1]) if (bbox and len(bbox) >= 4) else font_size

                        # Рендер эмодзи на отдельный слой
                        emoji_layer = Image.new("RGBA", (max(1, w_emo), max(1, h_emo)), (0, 0, 0, 0))
                        emoji_draw = ImageDraw.Draw(emoji_layer)
                        emoji_draw.text((0, 0), emo, font=used_font, fill=(255, 255, 255, 255))

                        scale = 1.0
                        alpha_mult = 1.0

                        if em_style == "pulse":
                            # emoji: pulse effect (scale + alpha)
                            if 0.0 < u < 1.0:
                                u_e = _ease_in_out_sine(u)
                                scale = 1.0 + 0.06 * math.sin(math.pi * u_e)
                                alpha_mult = 0.85 + 0.15 * u
                            else:
                                # Вне окна эффекта, но всё ещё в окне показа (0.8..1.0) — статика
                                scale = 1.0
                                alpha_mult = 1.0

                        elif em_style == "shiny":
                            # emoji: shiny effect (moving gloss stripe)
                            if 0.0 < u < 1.0:
                                # Блик поверх исходного размера
                                gloss_width = max(3, int(0.15 * emoji_layer.width))
                                gloss_alpha = 0.45
                                progress = _ease_out_cubic(u)
                                gloss_x = int(round(_lerp(-gloss_width, emoji_layer.width, progress)))

                                gloss_layer = Image.new("RGBA", emoji_layer.size, (0, 0, 0, 0))
                                gloss_draw = ImageDraw.Draw(gloss_layer)
                                x0 = max(0, gloss_x)
                                x1 = min(emoji_layer.width, gloss_x + gloss_width)
                                if x1 > x0:
                                    gloss_color = (255, 255, 255, int(round(255 * gloss_alpha)))
                                    # Вертикальная полоса блика
                                    gloss_draw.rectangle([x0, 0, x1, emoji_layer.height], fill=gloss_color)

                                emoji_layer = Image.alpha_composite(emoji_layer, gloss_layer)

                                # Лёгкий масштаб
                                scale = 0.98 + 0.02 * u
                                alpha_mult = 1.0
                            else:
                                scale = 1.0
                                alpha_mult = 1.0

                        # Масштабирование слоя эмодзи
                        if abs(scale - 1.0) > 1e-3:
                            new_w = max(1, int(round(emoji_layer.width * scale)))
                            new_h = max(1, int(round(emoji_layer.height * scale)))
                            emoji_layer = emoji_layer.resize((new_w, new_h), resample=Image.BICUBIC)

                        # Применение альфа-множителя
                        if alpha_mult < 0.999:
                            r, g, b, a = emoji_layer.split()
                            a = a.point(lambda v, am=alpha_mult: int(v * am))
                            emoji_layer = Image.merge("RGBA", (r, g, b, a))

                        # Компоновка
                        overlay_em.paste(emoji_layer, (x_cursor, y_draw), mask=emoji_layer)
                        x_cursor += w_emo + gap
                        any_drawn = True

                    except Exception:
                        # Тихий пропуск проблемного эмодзи
                        continue

                if any_drawn:
                    return Image.alpha_composite(base_img, overlay_em)
                return base_img
            except Exception:
                return base_img

        # --- Calculate Fixed Y Position (after getting height) ---
        font_ascent = 0 # Default if font fails
        if font:
            try:
                font_ascent, _ = font.getmetrics()
            except Exception:
                 print("Warning: Could not get font metrics.")
        # legacy defaults
        bottom_margin_legacy = 120
        fixed_top_y_legacy = height - bottom_margin_legacy - font_ascent

        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(temp_animated_video, fourcc, fps, (width, height))
        if not out.isOpened():
            logger.logger.error(f"[CAPTIONS] Ошибка: невозможно открыть VideoWriter для {temp_animated_video}")
            cap.release() # Release resource
            return False
        logger.logger.debug(f"[CAPTIONS] Создан VideoWriter для временного файла: {temp_animated_video}")

        # Read strip_punctuation flag from config (default True for backward compatibility)
        try:
            _cfg = get_config()
            strip_punct = bool(getattr(_cfg.captions, "strip_punctuation", True))
        except Exception:
            strip_punct = True

        frame_count = 0
        drawn_any_text = False
        logger.logger.debug("[CAPTIONS] Начало обработки кадров видео")
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            current_time = frame_count / fps

            # Find the segment and specific word active at this frame's time
            # Use the filtered transcription data
            active_segment, active_word_idx_in_segment = find_active_segment_and_word(transcription_result_filtered, current_time)

            # --- Get Words to Display (Max 2) ---
            words_to_display = ""
            window_words = []  # [(text, word_info)] for per-word animation window
            if active_segment:
                segment_words_list = active_segment.get('words', [])
                num_words_in_segment = len(segment_words_list)

                if 0 <= active_word_idx_in_segment < num_words_in_segment:
                    word1_info = segment_words_list[active_word_idx_in_segment]
                    # Fallback to 'word' if 'text' missing (compat)
                    word1_text = (word1_info.get('text') or word1_info.get('word') or '').strip()

                    # Skip if the primary word is [*]
                    if word1_text != '[*]':
                        # Очистка пунктуации (только для визуализации), затем верхний регистр
                        raw = word1_text
                        if strip_punct:
                            raw = sanitize_display_text(raw)
                        words_to_display = raw.upper()
                        window_words.append((words_to_display, word1_info))

            # --- Drawing Logic (Pillow - Max 2 words) ---
            # Draw only if we have a valid window and an active word within it
            if words_to_display and font: # Only draw if we have words and font loaded
                # Convert frame BGR OpenCV to RGB Pillow
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # При наличии style_cfg — поддерживаем RGBA + тень/альфа/letter-spacing,
                # иначе сохраняем прежнее поведение (совместимость).
                if style_cfg is None:
                    pil_image = Image.fromarray(frame_rgb)
                    draw = ImageDraw.Draw(pil_image)
                    text_bbox = draw.textbbox((0, 0), words_to_display, font=font)
                    text_width = text_bbox[2] - text_bbox[0] if text_bbox else 0
                    start_x = (width - text_width) // 2
                    fixed_top_y = fixed_top_y_legacy
                    # Draw the text (legacy)
                    draw.text(
                        (start_x, fixed_top_y),
                        words_to_display,
                        font=font,
                        fill=(255, 255, 0),
                        stroke_width=stroke_width,
                        stroke_fill=stroke_color_rgb
                    )
                    drawn_any_text = True
                    frame = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
                else:
                    # Расширенный путь с наложением RGBA + опциональная анимация per-word
                    base_rgb = Image.fromarray(frame_rgb).convert("RGBA")
                    anim_cfg = getattr(style_cfg, "animate", None)

                    # Segment fade-in/out alpha multiplier
                    seg_alpha_factor = 1.0
                    if active_segment:
                        try:
                            seg_start = float(active_segment.get("start", 0.0) or 0.0)
                            seg_end = float(active_segment.get("end", seg_start) or seg_start)
                            seg_dur = max(0.0, seg_end - seg_start)
                            fin = min(fade_in_seconds, 0.2 * seg_dur)
                            fout = min(fade_out_seconds, 0.15 * seg_dur)
                            t_seg = current_time - seg_start
                            fade_in_factor = 1.0 if fin <= EPS_TIME else _clamp01(t_seg / (fin if fin > 0.0 else 1.0))
                            time_to_end = max(0.0, seg_end - current_time)
                            fade_out_factor = 1.0 if fout <= EPS_TIME else _clamp01(time_to_end / (fout if fout > 0.0 else 1.0))
                            seg_alpha_factor = min(fade_in_factor, fade_out_factor)
                        except Exception:
                            seg_alpha_factor = 1.0

                    # Easing helpers + clamp (локальные)
                    def _clamp01(v):
                        try:
                            v = float(v)
                        except Exception:
                            return 0.0
                        if v < 0.0:
                            return 0.0
                        if v > 1.0:
                            return 1.0
                        return v

                    def _ease_out_cubic(t):
                        # easing: easeOutCubic
                        return 1.0 - (1.0 - float(t)) ** 3

                    # Общие измерения
                    tmp_draw = ImageDraw.Draw(Image.new("RGB", (1, 1)))
                    # Высота строки по bbox (вертикаль не зависит от spacing)
                    text_bbox = tmp_draw.textbbox((0, 0), words_to_display, font=font)
                    text_h = (text_bbox[3] - text_bbox[1]) if text_bbox else font_size

                    # Позиционирование (стабилизированная Y-координата)
                    if position_mode == "center":
                        # Центрируем по вертикали, затем смещаем ниже центра (фиксированная позиция)
                        try:
                            offset_px = int(height * (center_offset_pct / 100.0))
                        except Exception:
                            offset_px = int(height * 0.12)  # 12% fallback
                        start_y = (height // 2) + offset_px  # Фиксированная позиция по центру
                    else:  # safe_bottom
                        try:
                            margin_y = _compute_bottom_margin_px(height, bottom_offset_pct or 22)
                        except Exception:
                            margin_y = 120
                        start_y = height - margin_y  # Фиксированная позиция от низа

                    # --- Горизонтальное позиционирование и clamping ---
                    
                    # Общая ширина текста для центрирования
                    total_text_w = 0
                    
                    if not anim_cfg and not window_words:
                         total_text_w = _measure_text_width(tmp_draw, words_to_display, font, spacing_px=letter_spacing_px)
                    elif window_words:
                        space_w_bbox = tmp_draw.textbbox((0, 0), " ", font=font)
                        space_w = (space_w_bbox[2] - space_w_bbox[0]) if space_w_bbox else max(1, font_size // 3)
                        word_widths_local = [
                            _measure_text_width(tmp_draw, wt, font, spacing_px=letter_spacing_px)
                            for wt, _ in window_words
                        ]
                        total_text_w = sum(word_widths_local) + (len(word_widths_local) - 1) * space_w if word_widths_local else 0
                    
                    start_x = (width - total_text_w) // 2
                    
                    # Применяем boundary_padding_px для clamping
                    padding = boundary_padding_px or 10
                    start_x = max(padding, min(start_x, width - total_text_w - padding))
                    # Для вертикального clamping учитываем высоту текста от фиксированной позиции
                    if position_mode == "center":
                        # Для center режима текст центрируется вокруг start_y
                        min_y = padding + text_h // 2
                        max_y = height - padding - text_h // 2
                        start_y = max(min_y, min(start_y, max_y))
                    else:  # safe_bottom
                        # Для safe_bottom текст идет вниз от start_y
                        start_y = max(padding, min(start_y, height - text_h - padding))

                    if not anim_cfg:
                        import re as _re_kc
                        overlay = Image.new("RGBA", (width, height), (0, 0, 0, 0))
                        draw_ov = ImageDraw.Draw(overlay)
                        
                        # Если window_words пуст, отрисовываем всю строку
                        if not window_words:
                            # Shadow
                            if sx != 0 or sy != 0 or (shadow_rgba and shadow_rgba[3] > 0):
                                _draw_text_with_spacing(draw_ov, (start_x + sx, start_y + sy), words_to_display, font, shadow_rgba, 0, None, letter_spacing_px)
                            # Main text
                            _draw_text_with_spacing(draw_ov, (start_x, start_y), words_to_display, font, text_rgba, stroke_width, stroke_color_rgb, letter_spacing_px)
                        else:
                            # Отрисовка по словам (для keyword coloring)
                            space_w_bbox = tmp_draw.textbbox((0, 0), " ", font=font)
                            space_w = (space_w_bbox[2] - space_w_bbox[0]) if space_w_bbox else max(1, font_size // 3)
                            word_widths = [_measure_text_width(tmp_draw, wt, font, spacing_px=letter_spacing_px) for wt, _ in window_words]
                            
                            # Shadow pass
                            if sx != 0 or sy != 0 or (shadow_rgba and shadow_rgba[3] > 0):
                                x_cursor = start_x
                                for idx, (w_text, _) in enumerate(window_words):
                                    _draw_text_with_spacing(draw_ov, (x_cursor + sx, start_y + sy), w_text, font, shadow_rgba, 0, None, letter_spacing_px)
                                    x_cursor += word_widths[idx] + (space_w if idx < len(word_widths) - 1 else 0)
                            
                            # Main pass
                            x_cursor = start_x
                            def _sanitize_kw(token: str) -> str: return _re_kc.sub(r"[^\wа-яё]+", "", token.lower())
                            for idx, (w_text, _) in enumerate(window_words):
                                use_rgba = text_rgba
                                if tone_color_rgba and kw_set and _sanitize_kw(w_text) in kw_set:
                                    use_rgba = tone_color_rgba
                                _draw_text_with_spacing(draw_ov, (x_cursor, start_y), w_text, font, use_rgba, stroke_width, stroke_color_rgb, letter_spacing_px)
                                x_cursor += word_widths[idx] + (space_w if idx < len(word_widths) - 1 else 0)

                        # Apply segment fade to overlay alpha
                        if seg_alpha_factor < 0.999:
                            r, g, b, a = overlay.split()
                            a = a.point(lambda v, am=seg_alpha_factor: int(v * am))
                            overlay = Image.merge("RGBA", (r, g, b, a))

                        composed = Image.alpha_composite(base_rgb, overlay)
                        composed = _apply_emojis(composed, start_x, total_text_w, start_y)
                        drawn_any_text = True
                        frame = cv2.cvtColor(np.array(composed.convert("RGB")), cv2.COLOR_RGB2BGR)
                    else:
                        # --- Анимация per-word ---
                        try:
                            # Параметры анимации с клиппингом/фолбэками
                            anim_type = str(getattr(anim_cfg, "type", "slide-up") or "slide-up")
                            if anim_type not in ("slide-up", "pop-in"):
                                anim_type = "slide-up"
                            try:
                                duration_s = float(getattr(anim_cfg, "duration_s", 0.35) or 0.35)
                            except Exception:
                                duration_s = 0.35
                            duration_s = max(0.2, min(0.5, duration_s))
                            easing_name = str(getattr(anim_cfg, "easing", "easeOutCubic") or "easeOutCubic")
                            try:
                                stagger_ms = int(getattr(anim_cfg, "per_word_stagger_ms", 0) or 0)
                            except Exception:
                                stagger_ms = 0
                            stagger_s = max(0, stagger_ms) / 1000.0

                            # Выбор функции easing
                            if easing_name == "easeOutCubic":
                                ease = _ease_out_cubic
                            else:
                                ease = lambda t: float(t)  # Линейный фолбэк

                            # Подготовка окна слов; если не собрали — рендер одной строкой
                            if not window_words:
                                if words_to_display:
                                    window_words = [(words_to_display, {"start": current_time})]

                            # Измеряем ширины слов и пробела
                            space_w_bbox = tmp_draw.textbbox((0, 0), " ", font=font)
                            space_w = (space_w_bbox[2] - space_w_bbox[0]) if space_w_bbox else max(1, font_size // 3)
                            word_widths = [
                                _measure_text_width(tmp_draw, wt, font, spacing_px=letter_spacing_px)
                                for wt, _ in window_words
                            ]
                            total_text_w = sum(word_widths) + (len(word_widths) - 1) * space_w if word_widths else 0
                            start_x = (width - total_text_w) // 2

                            # Базовый вертикальный смещающий оффсет для slide-up
                            offsetY0 = int(max(12, min(24, round(0.25 * font_size))))

                            x_cursor = start_x
                            any_drawn_local = False

                            for idx, (w_text, w_info) in enumerate(window_words):
                                # t0 — время начала слова (сек)
                                try:
                                    t0 = float(w_info.get("start", current_time) or current_time)
                                except Exception:
                                    t0 = current_time
                                # Stagger по индексу в текущем окне
                                t0_staggered = t0 + idx * stagger_s

                                # Прогресс анимации per-word
                                progress_raw = (current_time - t0_staggered) / (duration_s if duration_s > 0 else 1e-6)
                                progress = _clamp01(progress_raw)
                                final = ease(progress)  # easing

                                # Преобразования: pop-in / slide-up
                                if anim_type == "pop-in":
                                    # pop-in: scale 0.85->1.0, alpha = final, translateY=0
                                    scale = 0.85 + 0.15 * final
                                    translateY = 0
                                    alpha_mult = final * seg_alpha_factor
                                else:
                                    # slide-up: смещение снизу, лёгкий scale 0.98->1.0, alpha = final
                                    translateY = int(round((1.0 - final) * offsetY0))
                                    scale = 0.98 + 0.02 * final
                                    alpha_mult = final * seg_alpha_factor

                                # Если слово ещё не началось с учётом stagger — не показываем
                                if progress <= 0.0 or alpha_mult <= 0.0:
                                    x_cursor += word_widths[idx] + (space_w if idx < len(word_widths) - 1 else 0)
                                    continue

                                # Рендер слова на отдельном RGBA-слое
                                word_w = word_widths[idx]
                                # Паддинги под тень, чтобы не обрезать смещённую копию
                                pad_left = max(0, -sx)
                                pad_top = max(0, -sy)
                                pad_right = max(0, sx)
                                pad_bottom = max(0, sy)
                                wl_w = max(1, word_w + pad_left + pad_right)
                                wl_h = max(1, text_h + pad_top + pad_bottom)
                                word_layer = Image.new("RGBA", (wl_w, wl_h), (0, 0, 0, 0))
                                draw_wl = ImageDraw.Draw(word_layer)

                                # Тень (идёт теми же трансформациями)
                                if sx != 0 or sy != 0 or (shadow_rgba and shadow_rgba[3] > 0):
                                    _draw_text_with_spacing(
                                        draw_wl,
                                        (pad_left + sx, pad_top + sy),
                                        w_text,
                                        font=font,
                                        fill=shadow_rgba,
                                        stroke_width=0,
                                        stroke_fill=None,
                                        spacing_px=letter_spacing_px
                                    )

                                # Основной текст с «keyword-based coloring (fallback to base_color)»
                                def _sanitize_kw(token: str) -> str:
                                    import re as _re_kw
                                    return _re_kw.sub(r"[^\wа-яё]+", "", token.lower())

                                use_rgba = text_rgba
                                if tone_color_rgba and kw_set:
                                    token_norm = _sanitize_kw(w_text)
                                    if token_norm in kw_set:
                                        use_rgba = tone_color_rgba

                                _draw_text_with_spacing(
                                    draw_wl,
                                    (pad_left, pad_top),
                                    w_text,
                                    font=font,
                                    fill=use_rgba,
                                    stroke_width=stroke_width,
                                    stroke_fill=stroke_color_rgb,
                                    spacing_px=letter_spacing_px
                                )

                                # Масштабирование слоя
                                if abs(scale - 1.0) > 1e-3:
                                    new_w = max(1, int(round(word_layer.width * scale)))
                                    new_h = max(1, int(round(word_layer.height * scale)))
                                    word_layer = word_layer.resize((new_w, new_h), resample=Image.BICUBIC)

                                # Умножение альфа-канала
                                if alpha_mult < 0.999:
                                    r, g, b, a = word_layer.split()
                                    a = a.point(lambda v, am=alpha_mult: int(v * am))
                                    word_layer = Image.merge("RGBA", (r, g, b, a))

                                # Позиционирование и композитинг (paste с альфа-маской)
                                paste_x = int(round(x_cursor - pad_left * (scale if scale else 1.0)))
                                paste_y = int(round(start_y + translateY - pad_top * (scale if scale else 1.0)))
                                base_rgb.paste(word_layer, (paste_x, paste_y), mask=word_layer)
                                any_drawn_local = True

                                # Сдвиг курсора по X (учитываем пробел между словами)
                                x_cursor += word_w + (space_w if idx < len(word_widths) - 1 else 0)

                            if any_drawn_local:
                                drawn_any_text = True
                                # Apply emojis near text (if enabled)
                                base_rgb = _apply_emojis(base_rgb, start_x, total_text_w, start_y)

                            # Конверсия обратно в OpenCV BGR
                            frame = cv2.cvtColor(np.array(base_rgb.convert("RGB")), cv2.COLOR_RGB2BGR)

                        except Exception as _anim_ex:
                            # Фолбэк: отрисовка без анимации при ошибке
                            overlay = Image.new("RGBA", (width, height), (0, 0, 0, 0))
                            draw_ov = ImageDraw.Draw(overlay)
                            total_text_w = _measure_text_width(tmp_draw, words_to_display, font, spacing_px=letter_spacing_px)
                            start_x = (width - total_text_w) // 2

                            if sx != 0 or sy != 0 or (shadow_rgba and shadow_rgba[3] > 0):
                                _draw_text_with_spacing(
                                    draw_ov,
                                    (start_x + sx, start_y + sy),
                                    words_to_display,
                                    font=font,
                                    fill=shadow_rgba,
                                    stroke_width=0,
                                    stroke_fill=None,
                                    spacing_px=letter_spacing_px
                                )

                            _draw_text_with_spacing(
                                draw_ov,
                                (start_x, start_y),
                                words_to_display,
                                font=font,
                                fill=text_rgba,
                                stroke_width=stroke_width,
                                stroke_fill=stroke_color_rgb,
                                spacing_px=letter_spacing_px
                            )

                            composed = Image.alpha_composite(base_rgb, overlay)
                            # Apply emojis near text (if enabled)
                            composed = _apply_emojis(composed, start_x, total_text_w, start_y)
                            drawn_any_text = True
                            frame = cv2.cvtColor(np.array(composed.convert("RGB")), cv2.COLOR_RGB2BGR)

            # --- Write frame ---
            out.write(frame)
            frame_count += 1
            if frame_count % 100 == 0:
                  logger.logger.debug(f"[CAPTIONS] Обработано {frame_count}/{total_frames} кадров для анимации...")

        logger.logger.debug(f"[CAPTIONS] Завершена обработка кадров: {frame_count} кадров обработано")

    except Exception as e:
        logger.logger.error(f"[CAPTIONS] Ошибка во время цикла анимации субтитров: {e}")
        logger.logger.debug(f"[CAPTIONS] Подробный traceback: {traceback.format_exc()}")
        success = False
    finally:
        logger.logger.debug("[CAPTIONS] Освобождение видеоресурсов...")
        if cap and cap.isOpened():
            cap.release()
        if out and out.isOpened():
            out.release()

        # Proceed with muxing only if frames were processed, some text was drawn, and temp file exists
        if drawn_any_text and frame_count > 0 and os.path.exists(temp_animated_video):
              try:
                  logger.logger.info("[CAPTIONS] Объединение аудио с анимированным видео...")
                  ffmpeg_mux_command = [
                      'ffmpeg',
                      '-i', temp_animated_video,
                      '-i', audio_source_path,
                      '-map', '0:v:0',
                      '-map', '1:a:0',
                      '-c:v', 'copy',
                      '-c:a', 'aac',
                      '-b:a', '128k',
                      '-shortest',
                      '-y',
                      output_path
                  ]
                  cmd_string = ' '.join([str(arg) for arg in ffmpeg_mux_command])
                  logger.logger.debug(f"[CAPTIONS] Команда mux: {cmd_string}")
                  process = subprocess.run(ffmpeg_mux_command, check=True, capture_output=True, text=True, timeout=300)
                  logger.logger.info(f"[CAPTIONS] Успешно создано видео с анимированными субтитрами: {output_path}")
                  success = True
              except subprocess.TimeoutExpired:
                  logger.logger.error("[CAPTIONS] Ошибка: таймаут FFmpeg muxing.")
                  success = False
              except subprocess.CalledProcessError as mux_e:
                   logger.logger.error(f"[CAPTIONS] Ошибка при объединении аудио (FFmpeg): {mux_e}")
                   logger.logger.debug(f"[CAPTIONS] FFmpeg stdout: {mux_e.stdout}")
                   logger.logger.debug(f"[CAPTIONS] FFmpeg stderr: {mux_e.stderr}")
                   success = False
              except Exception as mux_e:
                   logger.logger.error(f"[CAPTIONS] Непредвиденная ошибка при объединении аудио: {mux_e}")
                   success = False
              finally:
                  # Ensure cleanup even if muxing fails
                  if os.path.exists(temp_animated_video):
                      try:
                          os.remove(temp_animated_video)
                          logger.logger.debug(f"[CAPTIONS] Удален временный анимированный видео файл: {temp_animated_video}")
                      except Exception as e_clean:
                          logger.logger.warning(f"[CAPTIONS] Предупреждение: не удалось удалить временный анимированный файл: {e_clean}")
        elif frame_count > 0 and os.path.exists(temp_animated_video):
              logger.logger.warning("[CAPTIONS] Текст не был отрисован ни на одном кадре; пропуск объединения аудио для анимированных субтитров.")
              success = False
              # Cleanup temp animated video file
              try:
                  os.remove(temp_animated_video)
                  logger.logger.debug(f"[CAPTIONS] Удален временный анимированный видео файл: {temp_animated_video}")
              except Exception as e_clean:
                  logger.logger.warning(f"[CAPTIONS] Предупреждение: не удалось удалить временный анимированный файл: {e_clean}")
        elif not os.path.exists(temp_animated_video) and frame_count > 0:
              logger.logger.error(f"[CAPTIONS] Ошибка: временный анимированный видео файл {temp_animated_video} не найден, невозможно объединить аудио.")
              success = False
        else: # frame_count == 0 or initial error before loop
              logger.logger.warning("[CAPTIONS] Пропуск объединения аудио из-за ошибки обработки или отсутствия кадров.")
              success = False # Ensure success is false if animation failed early

    return success
</file>

<file path="Components/config.py">
from dataclasses import dataclass, field
from typing import Optional, Any, Dict
import os
import re
from pathlib import Path

try:
    import yaml  # type: ignore
except Exception:
    yaml = None  # type: ignore


@dataclass
class ProcessingConfig:
    use_animated_captions: bool = True
    shorts_dir: str = "shorts"
    videos_dir: str = "videos"
    transcriptions_dir: str = "transcriptions"
    crop_bottom_percent: float = 0.0
    min_video_dimension_px: int = 100
    log_transcription_preview_len: int = 200
    crop_mode: str = "70_percent_blur"  # "average_face" or "70_percent_blur"


@dataclass
class LLMConfig:
    model_name: str = "gemini-2.5-flash"
    temperature_highlights: float = 0.2
    temperature_metadata: float = 1.0
    max_attempts_highlights: int = 3
    max_attempts_metadata: int = 3
    highlight_min_sec: int = 29
    highlight_max_sec: int = 61
    max_highlights: int = 20


@dataclass
class IntelligentPauseAnalysisConfig:
    """Конфигурация для интеллектуального анализа пауз"""
    enabled: bool = True  # Включить ИИ-анализ пауз
    model: str = "gemini-2.5-flash-lite"  # Модель для простых задач анализа пауз
    temperature: float = 0.1  # Температура для анализа пауз (низкая для консистентности)
    max_attempts: int = 2  # Максимум попыток для анализа пауз
    auto_trim_confidence_threshold: float = 0.8  # Порог уверенности для автоматической обрезки
    batch_size: int = 10  # Размер батча для пакетного анализа пауз
    cache_enabled: bool = True  # Кеширование результатов анализа пауз
    cache_ttl_hours: int = 24  # Время жизни кеша в часах

    # Категории пауз для классификации
    pause_categories: dict = field(default_factory=lambda: {
        "structural": ["sentence_end", "paragraph_break", "topic_change"],  # Структурные паузы (не обрезать)
        "filler": ["um", "uh", "er", "ah", "like", "you_know"],  # Заполнители (обрезать)
        "emphasis": ["dramatic_pause", "for_effect"],  # Паузы для эффекта (анализировать контекст)
        "breathing": ["breath", "inhale", "exhale"]  # Дыхательные паузы (обрезать при длинных)
    })

    # Весовые коэффициенты для определения важности пауз
    importance_weights: dict = field(default_factory=lambda: {
        "duration": -0.4,  # Чем длиннее пауза, тем менее важна (отрицательный вес)
        "position": 0.3,   # Позиция в предложении (начало/середина/конец)
        "context": 0.4,    # Контекст вокруг паузы
        "audio_features": 0.2,  # Аудио-характеристики (тишина vs шум)
        "linguistic": 0.3  # Лингвистический анализ
    })

    # Оптимизация API
    api_optimization: dict = field(default_factory=lambda: {
        "use_batch_processing": True,  # Использовать пакетную обработку
        "max_concurrent_requests": 3,  # Максимум одновременных запросов
        "rate_limit_delay": 1.0,  # Задержка между запросами (секунды)
        "retry_on_failure": True,  # Повторять при неудачах
        "fallback_to_legacy": True  # Откат на старую логику при ошибках ИИ
    })


@dataclass
class FilmModeConfig:
    """Конфигурация для режима 'фильм'"""
    enabled: bool = True
    combo_duration: list = field(default_factory=lambda: [10, 20])  # секунды для COMBO моментов
    single_duration: list = field(default_factory=lambda: [30, 60])  # секунды для SINGLE моментов
    max_moments: int = 50  # монолитный режим: максимум моментов для анализа (увеличено для Film Mode v2)
    pause_threshold: float = 0.7  # порог для определения длинных пауз (секунды)
    filler_words: list = field(default_factory=lambda: ["э-э", "м-м", "ну", "эээ", "гм", "кхм"])  # слова-заполнители
    min_quality_score: float = 0.5  # минимальный порог качества для включения момента
    generate_shorts: bool = True  # генерировать шорты из найденных моментов

    # Интеллектуальный анализ пауз
    intelligent_pause_analysis: IntelligentPauseAnalysisConfig = field(default_factory=IntelligentPauseAnalysisConfig)

    # Эталонные ключевые слова для подсчета совпадений (новая система ранжирования)
    reference_keywords: dict = field(default_factory=lambda: {
        'emotional_peaks': ["эмоции", "чувства", "переживания", "радость", "гнев", "страх", "удивление", "грусть", "любовь", "ненависть", "восторг", "отчаяние", "надежда", "разочарование", "триумф", "поражение"],
        'conflict_escalation': ["конфликт", "спор", "ссора", "драка", "ругань", "оскорбление", "критика", "давление", "напряжение", "эскалация", "столкновение", "противостояние", "борьба", "конкуренция"],
        'punchlines_wit': ["юмор", "шутка", "сарказм", "ирония", "остроумие", "панчлайн", "каламбур", "смех", "комедия", "прикол", "насмешка", "сатира", "пародия", "абсурд"],
        'quotability_memes': ["цитата", "афоризм", "крылатая фраза", "мем", "вирусный", "тренд", "хайп", "легендарный", "знаменитый", "классика", "запоминающийся", "уникальный"],
        'stakes_goals': ["ставки", "цель", "риск", "опасность", "выбор", "решение", "судьба", "жизнь", "смерть", "выигрыш", "проигрыш", "успех", "провал", "достижение", "амбиции"],
        'hooks_cliffhangers': ["вопрос", "загадка", "тайна", "сюрприз", "интрига", "недосказанность", "продолжение", "развязка", "поворот", "неожиданно", "вдруг", "что дальше", "как же так"]
    })

    # Оконный сбор кандидатов (LLM-sweep)
    window_minutes: int = 8   # Уменьшен для большего числа окон
    window_overlap_minutes: int = 2  # Уменьшен для лучшего покрытия
    max_moments_per_window: int = 10  # Увеличено для большего числа моментов на окно

    # Цели и лимиты генерации
    target_shorts_count: int = 30
    generator_top_k: int = 30

    # Дедупликация/диверсификация
    dedupe_iou_threshold: float = 0.5
    diversity_bucket_minutes: int = 5
    min_combo_segments: int = 2
    max_combo_segments: int = 4

    # Весовые коэффициенты для ранжирования моментов
    ranking_weights: dict = field(default_factory=lambda: {
        'emotional_peaks': 0.20,      # Эмоциональные пики и переломы статуса
        'conflict_escalation': 0.18,  # Конфликт и эскалация
        'punchlines_wit': 0.16,       # Панчлайны и остроумие
        'quotability_memes': 0.14,    # Цитатность/мемность
        'stakes_goals': 0.12,         # Ставки и цель
        'hooks_cliffhangers': 0.10,   # Крючки/клиффхэнгеры
        'visual_penalty': -0.10,      # Штраф за визуальную зависимость
        'pace_score': 0.08,           # Плотность речи (слова/сек)
        'silence_penalty': -0.08,     # Наказание за долю длинных пауз
        'diversity_bonus': 0.05       # Бонус за диверсификацию (может заполняться в ранжировании)
    })

    # Пороговые настройки ранжирования и fallback
    ranking: dict = field(default_factory=lambda: {
        'min_quality_threshold': 0.3,  # Уменьшен порог для включения большего числа моментов
        'soft_min_quality': 0.2,       # Уменьшен мягкий порог
        'allow_fallback': True,
        'fallback_top_n': 12,          # Увеличено для гарантии 12+ моментов
        'max_best_moments': 50,        # Увеличено для поддержки большего числа моментов
    })

    # Настройки LLM для анализа фильма
    llm_model: str = "gemini-2.5-flash"
    llm_temperature: float = 0.3
    llm_max_attempts: int = 3


@dataclass
class LoggingConfig:
    # Основные настройки логирования
    log_dir: str = "logs"
    log_level: str = "INFO"
    enable_console_logging: bool = True
    enable_file_logging: bool = True

    # Настройки ротации логов
    log_rotation_max_bytes: int = 10485760  # 10MB
    log_rotation_backup_count: int = 5
    log_rotation_when: str = "midnight"
    log_compression: bool = False

    # Отдельные логгеры для разных типов сообщений
    enable_main_logger: bool = True
    enable_performance_logger: bool = True
    enable_error_logger: bool = True
    enable_debug_logger: bool = False

    # Настройки производительности
    enable_performance_monitoring: bool = True
    performance_log_max_bytes: int = 5242880  # 5MB
    performance_log_backup_count: int = 3
    performance_monitoring_interval: float = 0.5

    # Настройки GPU мониторинга
    enable_gpu_monitoring: bool = True
    gpu_priority_mode: bool = True
    gpu_memory_threshold: float = 0.9
    gpu_temperature_threshold: int = 80

    # Настройки CPU и памяти
    enable_cpu_monitoring: bool = True
    enable_memory_monitoring: bool = True
    memory_threshold: float = 0.85
    cpu_threshold: float = 90.0

    # Настройки прогресс-баров
    enable_progress_bars: bool = True
    progress_bar_update_interval: float = 0.1

    # Системная информация
    enable_system_info_logging: bool = True
    system_info_log_interval: int = 3600

    # Асинхронная обработка
    enable_async_logging: bool = True
    log_queue_size: int = 1000
    log_worker_threads: int = 2

    # Форматирование логов
    log_format: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    log_date_format: str = "%Y-%m-%d %H:%M:%S"
    enable_colors: bool = True

    # Фильтры логирования
    log_filters: list = field(default_factory=lambda: [
        "urllib3.connectionpool",
        "PIL.PngImagePlugin"
    ])

    # Настройки для разных сред
    development_mode: bool = False
    enable_detailed_tracing: bool = False
    enable_function_call_logging: bool = False

    # Ресурсный мониторинг
    resource_monitoring_interval: float = 1.0
    enable_resource_alerts: bool = True
    alert_threshold_duration: float = 30.0


@dataclass
class PathsConfig:
    """
    Путь-ориентированная конфигурация проекта.

    - base_dir: Абсолютный корень проекта/ресурсов.
    - fonts_dir: Каталог со шрифтами (может быть относительным к base_dir или абсолютным).
    """
    base_dir: str = str(Path(".").resolve())
    fonts_dir: str = "fonts"


@dataclass
class ShadowConfig:
    x_px: int = 2
    y_px: int = 2
    blur_px: int = 1
    color: str = "#00000080"


@dataclass
class AccentPalette:
    urgency: str = "#FFD400"
    drama: str = "#FF3B30"
    positive: str = "#34C759"


@dataclass
class AnimateConfig:
    type: str = "slide-up"  # "pop-in" | "slide-up"
    duration_s: float = 0.35  # [0.2, 0.5]
    easing: str = "easeOutCubic"
    per_word_stagger_ms: int = 120  # [0, 500]


@dataclass
class PositionConfig:
    mode: str = "safe_bottom"  # "safe_bottom" | "center"
    bottom_offset_pct: int = 22  # [0, 100]
    center_offset_pct: int = 12
    boundary_padding_px: int = 10


@dataclass
class EmojiConfig:
    enabled: bool = True
    max_per_short: int = 2  # [0, 5]
    style: str = "shiny"  # "shiny" | "pulse" | "none"


@dataclass
class CaptionsConfig:
    font_size_px: int = 38
    letter_spacing_px: float = 1.5
    line_height: float = 1.3
    base_color: str = "#FFFFFF"
    shadow: ShadowConfig = field(default_factory=ShadowConfig)
    accent_palette: AccentPalette = field(default_factory=AccentPalette)
    animate: AnimateConfig = field(default_factory=AnimateConfig)
    position: PositionConfig = field(default_factory=PositionConfig)
    emoji: EmojiConfig = field(default_factory=EmojiConfig)
    strip_punctuation: bool = True
    # New sync/animation fields
    align_to_audio: bool = True
    fade_in_seconds: float = 0.15
    fade_out_seconds: float = 0.12


@dataclass
class AppConfig:
    processing: ProcessingConfig = field(default_factory=ProcessingConfig)
    llm: LLMConfig = field(default_factory=LLMConfig)
    logging: LoggingConfig = field(default_factory=LoggingConfig)
    paths: PathsConfig = field(default_factory=PathsConfig)
    captions: CaptionsConfig = field(default_factory=CaptionsConfig)
    film_mode: FilmModeConfig = field(default_factory=FilmModeConfig)


def _as_bool(v: Any, default: bool) -> bool:
    try:
        if isinstance(v, bool):
            return v
        if isinstance(v, (int, float)):
            return bool(v)
        if isinstance(v, str):
            s = v.strip().lower()
            if s in ("1", "true", "yes", "y", "on"):
                return True
            if s in ("0", "false", "no", "n", "off"):
                return False
    except Exception:
        pass
    return default


def _as_int(v: Any, default: int) -> int:
    try:
        return int(v)
    except Exception:
        return default


def _as_float(v: Any, default: float) -> float:
    try:
        return float(v)
    except Exception:
        return default


def _as_str(v: Any, default: str) -> str:
    try:
        return str(v)
    except Exception:
        return default


def _clamp(val: float, lo: float, hi: float) -> float:
    try:
        return max(lo, min(hi, float(val)))
    except Exception:
        return lo


def load_config(path: str = "config.yaml") -> AppConfig:
    """
    Загружает конфигурацию приложения из YAML-файла и накладывает её на значения по умолчанию.

    Поведение:
    - Если файл отсутствует или не читается, возвращает значения по умолчанию и печатает:
      "Конфиг не найден. Использую значения по умолчанию."
    - Если файл частично заполнен, недостающие параметры берутся из дефолтов.
    - Выполняется базовая валидация типов и диапазонов (температуры, проценты, интервалы и т.п.).

    Возвращает:
    - Объект AppConfig с заполненными секциями processing, llm и logging.
    """
    defaults = AppConfig()

    if not os.path.exists(path):
        print("Конфиг не найден. Использую значения по умолчанию.")
        return defaults

    data: Dict[str, Any] = {}
    try:
        if yaml is None:
            raise RuntimeError("PyYAML не установлен")
        with open(path, "r", encoding="utf-8") as f:
            loaded = yaml.safe_load(f)  # type: ignore
            if isinstance(loaded, dict):
                data = loaded
    except Exception:
        # В случае любой ошибки парсинга — мягко откатываемся к дефолтам
        return defaults

    p_in = data.get("processing", {}) or {}
    l_in = data.get("llm", {}) or {}
    log_in = data.get("logging", {}) or {}
    paths_in = data.get("paths", {}) or {}
    film_in = data.get("film_mode", {}) or {}
    if not isinstance(p_in, dict):
        p_in = {}
    if not isinstance(l_in, dict):
        l_in = {}
    if not isinstance(log_in, dict):
        log_in = {}
    if not isinstance(paths_in, dict):
        paths_in = {}
    if not isinstance(film_in, dict):
        film_in = {}

    # Processing
    crop_mode = _as_str(p_in.get("crop_mode", defaults.processing.crop_mode), defaults.processing.crop_mode)
    if crop_mode not in ("average_face", "70_percent_blur"):
        crop_mode = defaults.processing.crop_mode

    p = ProcessingConfig(
        use_animated_captions=_as_bool(
            p_in.get("use_animated_captions", defaults.processing.use_animated_captions),
            defaults.processing.use_animated_captions,
        ),
        shorts_dir=_as_str(p_in.get("shorts_dir", defaults.processing.shorts_dir), defaults.processing.shorts_dir),
        videos_dir=_as_str(p_in.get("videos_dir", defaults.processing.videos_dir), defaults.processing.videos_dir),
        transcriptions_dir=_as_str(
            p_in.get("transcriptions_dir", defaults.processing.transcriptions_dir),
            defaults.processing.transcriptions_dir
        ),
        crop_bottom_percent=_clamp(
            _as_float(p_in.get("crop_bottom_percent", defaults.processing.crop_bottom_percent),
                      defaults.processing.crop_bottom_percent),
            0.0,
            100.0,
        ),
        min_video_dimension_px=max(
            1,
            _as_int(p_in.get("min_video_dimension_px", defaults.processing.min_video_dimension_px),
                    defaults.processing.min_video_dimension_px),
        ),
        log_transcription_preview_len=max(
            1,
            _as_int(p_in.get("log_transcription_preview_len", defaults.processing.log_transcription_preview_len),
                    defaults.processing.log_transcription_preview_len),
        ),
        crop_mode=crop_mode,
    )

    # LLM
    t_h = _as_float(l_in.get("temperature_highlights", defaults.llm.temperature_highlights),
                    defaults.llm.temperature_highlights)
    t_m = _as_float(l_in.get("temperature_metadata", defaults.llm.temperature_metadata),
                    defaults.llm.temperature_metadata)
    t_h = _clamp(t_h, 0.0, 2.0)
    t_m = _clamp(t_m, 0.0, 2.0)

    max_att_h = max(1, _as_int(l_in.get("max_attempts_highlights", defaults.llm.max_attempts_highlights),
                               defaults.llm.max_attempts_highlights))
    max_att_m = max(1, _as_int(l_in.get("max_attempts_metadata", defaults.llm.max_attempts_metadata),
                               defaults.llm.max_attempts_metadata))

    h_min = _as_int(l_in.get("highlight_min_sec", defaults.llm.highlight_min_sec),
                    defaults.llm.highlight_min_sec)
    h_max = _as_int(l_in.get("highlight_max_sec", defaults.llm.highlight_max_sec),
                    defaults.llm.highlight_max_sec)
    if h_min < 0:
        h_min = defaults.llm.highlight_min_sec
    if h_max <= h_min:
        h_max = defaults.llm.highlight_max_sec

    max_hls = max(1, _as_int(l_in.get("max_highlights", defaults.llm.max_highlights),
                             defaults.llm.max_highlights))

    l = LLMConfig(
        model_name=_as_str(l_in.get("model_name", defaults.llm.model_name), defaults.llm.model_name),
        temperature_highlights=t_h,
        temperature_metadata=t_m,
        max_attempts_highlights=max_att_h,
        max_attempts_metadata=max_att_m,
        highlight_min_sec=h_min,
        highlight_max_sec=h_max,
        max_highlights=max_hls,
    )

    # Logging
    log = LoggingConfig(
        # Основные настройки логирования
        log_dir=_as_str(log_in.get("log_dir", defaults.logging.log_dir), defaults.logging.log_dir),
        log_level=_as_str(log_in.get("log_level", defaults.logging.log_level), defaults.logging.log_level),
        enable_console_logging=_as_bool(
            log_in.get("enable_console_logging", defaults.logging.enable_console_logging),
            defaults.logging.enable_console_logging,
        ),
        enable_file_logging=_as_bool(
            log_in.get("enable_file_logging", defaults.logging.enable_file_logging),
            defaults.logging.enable_file_logging,
        ),

        # Настройки ротации логов
        log_rotation_max_bytes=max(
            1024,
            _as_int(log_in.get("log_rotation_max_bytes", defaults.logging.log_rotation_max_bytes),
                    defaults.logging.log_rotation_max_bytes),
        ),
        log_rotation_backup_count=max(
            1,
            _as_int(log_in.get("log_rotation_backup_count", defaults.logging.log_rotation_backup_count),
                    defaults.logging.log_rotation_backup_count),
        ),
        log_rotation_when=_as_str(log_in.get("log_rotation_when", defaults.logging.log_rotation_when),
                                  defaults.logging.log_rotation_when),
        log_compression=_as_bool(
            log_in.get("log_compression", defaults.logging.log_compression),
            defaults.logging.log_compression,
        ),

        # Отдельные логгеры для разных типов сообщений
        enable_main_logger=_as_bool(
            log_in.get("enable_main_logger", defaults.logging.enable_main_logger),
            defaults.logging.enable_main_logger,
        ),
        enable_performance_logger=_as_bool(
            log_in.get("enable_performance_logger", defaults.logging.enable_performance_logger),
            defaults.logging.enable_performance_logger,
        ),
        enable_error_logger=_as_bool(
            log_in.get("enable_error_logger", defaults.logging.enable_error_logger),
            defaults.logging.enable_error_logger,
        ),
        enable_debug_logger=_as_bool(
            log_in.get("enable_debug_logger", defaults.logging.enable_debug_logger),
            defaults.logging.enable_debug_logger,
        ),

        # Настройки производительности
        enable_performance_monitoring=_as_bool(
            log_in.get("enable_performance_monitoring", defaults.logging.enable_performance_monitoring),
            defaults.logging.enable_performance_monitoring,
        ),
        performance_log_max_bytes=max(
            1024,
            _as_int(log_in.get("performance_log_max_bytes", defaults.logging.performance_log_max_bytes),
                    defaults.logging.performance_log_max_bytes),
        ),
        performance_log_backup_count=max(
            1,
            _as_int(log_in.get("performance_log_backup_count", defaults.logging.performance_log_backup_count),
                    defaults.logging.performance_log_backup_count),
        ),
        performance_monitoring_interval=max(
            0.1,
            _as_float(log_in.get("performance_monitoring_interval", defaults.logging.performance_monitoring_interval),
                      defaults.logging.performance_monitoring_interval),
        ),

        # Настройки GPU мониторинга
        enable_gpu_monitoring=_as_bool(
            log_in.get("enable_gpu_monitoring", defaults.logging.enable_gpu_monitoring),
            defaults.logging.enable_gpu_monitoring,
        ),
        gpu_priority_mode=_as_bool(
            log_in.get("gpu_priority_mode", defaults.logging.gpu_priority_mode),
            defaults.logging.gpu_priority_mode,
        ),
        gpu_memory_threshold=_clamp(
            _as_float(log_in.get("gpu_memory_threshold", defaults.logging.gpu_memory_threshold),
                      defaults.logging.gpu_memory_threshold),
            0.1, 1.0,
        ),
        gpu_temperature_threshold=max(
            1,
            _as_int(log_in.get("gpu_temperature_threshold", defaults.logging.gpu_temperature_threshold),
                    defaults.logging.gpu_temperature_threshold),
        ),

        # Настройки CPU и памяти
        enable_cpu_monitoring=_as_bool(
            log_in.get("enable_cpu_monitoring", defaults.logging.enable_cpu_monitoring),
            defaults.logging.enable_cpu_monitoring,
        ),
        enable_memory_monitoring=_as_bool(
            log_in.get("enable_memory_monitoring", defaults.logging.enable_memory_monitoring),
            defaults.logging.enable_memory_monitoring,
        ),
        memory_threshold=_clamp(
            _as_float(log_in.get("memory_threshold", defaults.logging.memory_threshold),
                      defaults.logging.memory_threshold),
            0.1, 1.0,
        ),
        cpu_threshold=_clamp(
            _as_float(log_in.get("cpu_threshold", defaults.logging.cpu_threshold),
                      defaults.logging.cpu_threshold),
            1.0, 100.0,
        ),

        # Настройки прогресс-баров
        enable_progress_bars=_as_bool(
            log_in.get("enable_progress_bars", defaults.logging.enable_progress_bars),
            defaults.logging.enable_progress_bars,
        ),
        progress_bar_update_interval=max(
            0.01,
            _as_float(log_in.get("progress_bar_update_interval", defaults.logging.progress_bar_update_interval),
                      defaults.logging.progress_bar_update_interval),
        ),

        # Системная информация
        enable_system_info_logging=_as_bool(
            log_in.get("enable_system_info_logging", defaults.logging.enable_system_info_logging),
            defaults.logging.enable_system_info_logging,
        ),
        system_info_log_interval=max(
            60,
            _as_int(log_in.get("system_info_log_interval", defaults.logging.system_info_log_interval),
                    defaults.logging.system_info_log_interval),
        ),

        # Асинхронная обработка
        enable_async_logging=_as_bool(
            log_in.get("enable_async_logging", defaults.logging.enable_async_logging),
            defaults.logging.enable_async_logging,
        ),
        log_queue_size=max(
            10,
            _as_int(log_in.get("log_queue_size", defaults.logging.log_queue_size),
                    defaults.logging.log_queue_size),
        ),
        log_worker_threads=max(
            1,
            _as_int(log_in.get("log_worker_threads", defaults.logging.log_worker_threads),
                    defaults.logging.log_worker_threads),
        ),

        # Форматирование логов
        log_format=_as_str(log_in.get("log_format", defaults.logging.log_format), defaults.logging.log_format),
        log_date_format=_as_str(log_in.get("log_date_format", defaults.logging.log_date_format),
                                defaults.logging.log_date_format),
        enable_colors=_as_bool(
            log_in.get("enable_colors", defaults.logging.enable_colors),
            defaults.logging.enable_colors,
        ),

        # Фильтры логирования
        log_filters=log_in.get("log_filters", defaults.logging.log_filters) if isinstance(log_in.get("log_filters"), list) else defaults.logging.log_filters,

        # Настройки для разных сред
        development_mode=_as_bool(
            log_in.get("development_mode", defaults.logging.development_mode),
            defaults.logging.development_mode,
        ),
        enable_detailed_tracing=_as_bool(
            log_in.get("enable_detailed_tracing", defaults.logging.enable_detailed_tracing),
            defaults.logging.enable_detailed_tracing,
        ),
        enable_function_call_logging=_as_bool(
            log_in.get("enable_function_call_logging", defaults.logging.enable_function_call_logging),
            defaults.logging.enable_function_call_logging,
        ),

        # Ресурсный мониторинг
        resource_monitoring_interval=max(
            0.1,
            _as_float(log_in.get("resource_monitoring_interval", defaults.logging.resource_monitoring_interval),
                      defaults.logging.resource_monitoring_interval),
        ),
        enable_resource_alerts=_as_bool(
            log_in.get("enable_resource_alerts", defaults.logging.enable_resource_alerts),
            defaults.logging.enable_resource_alerts,
        ),
        alert_threshold_duration=max(
            1.0,
            _as_float(log_in.get("alert_threshold_duration", defaults.logging.alert_threshold_duration),
                      defaults.logging.alert_threshold_duration),
        ),
    )

    # Paths
    base_dir_raw = _as_str(paths_in.get("base_dir", "."), ".")
    fonts_dir_raw = _as_str(paths_in.get("fonts_dir", "fonts"), "fonts")

    try:
        base_abs = Path(base_dir_raw).resolve()
    except Exception:
        base_abs = Path(".").resolve()

    paths = PathsConfig(
        base_dir=str(base_abs),
        fonts_dir=fonts_dir_raw,
    )

    # Captions (with safe defaults and clipping)
    captions_in = data.get("captions", {}) or {}
    if not isinstance(captions_in, dict):
        captions_in = {}

    # Local helpers (scoped to load_config)
    def _is_hex_color(s: str) -> bool:
        if not isinstance(s, str):
            return False
        return bool(re.fullmatch(r"#([0-9A-Fa-f]{6}|[0-9A-Fa-f]{8})", s.strip()))

    def _as_hex_color(v: Any, default: str) -> str:
        s = _as_str(v, default)
        return s if _is_hex_color(s) else default

    def _clamp_int_val(v: Any, default: int, lo: int, hi: int) -> int:
        try:
            iv = int(v)
        except Exception:
            iv = default
        if iv < lo:
            iv = lo
        if iv > hi:
            iv = hi
        return iv

    def _clamp_float_val(v: Any, default: float, lo: float, hi: float) -> float:
        try:
            fv = float(v)
        except Exception:
            fv = default
        return max(lo, min(hi, fv))

    # Top-level caption fields
    fs = _clamp_int_val(
        captions_in.get("font_size_px", defaults.captions.font_size_px),
        defaults.captions.font_size_px, 20, 60
    )
    ls = _clamp_float_val(
        captions_in.get("letter_spacing_px", defaults.captions.letter_spacing_px),
        defaults.captions.letter_spacing_px, 0.0, 10.0
    )
    lh = _clamp_float_val(
        captions_in.get("line_height", defaults.captions.line_height),
        defaults.captions.line_height, 1.0, 2.0
    )
    base_color = _as_hex_color(
        captions_in.get("base_color", defaults.captions.base_color),
        defaults.captions.base_color
    )

    # Shadow
    shadow_in = captions_in.get("shadow", {}) or {}
    if not isinstance(shadow_in, dict):
        shadow_in = {}
    shadow = ShadowConfig(
        x_px=_clamp_int_val(
            shadow_in.get("x_px", defaults.captions.shadow.x_px),
            defaults.captions.shadow.x_px, 0, 20
        ),
        y_px=_clamp_int_val(
            shadow_in.get("y_px", defaults.captions.shadow.y_px),
            defaults.captions.shadow.y_px, 0, 20
        ),
        blur_px=_clamp_int_val(
            shadow_in.get("blur_px", defaults.captions.shadow.blur_px),
            defaults.captions.shadow.blur_px, 0, 16
        ),
        color=_as_hex_color(
            shadow_in.get("color", defaults.captions.shadow.color),
            defaults.captions.shadow.color
        ),
    )

    # Accent palette
    palette_in = captions_in.get("accent_palette", {}) or {}
    if not isinstance(palette_in, dict):
        palette_in = {}
    accent_palette = AccentPalette(
        urgency=_as_hex_color(
            palette_in.get("urgency", defaults.captions.accent_palette.urgency),
            defaults.captions.accent_palette.urgency
        ),
        drama=_as_hex_color(
            palette_in.get("drama", defaults.captions.accent_palette.drama),
            defaults.captions.accent_palette.drama
        ),
        positive=_as_hex_color(
            palette_in.get("positive", defaults.captions.accent_palette.positive),
            defaults.captions.accent_palette.positive
        ),
    )

    # Animate
    animate_in = captions_in.get("animate", {}) or {}
    if not isinstance(animate_in, dict):
        animate_in = {}
    a_type = _as_str(animate_in.get("type", defaults.captions.animate.type), defaults.captions.animate.type)
    if a_type not in ("pop-in", "slide-up"):
        a_type = defaults.captions.animate.type
    duration_s = _clamp_float_val(
        animate_in.get("duration_s", defaults.captions.animate.duration_s),
        defaults.captions.animate.duration_s, 0.2, 0.5
    )
    easing = _as_str(animate_in.get("easing", defaults.captions.animate.easing), defaults.captions.animate.easing)
    per_word_stagger_ms = _clamp_int_val(
        animate_in.get("per_word_stagger_ms", defaults.captions.animate.per_word_stagger_ms),
        defaults.captions.animate.per_word_stagger_ms, 0, 500
    )
    animate = AnimateConfig(
        type=a_type,
        duration_s=duration_s,
        easing=easing,
        per_word_stagger_ms=per_word_stagger_ms,
    )

    # Position
    position_in = captions_in.get("position", {}) or {}
    if not isinstance(position_in, dict):
        position_in = {}
    mode = _as_str(position_in.get("mode", defaults.captions.position.mode), defaults.captions.position.mode)
    if mode not in ("safe_bottom", "center"):
        mode = defaults.captions.position.mode
    bottom_offset_pct = _clamp_int_val(
        position_in.get("bottom_offset_pct", defaults.captions.position.bottom_offset_pct),
        defaults.captions.position.bottom_offset_pct, 0, 100
    )
    center_offset_pct = _clamp_int_val(
        position_in.get("center_offset_pct", defaults.captions.position.center_offset_pct),
        defaults.captions.position.center_offset_pct, -50, 50
    )
    boundary_padding_px = _clamp_int_val(
        position_in.get("boundary_padding_px", defaults.captions.position.boundary_padding_px),
        defaults.captions.position.boundary_padding_px, 0, 100
    )
    position = PositionConfig(
        mode=mode,
        bottom_offset_pct=bottom_offset_pct,
        center_offset_pct=center_offset_pct,
        boundary_padding_px=boundary_padding_px
    )

    # Emoji
    emoji_in = captions_in.get("emoji", {}) or {}
    if not isinstance(emoji_in, dict):
        emoji_in = {}
    enabled = _as_bool(emoji_in.get("enabled", defaults.captions.emoji.enabled), defaults.captions.emoji.enabled)
    max_per_short = _clamp_int_val(
        emoji_in.get("max_per_short", defaults.captions.emoji.max_per_short),
        defaults.captions.emoji.max_per_short, 0, 5
    )
    style = _as_str(emoji_in.get("style", defaults.captions.emoji.style), defaults.captions.emoji.style)
    if style not in ("shiny", "pulse", "none"):
        style = defaults.captions.emoji.style
    emoji = EmojiConfig(enabled=enabled, max_per_short=max_per_short, style=style)

    # Strip punctuation flag (default True if missing)
    strip_punct = _as_bool(
        captions_in.get("strip_punctuation", defaults.captions.strip_punctuation),
        defaults.captions.strip_punctuation
    )

    # New sync/animation fields with safe defaults and ranges
    align_to_audio = _as_bool(
        captions_in.get("align_to_audio", defaults.captions.align_to_audio),
        defaults.captions.align_to_audio
    )
    fade_in_seconds = _clamp_float_val(
        captions_in.get("fade_in_seconds", defaults.captions.fade_in_seconds),
        defaults.captions.fade_in_seconds, 0.0, 5.0
    )
    fade_out_seconds = _clamp_float_val(
        captions_in.get("fade_out_seconds", defaults.captions.fade_out_seconds),
        defaults.captions.fade_out_seconds, 0.0, 5.0
    )

    captions = CaptionsConfig(
        font_size_px=fs,
        letter_spacing_px=ls,
        line_height=lh,
        base_color=base_color,
        shadow=shadow,
        accent_palette=accent_palette,
        animate=animate,
        position=position,
        emoji=emoji,
        strip_punctuation=strip_punct,
        align_to_audio=align_to_audio,
        fade_in_seconds=fade_in_seconds,
        fade_out_seconds=fade_out_seconds,
    )

    # Film Mode
    film_enabled = _as_bool(film_in.get("enabled", defaults.film_mode.enabled), defaults.film_mode.enabled)
    film_combo_duration = film_in.get("combo_duration", defaults.film_mode.combo_duration)
    film_single_duration = film_in.get("single_duration", defaults.film_mode.single_duration)
    film_max_moments = max(1, _as_int(film_in.get("max_moments", defaults.film_mode.max_moments), defaults.film_mode.max_moments))
    film_pause_threshold = _clamp(_as_float(film_in.get("pause_threshold", defaults.film_mode.pause_threshold), defaults.film_mode.pause_threshold), 0.1, 2.0)
    film_filler_words = film_in.get("filler_words", defaults.film_mode.filler_words)
    if not isinstance(film_filler_words, list):
        film_filler_words = defaults.film_mode.filler_words

    # Intelligent Pause Analysis
    intelligent_pause_in = film_in.get("intelligent_pause_analysis", {}) or {}
    if not isinstance(intelligent_pause_in, dict):
        intelligent_pause_in = {}

    intelligent_pause_enabled = _as_bool(
        intelligent_pause_in.get("enabled", defaults.film_mode.intelligent_pause_analysis.enabled),
        defaults.film_mode.intelligent_pause_analysis.enabled
    )
    intelligent_pause_model = _as_str(
        intelligent_pause_in.get("model", defaults.film_mode.intelligent_pause_analysis.model),
        defaults.film_mode.intelligent_pause_analysis.model
    )
    intelligent_pause_temperature = _clamp(
        _as_float(intelligent_pause_in.get("temperature", defaults.film_mode.intelligent_pause_analysis.temperature),
                  defaults.film_mode.intelligent_pause_analysis.temperature),
        0.0, 2.0
    )
    intelligent_pause_max_attempts = max(1, _as_int(
        intelligent_pause_in.get("max_attempts", defaults.film_mode.intelligent_pause_analysis.max_attempts),
        defaults.film_mode.intelligent_pause_analysis.max_attempts
    ))
    intelligent_pause_auto_trim_threshold = _clamp(
        _as_float(intelligent_pause_in.get("auto_trim_confidence_threshold",
                  defaults.film_mode.intelligent_pause_analysis.auto_trim_confidence_threshold),
                  defaults.film_mode.intelligent_pause_analysis.auto_trim_confidence_threshold),
        0.0, 1.0
    )
    intelligent_pause_batch_size = max(1, _as_int(
        intelligent_pause_in.get("batch_size", defaults.film_mode.intelligent_pause_analysis.batch_size),
        defaults.film_mode.intelligent_pause_analysis.batch_size
    ))
    intelligent_pause_cache_enabled = _as_bool(
        intelligent_pause_in.get("cache_enabled", defaults.film_mode.intelligent_pause_analysis.cache_enabled),
        defaults.film_mode.intelligent_pause_analysis.cache_enabled
    )
    intelligent_pause_cache_ttl = max(1, _as_int(
        intelligent_pause_in.get("cache_ttl_hours", defaults.film_mode.intelligent_pause_analysis.cache_ttl_hours),
        defaults.film_mode.intelligent_pause_analysis.cache_ttl_hours
    ))

    # Pause categories
    intelligent_pause_categories = intelligent_pause_in.get("pause_categories",
        defaults.film_mode.intelligent_pause_analysis.pause_categories)
    if not isinstance(intelligent_pause_categories, dict):
        intelligent_pause_categories = defaults.film_mode.intelligent_pause_analysis.pause_categories

    # Importance weights
    intelligent_pause_weights = intelligent_pause_in.get("importance_weights",
        defaults.film_mode.intelligent_pause_analysis.importance_weights)
    if not isinstance(intelligent_pause_weights, dict):
        intelligent_pause_weights = defaults.film_mode.intelligent_pause_analysis.importance_weights

    # API optimization
    intelligent_pause_api_opt = intelligent_pause_in.get("api_optimization",
        defaults.film_mode.intelligent_pause_analysis.api_optimization)
    if not isinstance(intelligent_pause_api_opt, dict):
        intelligent_pause_api_opt = defaults.film_mode.intelligent_pause_analysis.api_optimization

    intelligent_pause_analysis = IntelligentPauseAnalysisConfig(
        enabled=intelligent_pause_enabled,
        model=intelligent_pause_model,
        temperature=intelligent_pause_temperature,
        max_attempts=intelligent_pause_max_attempts,
        auto_trim_confidence_threshold=intelligent_pause_auto_trim_threshold,
        batch_size=intelligent_pause_batch_size,
        cache_enabled=intelligent_pause_cache_enabled,
        cache_ttl_hours=intelligent_pause_cache_ttl,
        pause_categories=intelligent_pause_categories,
        importance_weights=intelligent_pause_weights,
        api_optimization=intelligent_pause_api_opt
    )

    # Reference keywords for new ranking system
    film_reference_keywords = film_in.get("reference_keywords", defaults.film_mode.reference_keywords)
    if not isinstance(film_reference_keywords, dict):
        film_reference_keywords = defaults.film_mode.reference_keywords

    # Ranking weights
    film_ranking_weights = film_in.get("ranking_weights", defaults.film_mode.ranking_weights)
    if not isinstance(film_ranking_weights, dict):
        film_ranking_weights = defaults.film_mode.ranking_weights

    # LLM settings for film mode
    film_llm_model = _as_str(film_in.get("llm", {}).get("model", defaults.film_mode.llm_model), defaults.film_mode.llm_model)
    film_llm_temperature = _clamp(_as_float(film_in.get("llm", {}).get("temperature", defaults.film_mode.llm_temperature), defaults.film_mode.llm_temperature), 0.0, 2.0)
    film_llm_max_attempts = max(1, _as_int(film_in.get("llm", {}).get("max_attempts", defaults.film_mode.llm_max_attempts), defaults.film_mode.llm_max_attempts))
 
    # Ranking and fallback settings
    film_ranking_in = film_in.get("ranking", {}) or {}
    if not isinstance(film_ranking_in, dict):
        film_ranking_in = {}
    ranking_defaults = defaults.film_mode.ranking
    min_q = _clamp(
        _as_float(film_ranking_in.get("min_quality_threshold", ranking_defaults.get("min_quality_threshold")), ranking_defaults.get("min_quality_threshold")),
        0.0, 1.0
    )
    soft_min_q = _clamp(
        _as_float(film_ranking_in.get("soft_min_quality", ranking_defaults.get("soft_min_quality")), ranking_defaults.get("soft_min_quality")),
        0.0, 1.0
    )
    allow_fb = _as_bool(
        film_ranking_in.get("allow_fallback", ranking_defaults.get("allow_fallback")),
        ranking_defaults.get("allow_fallback")
    )
    fb_top_n = max(
        1,
        _as_int(film_ranking_in.get("fallback_top_n", ranking_defaults.get("fallback_top_n")), ranking_defaults.get("fallback_top_n"))
    )
    max_best = max(
        1,
        _as_int(film_ranking_in.get("max_best_moments", ranking_defaults.get("max_best_moments")), ranking_defaults.get("max_best_moments"))
    )
    film_ranking = {
        "min_quality_threshold": min_q,
        "soft_min_quality": soft_min_q,
        "allow_fallback": allow_fb,
        "fallback_top_n": fb_top_n,
        "max_best_moments": max_best,
    }
 
    # Дополнительные поля Film Mode v2 (с безопасными дефолтами и возможностью переопределения из YAML)
    film_window_minutes = max(1, _as_int(film_in.get("window_minutes", defaults.film_mode.window_minutes), defaults.film_mode.window_minutes))
    film_window_overlap_minutes = max(0, _as_int(film_in.get("window_overlap_minutes", defaults.film_mode.window_overlap_minutes), defaults.film_mode.window_overlap_minutes))
    film_max_mom_per_win = max(1, _as_int(film_in.get("max_moments_per_window", defaults.film_mode.max_moments_per_window), defaults.film_mode.max_moments_per_window))
    film_target_shorts = max(1, _as_int(film_in.get("target_shorts_count", defaults.film_mode.target_shorts_count), defaults.film_mode.target_shorts_count))
    film_generator_top_k = max(1, _as_int(film_in.get("generator_top_k", defaults.film_mode.generator_top_k), defaults.film_mode.generator_top_k))
    film_dedupe_iou = _clamp(_as_float(film_in.get("dedupe_iou_threshold", defaults.film_mode.dedupe_iou_threshold), defaults.film_mode.dedupe_iou_threshold), 0.0, 1.0)
    film_diversity_bucket = max(1, _as_int(film_in.get("diversity_bucket_minutes", defaults.film_mode.diversity_bucket_minutes), defaults.film_mode.diversity_bucket_minutes))
    film_min_combo_segments = max(1, _as_int(film_in.get("min_combo_segments", defaults.film_mode.min_combo_segments), defaults.film_mode.min_combo_segments))
    film_max_combo_segments = max(film_min_combo_segments, _as_int(film_in.get("max_combo_segments", defaults.film_mode.max_combo_segments), defaults.film_mode.max_combo_segments))

    film = FilmModeConfig(
        enabled=film_enabled,
        combo_duration=film_combo_duration,
        single_duration=film_single_duration,
        max_moments=film_max_moments,
        pause_threshold=film_pause_threshold,
        filler_words=film_filler_words,
        reference_keywords=film_reference_keywords,
        ranking_weights=film_ranking_weights,
        ranking=film_ranking,
        llm_model=film_llm_model,
        llm_temperature=film_llm_temperature,
        llm_max_attempts=film_llm_max_attempts,

        # v2 поля
        window_minutes=film_window_minutes,
        window_overlap_minutes=film_window_overlap_minutes,
        max_moments_per_window=film_max_mom_per_win,
        target_shorts_count=film_target_shorts,
        generator_top_k=film_generator_top_k,
        dedupe_iou_threshold=film_dedupe_iou,
        diversity_bucket_minutes=film_diversity_bucket,
        min_combo_segments=film_min_combo_segments,
        max_combo_segments=film_max_combo_segments,

        # Интеллектуальный анализ пауз
        intelligent_pause_analysis=intelligent_pause_analysis,
    )

    return AppConfig(processing=p, llm=l, logging=log, paths=paths, captions=captions, film_mode=film)


_CONFIG: Optional[AppConfig] = None


def get_config() -> AppConfig:
    """
    Ленивая кэширующая обёртка над load_config(). Загружает конфигурацию один раз
    из файла (по умолчанию config.yaml), кеширует результат в памяти и возвращает
    один и тот же экземпляр AppConfig при последующих вызовах.
    """
    global _CONFIG
    if _CONFIG is None:
        _CONFIG = load_config()
    return _CONFIG


def reload_config() -> AppConfig:
    """
    Принудительно перезагружает конфигурацию из файла, сбрасывая кэш.
    Полезно при изменении config.yaml во время выполнения программы.
    """
    global _CONFIG
    _CONFIG = None  # Сбрасываем кэш
    return get_config()
</file>

<file path="Components/Database.py">
# Components/Database.py
import sqlite3
from typing import Optional, List, Tuple
import json
import os
from datetime import datetime


class VideoDatabase:
    def __init__(self, db_path: str = "video_processing.db"):
        self.db_path = db_path
        self.init_database()

    def init_database(self):
        """Initialize the database with required tables."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # Create videos table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS videos (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    youtube_url TEXT UNIQUE,
                    local_path TEXT,
                    audio_path TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # Create transcriptions table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS transcriptions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    video_id INTEGER,
                    transcription_data TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (video_id) REFERENCES videos (id)
                )
            """)

            # Create highlights table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS highlights (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    video_id INTEGER,
                    start_time FLOAT,
                    end_time FLOAT,
                    output_path TEXT,
                    segment_text TEXT,
                    caption_with_hashtags TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (video_id) REFERENCES videos (id)
                )
            """)

            conn.commit()

    def add_video(
        self, youtube_url: Optional[str], local_path: str, audio_path: str
    ) -> int:
        """Add a new video entry and return its ID."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                INSERT INTO videos (youtube_url, local_path, audio_path)
                VALUES (?, ?, ?)
            """,
                (youtube_url, local_path, audio_path),
            )
            return cursor.lastrowid

    def update_video_audio_path(self, video_id: int, audio_path: str) -> bool:
        """Update the audio path for an existing video."""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    UPDATE videos 
                    SET audio_path = ?
                    WHERE id = ?
                """,
                    (audio_path, video_id),
                )
                return cursor.rowcount > 0
        except Exception as e:
            print(f"Error updating video audio path: {e}")
            return False

    def get_video(
        self, youtube_url: Optional[str] = None, local_path: Optional[str] = None
    ) -> Optional[tuple]:
        """Get video entry by URL or local path."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            if youtube_url:
                cursor.execute(
                    "SELECT * FROM videos WHERE youtube_url = ?", (youtube_url,)
                )
            elif local_path:
                cursor.execute(
                    "SELECT * FROM videos WHERE local_path = ?", (local_path,)
                )
            else:
                return None
            return cursor.fetchone()

    def add_transcription(
        self, video_id: int, transcriptions: List[Tuple[str, float, float]]
    ) -> int:
        """Add transcription data for a video."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            transcription_json = json.dumps(transcriptions)
            cursor.execute(
                """
                INSERT INTO transcriptions (video_id, transcription_data)
                VALUES (?, ?)
            """,
                (video_id, transcription_json),
            )
            return cursor.lastrowid

    def get_transcription(
        self, video_id: int
    ) -> Optional[List[Tuple[str, float, float]]]:
        """Get transcription data for a video."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT transcription_data FROM transcriptions WHERE video_id = ?",
                (video_id,),
            )
            result = cursor.fetchone()
            if result:
                return json.loads(result[0])
            return None

    def add_highlight(
        self, video_id: int, start_time: float, end_time: float, output_path: str,
        segment_text: Optional[str] = None, caption_with_hashtags: Optional[str] = None
    ) -> int:
        """Add a highlight segment for a video with enriched data."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                INSERT INTO highlights (
                    video_id, start_time, end_time, output_path, 
                    segment_text, caption_with_hashtags
                )
                VALUES (?, ?, ?, ?, ?, ?)
            """,
                (video_id, start_time, end_time, output_path, 
                 segment_text, caption_with_hashtags),
            )
            return cursor.lastrowid

    def get_highlights(self, video_id: int) -> List[Tuple[float, float, str, str, str]]:
        """Get all highlights for a video including enriched data."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                SELECT start_time, end_time, output_path, segment_text, caption_with_hashtags
                FROM highlights 
                WHERE video_id = ?
            """,
                (video_id,),
            )
            return cursor.fetchall()

    def video_exists(
        self, youtube_url: Optional[str] = None, local_path: Optional[str] = None
    ) -> bool:
        """Check if a video exists in the database."""
        return self.get_video(youtube_url, local_path) is not None

    def get_cached_processing(
        self, youtube_url: Optional[str] = None, local_path: Optional[str] = None
    ) -> Optional[dict]:
        """Get all cached processing data for a video."""
        video = self.get_video(youtube_url, local_path)
        if not video:
            return None

        video_id = video[0]
        transcription = self.get_transcription(video_id)
        highlights = self.get_highlights(video_id)

        return {
            "video": video,
            "transcription": transcription,
            "highlights": highlights,
        }
</file>

<file path="Components/Edit.py">
from moviepy.video.io.VideoFileClip import VideoFileClip
from moviepy.editor import VideoFileClip
import subprocess
import math
import tempfile
import os
import shlex
import json # For ffprobe output
import numpy as np
import cv2

# Import caption functions from the new module
from .Captions import burn_captions, animate_captions
# Logger for detailed debug output
from .Logger import logger

def extractAudio(video_path):
    try:
        video_clip = VideoFileClip(video_path)
        audio_path = "audio.wav"
        video_clip.audio.write_audiofile(audio_path)
        video_clip.close()
        print(f"Extracted audio to: {audio_path}")
        return audio_path
    except Exception as e:
        print(f"An error occurred while extracting audio: {e}")
        return None


def crop_video(input_file, output_file, start_time, end_time, original_width, original_height):
    """Extracts a video segment using FFmpeg, ensuring original resolution."""
    try:
        duration = end_time - start_time
        if duration <= 0:
            print("Error: End time must be after start time for cropping.")
            return False
            
        ffmpeg_command = [
            'ffmpeg',
            # -ss перед -i: указывает FFmpeg использовать быстрый поиск по ключевым кадрам.
            # Это принципиально для высокой скорости, так как декодируются только необходимые части файла.
            '-ss', str(start_time),
            
            # -i: определяет входной файл.
            '-i', input_file,
            
            # -t: задает длительность сегмента для извлечения.
            '-t', str(duration),
            
            # -map: явно выбирает потоки для включения в выходной файл.
            # 0:v:0 - первый видеопоток из первого входного файла.
            # 0:a:0 - первый аудиопоток из первого входного файла.
            '-map', '0:v:0',
            '-map', '0:a:0',
            
            # -c copy: КЛЮЧЕВОЕ ИЗМЕНЕНИЕ. Эта опция приказывает FFmpeg не перекодировать
            # (decode -> encode), а напрямую копировать данные видео- и аудиопотоков
            # из исходного контейнера в новый. Это операция I/O-bound, быстрая и без потерь качества.
            '-c', 'copy',
            
            # -sn: отключает копирование потоков субтитров, если они присутствуют в исходном файле.
            '-sn',
            
            # -y: перезаписывать выходной файл без интерактивного подтверждения.
            '-y',
            output_file
        ]
        
        print("Running FFmpeg command for segment extraction (crop_video):")
        cmd_string = ' '.join([str(arg) for arg in ffmpeg_command])
        print(f"Command: {cmd_string}")

        process = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)
        print(f"Successfully extracted segment to: {output_file}")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"Error running FFmpeg during segment extraction: {e}")
        print(f"FFmpeg stdout: {e.stdout}")
        print(f"FFmpeg stderr: {e.stderr}")
        return False
    except Exception as e:
        print(f"An error occurred during segment extraction: {e}")
        return False

# Function to format time in SRT format
def format_time(seconds):
    milliseconds = int((seconds - math.floor(seconds)) * 1000)
    seconds = int(math.floor(seconds))
    minutes = seconds // 60
    seconds %= 60
    hours = minutes // 60
    minutes %= 60
    return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"

# Function to format time in ASS format (hours:mm:ss.cc)
def format_time_ass(seconds):
    centiseconds = int((seconds - math.floor(seconds)) * 100)
    seconds = int(math.floor(seconds))
    minutes = seconds // 60
    seconds %= 60
    hours = minutes // 60
    minutes %= 60
    return f"{hours:d}:{minutes:02d}:{seconds:02d}.{centiseconds:02d}"

def crop_bottom_video(input_path, output_path, crop_percentage_bottom):
    """Crops a percentage from the bottom of the video using FFmpeg."""
    try:
        if not 0 < crop_percentage_bottom < 1:
            print("Error: Crop percentage must be between 0 and 1 (exclusive).")
            return False
            
        height_multiplier = 1.0 - crop_percentage_bottom
        
        ffmpeg_command = [
            'ffmpeg',
            '-i', input_path,
            # vf filter: keep original width (iw), calculate new height based on percentage, ensure it's even, crop from top-left (0,0)
            '-vf', f'crop=iw:floor(ih*{height_multiplier}/2)*2:0:0',
            '-c:v', 'libx264', # Re-encode video
            '-preset', 'medium',
            '-crf', '23',
            '-c:a', 'copy',   # Copy existing audio stream
            '-y',           # Overwrite output file
            output_path
        ]

        print("Running FFmpeg command to crop bottom of video:")
        cmd_string = ' '.join([str(arg) for arg in ffmpeg_command])
        print(f"Command: {cmd_string}")

        process = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)
        print(f"Successfully cropped bottom off video to: {output_path}")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"Error running FFmpeg during bottom crop: {e}")
        print(f"FFmpeg stdout: {e.stdout}")
        print(f"FFmpeg stderr: {e.stderr}")
        # Clean up potentially incomplete output file
        if os.path.exists(output_path):
             try: os.remove(output_path) 
             except: pass
        return False
    except Exception as e:
        print(f"An unexpected error occurred during bottom cropping: {e}")
        # Clean up potentially incomplete output file
        if os.path.exists(output_path):
             try: os.remove(output_path) 
             except: pass
        return False

def get_video_dimensions(video_path):
    """Gets the width and height of a video file using ffprobe."""
    try:
        print(f"Checking dimensions for: {video_path}")
        if not os.path.exists(video_path):
            print("  Error: File not found.")
            return None, None
            
        ffprobe_command = [
            'ffprobe',
            '-v', 'error',
            '-select_streams', 'v:0', # Select the first video stream
            '-show_entries', 'stream=width,height',
            '-of', 'json',
            video_path
        ]
        
        result = subprocess.run(ffprobe_command, check=True, capture_output=True, text=True)
        output_json = json.loads(result.stdout)
        
        if output_json and 'streams' in output_json and len(output_json['streams']) > 0:
            width = output_json['streams'][0].get('width')
            height = output_json['streams'][0].get('height')
            if width is not None and height is not None:
                 print(f"  Dimensions found: {width}x{height}")
                 return int(width), int(height)
            else:
                 print("  Error: Could not find width/height in ffprobe stream data.")
                 return None, None
        else:
            print("  Error: No video streams found by ffprobe or invalid JSON output.")
            print(f"  ffprobe output: {output_json}")
            return None, None
            
    except subprocess.CalledProcessError as e:
        print(f"  Error running ffprobe: {e}")
        print(f"  ffprobe stderr: {e.stderr}")
        return None, None
    except json.JSONDecodeError as e:
        print(f"  Error decoding ffprobe JSON output: {e}")
        if 'result' in locals(): print(f"  Raw ffprobe output: {result.stdout}")
        return None, None
    except Exception as e:
        print(f"  An unexpected error occurred getting dimensions: {e}")
def create_shorts_video(
    input_path,
    output_path,
    crop_width_percentage=0.7,
    left_crop_percent=0.15,
    right_crop_percent=0.15,
    debug: bool = False
):
    """
    Создает вертикальный клип с размытым фоном из исходного видео.

    Требования:
    - Две ветки из входа 0:v:
      Фон: scale=OUT_W:OUT_H, blur (boxblur или gblur), setsar=1
      Контент: crop (70% или лев/прав проценты), затем scale до FG_W:FG_H, setsar=1
    - Никакого pad=color=black
    - Сведение: [bg][fg]overlay=(W-w)/2:(H-h)/2:shortest=1,format=yuv420p
    - Совместимость контейнера: -pix_fmt yuv420p, -map 0:a? -c:a copy
    - OUT_W:OUT_H = 213:274
    - Обратная совместимость параметров crop_width_percentage, left_crop_percent, right_crop_percent
    """
    try:
        # 1) Размеры исходного видео
        original_width, original_height = get_video_dimensions(input_path)
        if not original_width or not original_height:
            print("Error: Could not get video dimensions.")
            return False

        # 2) Целевые размеры кадра и внутренняя логика кропа
        OUT_W, OUT_H = 213, 274  # Требуемые целевые размеры кадра
        # Вычисляем ширину и смещение кропа
        if left_crop_percent is not None and right_crop_percent is not None:
            crop_width = int(original_width * (1.0 - left_crop_percent - right_crop_percent))
            crop_width = max(1, crop_width)
            crop_x = int(original_width * left_crop_percent)
        else:
            crop_width = int(original_width * crop_width_percentage)
            crop_width = max(1, crop_width)
            crop_x = int((original_width - crop_width) / 2)
        crop_height = original_height

        # 3) Построение filter_complex:
        #    - Фон: из того же 0:v, растянуть до OUT_W:OUT_H, размыть, выровнять SAR
        #    - Контент: кроп по логике выше, затем масштаб по высоте до OUT_H с сохранением AR, выровнять SAR
        #    - Оверлей: центрировать, shortest=1, финально привести к yuv420p
        filter_complex = (
            f"[0:v]split=2[fg_src][bg_src];"
            f"[bg_src]scale={OUT_W}:{OUT_H}:flags=bicubic,boxblur=20:1,setsar=1[bg];"
            f"[fg_src]crop={crop_width}:{crop_height}:{crop_x}:0,scale=-2:{OUT_H},setsar=1[fg];"
            f"[bg][fg]overlay=(W-w)/2:(H-h)/2:shortest=1,format=yuv420p[vout]"
        )

        # 4) Полная команда FFmpeg с маппингом аудио, libx264 и yuv420p
        ffmpeg_command = [
            'ffmpeg', '-y',
            '-i', input_path,
            '-filter_complex', filter_complex,
            '-map', '[vout]', '-map', '0:a?',
            '-c:v', 'libx264',
            '-preset', 'fast',
            '-crf', '22',
            '-pix_fmt', 'yuv420p',
            '-c:a', 'copy',
            output_path
        ]

        # 5) Подробное логирование команды и filter_complex
        cmd_string = ' '.join(shlex.quote(arg) for arg in ffmpeg_command)
        logger.logger.debug(f"FFmpeg filter_complex: {filter_complex}")
        logger.logger.debug(f"FFmpeg command: {cmd_string}")
        print("Running FFmpeg command to create shorts video:")
        print(f"Command: {cmd_string}")
        if debug:
            print(f"Filter graph:\n{filter_complex}")

        # 6) Запуск FFmpeg
        process = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)
        print(f"Successfully created shorts video: {output_path}")
        return True

    except subprocess.CalledProcessError as e:
        print(f"Error running FFmpeg for shorts creation: {e}")
        print(f"FFmpeg stdout: {e.stdout}")
        print(f"FFmpeg stderr: {e.stderr}")
        logger.logger.error(f"FFmpeg failed. STDOUT: {e.stdout}")
        logger.logger.error(f"FFmpeg failed. STDERR: {e.stderr}")
        return False
    except Exception as e:
        print(f"An unexpected error occurred during shorts creation: {e}")
        logger.logger.error(f"Unexpected error in create_shorts_video: {e}")
        return False

# Example usage:
# if __name__ == "__main__":
#    # ... (old example usage)
</file>

<file path="Components/FaceCrop.py">
import cv2
import numpy as np
from moviepy.editor import *
import subprocess
import os
# Note: detect_faces_and_speakers and Frames are no longer used by crop_to_vertical_static
# from Components.Speaker import detect_faces_and_speakers, Frames
global Fps

def crop_to_vertical_static(input_video_path, output_video_path):
    """Crops the video to a 9:16 aspect ratio using a static centered crop."""
    cap = cv2.VideoCapture(input_video_path, cv2.CAP_FFMPEG)
    if not cap.isOpened():
        print("Error: Could not open video.")
        return None # Return None on failure

    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if original_height == 0 or fps == 0:
        print("Error: Video properties (height/fps) are invalid.")
        cap.release()
        return None

    # Calculate target 9:16 width based on original height
    vertical_height = original_height
    vertical_width = int(vertical_height * 9 / 16)

    # Ensure the calculated width is even (required by some codecs)
    if vertical_width % 2 != 0:
        vertical_width -= 1

    print(f"Original Dims: {original_width}x{original_height} @ {fps:.2f}fps")
    print(f"Target Vertical Dims: {vertical_width}x{vertical_height}")

    if original_width < vertical_width or vertical_width <= 0:
        print("Error: Original video width is less than the calculated vertical width or width is invalid.")
        cap.release()
        return None

    # Calculate static horizontal crop start/end points (centered)
    x_start = (original_width - vertical_width) // 2
    x_end = x_start + vertical_width
    print(f"Static Crop Range (Horizontal): {x_start} to {x_end}")

    # Setup video writer
    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Use 'mp4v' for .mp4 output
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (vertical_width, vertical_height))
    if not out.isOpened():
        print(f"Error: Could not open video writer for {output_video_path}")
        cap.release()
        return None

    # Set global Fps (if needed elsewhere, otherwise consider removing global)
    global Fps
    Fps = fps

    processed_frames = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break # End of video

        # Apply the static crop
        cropped_frame = frame[:, x_start:x_end]

        # Basic check in case cropping resulted in unexpected shape
        if cropped_frame.shape[1] != vertical_width or cropped_frame.shape[0] != vertical_height:
             print(f"Warning: Cropped frame shape {cropped_frame.shape} doesn't match target {vertical_width}x{vertical_height}. Adjusting...")
             # Attempt to resize, though this indicates an issue upstream or with calculations
             cropped_frame = cv2.resize(cropped_frame, (vertical_width, vertical_height))

        out.write(cropped_frame)
        processed_frames += 1

    print(f"Processed {processed_frames}/{total_frames} frames.")
    cap.release()
    out.release()
    print(f"Static vertical cropping complete. Video saved to: {output_video_path}")
    return output_video_path # Return path on success


def crop_to_vertical(input_video_path, output_video_path):
    # detect_faces_and_speakers(input_video_path, "DecOut.mp4")
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    cap = cv2.VideoCapture(input_video_path, cv2.CAP_FFMPEG)
    if not cap.isOpened():
        print("Error: Could not open video.")
        return

    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    vertical_height = int(original_height)
    vertical_width = int(vertical_height * 9 / 16)
    print(vertical_height, vertical_width)


    if original_width < vertical_width:
        print("Error: Original video width is less than the desired vertical width.")
        return

    x_start = (original_width - vertical_width) // 2
    x_end = x_start + vertical_width
    print(f"start and end - {x_start} , {x_end}")
    print(x_end-x_start)
    half_width = vertical_width // 2

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (vertical_width, vertical_height))
    global Fps
    Fps = fps
    print(fps)
    count = 0
    for _ in range(total_frames):
        ret, frame = cap.read()
        if not ret:
            print("Error: Could not read frame.")
            break
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
        if len(faces) >-1:
            if len(faces) == 0:
                (x, y, w, h) = Frames[count]

            # (x, y, w, h) = faces[0]
            try:
                #check if face 1 is active
                (X, Y, W, H) = Frames[count]
            except Exception as e:
                print(e)
                (X, Y, W, H) = Frames[count][0]
                print(Frames[count][0])

            for f in faces:
                x1, y1, w1, h1 = f
                center = x1+ w1//2
                if center > X and center < X+W:
                    x = x1
                    y = y1
                    w = w1
                    h = h1
                    break

            # print(faces[0])
            centerX = x+(w//2)
            print(centerX)
            print(x_start - (centerX - half_width))
            if count == 0 or (x_start - (centerX - half_width)) <1 :
                ## IF dif from prev fram is low then no movement is done
                pass #use prev vals
            else:
                x_start = centerX - half_width
                x_end = centerX + half_width


                if int(cropped_frame.shape[1]) != x_end- x_start:
                    if x_end < original_width:
                        x_end += int(cropped_frame.shape[1]) - (x_end-x_start)
                        if x_end > original_width:
                            x_start -= int(cropped_frame.shape[1]) - (x_end-x_start)
                    else:
                        x_start -= int(cropped_frame.shape[1]) - (x_end-x_start)
                        if x_start < 0:
                            x_end += int(cropped_frame.shape[1]) - (x_end-x_start)
                    print("Frame size inconsistant")
                    print(x_end- x_start)

        count += 1
        cropped_frame = frame[:, x_start:x_end]
        if cropped_frame.shape[1] == 0:
            x_start = (original_width - vertical_width) // 2
            x_end = x_start + vertical_width
            cropped_frame = frame[:, x_start:x_end]

        print(cropped_frame.shape)

        out.write(cropped_frame)

    cap.release()
    out.release()
    print("Cropping complete. The video has been saved to", output_video_path, count)


# --- New Function: Average Face Centered Crop ---

def crop_to_vertical_average_face(input_video_path, output_video_path, sample_interval_seconds=0.5):
    """Crops video to 9:16 based on the average horizontal face position sampled periodically."""
    print("Starting average face centered vertical crop...")
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    cap = cv2.VideoCapture(input_video_path, cv2.CAP_FFMPEG)
    if not cap.isOpened():
        print(f"Error: Could not open video {input_video_path}")
        return None

    # Get video properties
    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if original_height <= 0 or fps <= 0:
        print("Error: Invalid video properties (height or fps <= 0).")
        cap.release()
        return None

    print(f"Input: {original_width}x{original_height} @ {fps:.2f}fps")

    # Calculate target 9:16 width
    vertical_height = original_height
    vertical_width = int(vertical_height * 9 / 16)
    if vertical_width % 2 != 0: vertical_width -= 1

    if original_width < vertical_width or vertical_width <= 0:
        print("Error: Original width too small for vertical crop.")
        cap.release()
        return None

    print(f"Target Vertical Dims: {vertical_width}x{vertical_height}")

    # --- First Pass: Sample face positions ---
    face_centers_x = []
    frames_to_skip = int(fps * sample_interval_seconds)
    if frames_to_skip < 1: frames_to_skip = 1 # Sample at least every frame if interval is too small

    frame_count = 0
    print(f"Sampling face position every {frames_to_skip} frames...")
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_count % frames_to_skip == 0:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            # Adjust detection parameters if needed (e.g., scaleFactor, minNeighbors)
            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60))

            if len(faces) > 0:
                # Assume the largest face is the main one if multiple are detected
                faces = sorted(faces, key=lambda f: f[2]*f[3], reverse=True)
                x, y, w, h = faces[0]
                centerX = x + w / 2
                face_centers_x.append(centerX)
                # Optional: Draw box on sample frame for debugging
                # cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                # cv2.imshow('Sample', frame); cv2.waitKey(1)

        frame_count += 1

    # --- Calculate Average Position ---
    average_face_center_x = None
    if face_centers_x:
        average_face_center_x = np.mean(face_centers_x)
        print(f"Found {len(face_centers_x)} face samples. Average center X: {average_face_center_x:.2f}")
    else:
        print("Warning: No faces detected during sampling. Falling back to frame center.")
        average_face_center_x = original_width / 2

    # --- Calculate Static Crop Box ---
    half_vertical_width = vertical_width // 2
    x_start = int(average_face_center_x - half_vertical_width)
    x_end = x_start + vertical_width

    # Clamp crop box to frame boundaries
    x_start = max(0, x_start)
    x_end = min(original_width, x_end)

    # Adjust x_start if clamping x_end changed the width
    if x_end - x_start != vertical_width:
         x_start = x_end - vertical_width
         x_start = max(0, x_start) # Re-clamp x_start just in case

    # Final check if width calculation is still correct after clamping
    if x_end - x_start != vertical_width:
        print(f"Error: Could not calculate valid crop window ({x_start}-{x_end}) for width {vertical_width}. Check logic.")
        cap.release()
        return None

    print(f"Calculated Static Crop Box: X = {x_start} to {x_end}")

    # --- Second Pass: Apply crop and write video ---
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0) # Rewind video capture

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (vertical_width, vertical_height))
    if not out.isOpened():
        print(f"Error: Could not open video writer for {output_video_path}")
        cap.release()
        return None

    print("Applying static crop and writing output video...")
    written_frames = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        cropped_frame = frame[:, x_start:x_end]

        # Sanity check shape before writing (optional but good)
        if cropped_frame.shape[1] != vertical_width or cropped_frame.shape[0] != vertical_height:
            print(f"Warning: Frame {written_frames} cropped shape {cropped_frame.shape} != target {vertical_width}x{vertical_height}. Resizing.")
            cropped_frame = cv2.resize(cropped_frame, (vertical_width, vertical_height))

        out.write(cropped_frame)
        written_frames += 1

    print(f"Finished writing {written_frames} frames.")
    cap.release()
    out.release()
    print(f"Average face centered vertical crop complete. Saved to: {output_video_path}")
    return output_video_path


def crop_to_70_percent_with_blur(input_video_path, output_video_path):
    """
    Crops video to 70% of original width with 213:274 aspect ratio for content,
    then creates a 9:16 final frame with blurred background and centered content.
    Uses dynamic sizing based on original video height.
    """
    print("Starting 70% width crop with blur background (9:16 final aspect ratio)...")

    # Get video properties using ffprobe
    try:
        import json
        ffprobe_cmd = [
            'ffprobe',
            '-v', 'error',
            '-select_streams', 'v:0',
            '-show_entries', 'stream=width,height',
            '-of', 'json',
            input_video_path
        ]
        result = subprocess.run(ffprobe_cmd, capture_output=True, text=True, check=True)
        probe_data = json.loads(result.stdout)
        original_width = probe_data['streams'][0]['width']
        original_height = probe_data['streams'][0]['height']
    except Exception as e:
        print(f"Error getting video dimensions: {e}")
        return None

    print(f"Original dimensions: {original_width}x{original_height}")

    # Calculate final 9:16 dimensions based on original height
    final_height = original_height
    final_width = int(final_height * 9 / 16)

    # Ensure final dimensions are even (required by some codecs)
    if final_width % 2 != 0:
        final_width -= 1

    print(f"Final 9:16 dimensions: {final_width}x{final_height}")

    # Calculate 70% width crop for content (maintain 213:274 aspect ratio)
    content_width = int(original_width * 0.7)
    content_aspect_ratio = 213 / 274  # ≈ 0.777
    content_height = min(int(content_width / content_aspect_ratio), original_height)

    # Ensure content dimensions are even
    if content_width % 2 != 0:
        content_width -= 1
    if content_height % 2 != 0:
        content_height -= 1

    print(f"Content crop dimensions (70% width, 213:274 aspect): {content_width}x{content_height}")

    # Calculate scaling for content to fit in final frame while maintaining aspect ratio
    scale_factor = min(final_width / content_width, final_height / content_height)
    scaled_content_width = int(content_width * scale_factor)
    scaled_content_height = int(content_height * scale_factor)

    # Ensure scaled dimensions are even
    if scaled_content_width % 2 != 0:
        scaled_content_width -= 1
    if scaled_content_height % 2 != 0:
        scaled_content_height -= 1

    print(f"Scaled content dimensions: {scaled_content_width}x{scaled_content_height}")

    # Calculate positioning for centering content in final frame
    content_x = (final_width - scaled_content_width) // 2
    content_y = (final_height - scaled_content_height) // 2

    print(f"Content positioning in final frame: x={content_x}, y={content_y}")

    # Calculate blur radius based on original dimensions
    blur_radius = min(original_width, original_height) / 20
    print(f"Blur radius: {blur_radius}")

    # Build filter_complex to eliminate black bars and use blurred background that fills the entire frame.
    # Background: scale with force_original_aspect_ratio=increase then crop to FINAL WxH, apply boxblur, setsar=1.
    # Foreground: center crop to 70% width (CROP_W x CROP_H), then scale to FINAL_CONTENT_W x FINAL_CONTENT_H, setsar=1.
    # Compose: overlay at computed offsets, finalize as yuv420p.
    filter_complex = (
        "[0:v]split=2[fg_src][bg_src];"
        f"[bg_src]scale={final_width}:{final_height}:force_original_aspect_ratio=increase,"
        f"crop={final_width}:{final_height},"
        f"boxblur=luma_radius={blur_radius}:luma_power=5:chroma_radius={blur_radius}:chroma_power=1,setsar=1[bg];"
        f"[fg_src]crop={content_width}:{content_height}:(iw-{content_width})/2:(ih-{content_height})/2,"
        f"scale={scaled_content_width}:{scaled_content_height}:force_original_aspect_ratio=decrease,setsar=1[fg];"
        f"[bg][fg]overlay={content_x}:{content_y}:shortest=1,format=yuv420p[vout]"
    )

    # FFmpeg command using new filter and mappings; keep current crf/preset values
    ffmpeg_cmd = [
        'ffmpeg',
        '-i', input_video_path,
        '-filter_complex', filter_complex,
        '-map', '[vout]',
        '-map', '0:a?',
        '-c:v', 'libx264',
        '-pix_fmt', 'yuv420p',
        '-crf', '23',
        '-preset', 'medium',
        '-c:a', 'copy',
        '-y',
        output_video_path
    ]

    # Logging: print and, if logger available, logger.debug
    try:
        from Components.Logger import logger  # type: ignore
    except Exception:
        logger = None

    print("FFmpeg filter_complex:", filter_complex)
    print("FFmpeg command:", ' '.join([str(a) for a in ffmpeg_cmd]))
    if logger:
        try:
            logger.debug(f"FFmpeg filter_complex: {filter_complex}")
            logger.debug(f"FFmpeg command: {' '.join([str(a) for a in ffmpeg_cmd])}")
        except Exception:
            pass

    try:
        result = subprocess.run(ffmpeg_cmd, check=True, capture_output=True, text=True)
        print(f"Successfully created 70% crop with blurred background video: {output_video_path}")
        if logger:
            try:
                logger.debug(f"ffmpeg stdout: {result.stdout}")
                logger.debug(f"ffmpeg stderr: {result.stderr}")
            except Exception:
                pass
        return output_video_path
    except subprocess.CalledProcessError as e:
        print(f"Error running FFmpeg: {e}")
        print(f"FFmpeg stdout: {e.stdout}")
        print(f"FFmpeg stderr: {e.stderr}")
        if logger:
            try:
                logger.error(f"Error running FFmpeg: {e}")
                logger.error(f"FFmpeg stdout: {e.stdout}")
                logger.error(f"FFmpeg stderr: {e.stderr}")
            except Exception:
                pass
        return None
    except Exception as e:
        print(f"An error occurred during processing: {e}")
        if logger:
            try:
                logger.error(f"An error occurred during processing: {e}")
            except Exception:
                pass
        return None
</file>

<file path="Components/FilmMode.py">
"""
Режим "фильм" для анализа видео и выделения лучших моментов.
Анализирует длинные видео и предлагает оптимальные фрагменты для создания фильма из лучших частей.
"""

from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any, Tuple
import json
import os
from datetime import datetime

from Components.YoutubeDownloader import download_youtube_video
from Components.Transcription import transcribe_unified
from Components.LanguageTasks import (
    build_transcription_prompt,
    GetHighlights,
    call_llm_with_retry,
    call_llm_with_film_mode_retry,
    make_generation_config,
    compute_tone_and_keywords,
    compute_emojis_for_segment
)
from Components.Transcription import prepare_words_for_segment
from Components.Database import VideoDatabase
from Components.config import get_config, AppConfig
from Components.Logger import logger
from Components.Edit import crop_video, burn_captions, crop_bottom_video, animate_captions, get_video_dimensions
from Components.FaceCrop import crop_to_70_percent_with_blur, crop_to_vertical_average_face
from Components.Paths import build_short_output_name
from faster_whisper import WhisperModel
import math


@dataclass
class FilmMoment:
    """Структура для хранения информации о моменте фильма"""
    moment_type: str  # "COMBO" или "SINGLE"
    start_time: float
    end_time: float
    text: str
    segments: List[Dict[str, Any]] = field(default_factory=list)  # Для COMBO: суб-сегменты
    context: str = ""  # Описание контекста
    keywords: List[str] = field(default_factory=list)  # Ключевые слова, выделенные ИИ


@dataclass
class RankedMoment:
    """Ранжированный момент с оценками"""
    moment: FilmMoment
    scores: Dict[str, float]  # Оценки по критериям
    total_score: float
    rank: int


@dataclass
class FilmAnalysisResult:
    """Результат анализа фильма"""
    video_id: str
    duration: float
    keep_ranges: List[Dict[str, Any]]
    scores: List[Dict[str, Any]]
    preview_text: str
    risks: List[str]
    metadata: Dict[str, Any]
    generated_shorts: List[str] = field(default_factory=list)  # Пути к сгенерированным шортам


class FilmAnalyzer:
    """
    Анализатор фильмов для выделения лучших моментов.
    Интегрируется с существующими компонентами проекта.
    """

    def __init__(self, config: AppConfig):
        logger.logger.info("Инициализация FilmAnalyzer...")
        self.config = config
        logger.logger.info("Создание подключения к базе данных VideoDatabase...")
        self.db = VideoDatabase()
        logger.logger.info("✅ FilmAnalyzer инициализирован успешно")

        # Настройки для режима фильм
        self.film_config = config.film_mode
        self._last_ranking_info = {}
        # Контекст транскрипции для расширенного скоринга (pace/silence)
        self._ctx_transcription_data = None

    def analyze_film(self, url: Optional[str] = None, local_path: Optional[str] = None) -> FilmAnalysisResult:
        """
        Основной пайплайн анализа фильма
        1. Получение видео
        2. Транскрибация
        3. Анализ моментов через ИИ
        4. Ранжирование
        5. Обрезка скучных секунд
        6. Генерация шортов (если включено)
        7. Формирование результата
        """
        logger.logger.info("Начало анализа фильма в режиме 'фильм'")

        # 1. Получение видео и транскрибация
        video_path, transcription_data = self._get_video_and_transcription(url, local_path)
        if not video_path or not transcription_data:
            raise ValueError("Не удалось получить видео или транскрибацию")

        # Логируем информацию о полученных данных + единое разрешение длительности
        segments_count = len(transcription_data.get('segments', []))
        # Единое разрешение длительности и запись в transcription_data['duration']
        resolved_duration = self._resolve_video_duration(video_path, transcription_data)
        logger.logger.info(f"Получены данные транскрибации: сегментов={segments_count}")
        logger.logger.info(f"Duration: {resolved_duration:.2f} seconds")
        # Сохраняем транскрипцию в контекст для последующих фаз (скоринг pace/silence)
        try:
            self._ctx_transcription_data = transcription_data
        except Exception:
            self._ctx_transcription_data = None

        # 2. Анализ моментов через ИИ
        moments = self._analyze_moments(transcription_data)
        if not moments:
            logger.logger.warning("Не найдено подходящих моментов для анализа")
            logger.logger.warning(f"Данные транскрибации: duration={resolved_duration}, segments={segments_count}")
            return self._create_empty_result(video_path)

        # 3. Ранжирование моментов
        ranked_moments = self._rank_moments(moments)
        try:
            info = getattr(self, "_last_ranking_info", {}) or {}
            strategy = info.get("selection_strategy", "quality_threshold")
            logger.logger.info(f"[OK] Ranking: selected={len(ranked_moments)} (strategy={strategy}), proceed")
        except Exception:
            pass
        if not ranked_moments:
            logger.logger.warning("После фильтрации по качеству не осталось подходящих моментов")
            return self._create_empty_result(video_path)

        # 4. Обрезка скучных секунд
        trimmed_moments = self._trim_boring_segments(ranked_moments, transcription_data)

        # 5. Генерация шортов (если включено)
        generated_shorts = []
        logger.logger.info(f"Проверка условий для генерации шортов:")
        logger.logger.info(f"  generate_shorts: {self.film_config.generate_shorts}")
        logger.logger.info(f"  количество моментов: {len(trimmed_moments)}")

        if self.film_config.generate_shorts and trimmed_moments:
            logger.logger.info("✅ Условия выполнены, начинаем генерацию шортов...")
            generated_shorts = self._generate_shorts_from_moments(video_path, trimmed_moments, transcription_data)
            logger.logger.info(f"Сгенерировано {len(generated_shorts)} шортов")
        else:
            if not self.film_config.generate_shorts:
                logger.logger.warning("⚠️ Генерация шортов отключена в конфигурации")
            if not trimmed_moments:
                logger.logger.warning("⚠️ Нет подходящих моментов для генерации шортов")

        # 6. Формирование результата
        result = self._create_result(video_path, trimmed_moments, transcription_data, generated_shorts)

        logger.logger.info(f"Анализ фильма завершен. Найдено {len(trimmed_moments)} моментов, сгенерировано {len(generated_shorts)} шортов")
        try:
            _ctx_dur = float(transcription_data.get('duration', 0.0) or 0.0)
            logger.logger.info(f"[OK] Duration consistency: ctx={_ctx_dur:.2f}s, warnings=0")
            logger.logger.info(f"[OK] Summary duration: {_ctx_dur:.2f}s записано в JSON и в финальный лог")
        except Exception:
            logger.logger.info(f"[OK] Duration consistency: ctx={transcription_data.get('duration', 0.0)}s, warnings=0")
            logger.logger.info(f"[OK] Summary duration: {transcription_data.get('duration', 0.0)}s записано в JSON и в финальный лог")
        return result

    def _get_video_and_transcription(self, url: Optional[str], local_path: Optional[str]) -> tuple:
        """Получение видео и его транскрибация"""
        try:
            # Получение видео
            if url:
                logger.logger.info(f"Загрузка видео по URL: {url}")
                video_path = download_youtube_video(url)
            elif local_path:
                logger.logger.info(f"Использование локального файла: {local_path}")
                video_path = local_path
            else:
                raise ValueError("Не указан URL или локальный путь к видео")

            if not video_path or not os.path.exists(video_path):
                raise FileNotFoundError(f"Видео файл не найден: {video_path}")

            # Транскрибация
            logger.logger.info("Начало транскрибации видео")
            model = self._load_whisper_model()
            segments_legacy, word_level_transcription = transcribe_unified(video_path, model)

            # Используем длительность из модели Whisper вместо ffprobe
            duration = 0.0
            if word_level_transcription and 'segments' in word_level_transcription:
                # Извлекаем длительность из последнего сегмента
                segments = word_level_transcription['segments']
                if segments:
                    last_segment = segments[-1]
                    duration = float(last_segment.get('end', 0.0))

            # Если длительность все еще 0, пробуем ffprobe как fallback
            if duration == 0.0:
                try:
                    duration = self._get_video_duration(video_path)
                    logger.logger.info(f"Длительность получена через ffprobe: {duration:.2f} секунд")
                except Exception as e:
                    logger.logger.warning(f"Не удалось получить длительность через ffprobe: {e}")

            logger.logger.info(f"Финальная длительность видео: {duration:.2f} секунд")

            transcription_data = {
                'segments': segments_legacy,
                'word_level': word_level_transcription,
                'duration': duration
            }

            return video_path, transcription_data

        except Exception as e:
            logger.logger.error(f"Ошибка при получении видео или транскрибации: {e}")
            return None, None

    def _load_whisper_model(self) -> WhisperModel:
        """Загрузка модели Whisper"""
        # Используем ту же логику, что и в main.py
        from Components.config import reload_config
        cfg = reload_config()

        # Определение параметров модели
        try:
            import torch
            has_cuda = torch.cuda.is_available()
        except:
            has_cuda = False

        if has_cuda and cfg.logging.gpu_priority_mode:
            device = "cuda"
            model_size = "large-v3"
            compute_type = "float16"
        else:
            device = "cpu"
            model_size = "small"
            compute_type = "int8"

        logger.logger.info(f"Загрузка модели Whisper: {model_size} на {device}")
        return WhisperModel(
            model_size,
            device=device,
            compute_type=compute_type,
            num_workers=2,
        )

    def _get_video_duration(self, video_path: str) -> float:
        """Получение длительности видео через ffprobe"""
        try:
            import subprocess
            import json

            cmd = [
                "ffprobe",
                "-v", "quiet",
                "-print_format", "json",
                "-show_format",
                video_path
            ]

            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                data = json.loads(result.stdout)
                return float(data['format']['duration'])
            else:
                logger.logger.warning("Не удалось получить длительность видео через ffprobe")
                return 0.0
        except Exception as e:
            logger.logger.warning(f"Ошибка при получении длительности видео: {e}")
            return 0.0

    def _resolve_video_duration(self, video_path: str, transcription_data: Dict[str, Any]) -> float:
        """
        Единое определение длительности видео с приоритетом:
          a) транскрипция (макс. end среди сегментов word-level и/или legacy)
          b) fallback: ffprobe через _get_video_duration
        Валидация: значение > 0; логируем INFO итог; при сомнении — ERROR и переключение на альтернативный источник.
        """
        # Извлекаем длительность из транскрипции
        trans_end_word = 0.0
        try:
            wl = transcription_data.get('word_level', {})
            if isinstance(wl, dict) and isinstance(wl.get('segments'), list) and wl['segments']:
                trans_end_word = max(float(s.get('end', 0.0) or 0.0) for s in wl['segments'])
        except Exception as e:
            logger.logger.debug(f"Не удалось вычислить длительность из word_level: {e}")

        trans_end_legacy = 0.0
        try:
            segs = transcription_data.get('segments') or []
            ends = []
            for seg in segs:
                if isinstance(seg, (list, tuple)) and len(seg) >= 3:
                    ends.append(float(seg[2]))
                elif isinstance(seg, dict):
                    ends.append(float(seg.get('end', 0.0) or 0.0))
            if ends:
                trans_end_legacy = max(ends)
        except Exception as e:
            logger.logger.debug(f"Не удалось вычислить длительность из legacy segments: {e}")

        transcription_duration = max(trans_end_word, trans_end_legacy)

        # Fallback: ffprobe
        ffprobe_duration = 0.0
        try:
            ffprobe_duration = float(self._get_video_duration(video_path) or 0.0)
        except Exception as e:
            logger.logger.debug(f"Не удалось получить длительность через ffprobe: {e}")
            ffprobe_duration = 0.0

        # Сохраняем источники в transcription_data для последующего логирования
        transcription_data['duration_from_transcription'] = float(transcription_duration or 0.0)
        transcription_data['duration_from_ffprobe'] = float(ffprobe_duration or 0.0)

        # Выбор значения по правилам
        chosen = transcription_duration if transcription_duration and transcription_duration > 0 else ffprobe_duration

        # Диагностика сомнительных случаев
        long_transcription = transcription_duration and transcription_duration > 600
        if (not chosen or chosen <= 0):
            logger.logger.error("[DURATION] Получено некорректное значение длительности (<= 0). Переключение на альтернативный источник (ffprobe).")
            if ffprobe_duration and ffprobe_duration > 0:
                chosen = ffprobe_duration

        if long_transcription and (chosen < 300 or chosen <= 0):
            logger.logger.error("[DURATION] Подозрительно малая длительность при длинной транскрипции. Переключение на альтернативный источник.")
            alt = ffprobe_duration if chosen == transcription_duration else transcription_duration
            if alt and alt > chosen:
                chosen = alt

        # Если оба источника валидны и сильно расходятся, выбираем большее для предотвращения обрезки
        try:
            if transcription_duration > 0 and ffprobe_duration > 0:
                delta = abs(transcription_duration - ffprobe_duration)
                if delta > 1.0 and ffprobe_duration > transcription_duration * 1.05:
                    logger.logger.warning(f"[DURATION] Расхождение источников: transcription={transcription_duration:.2f}s, ffprobe={ffprobe_duration:.2f}s. Выбрано большее значение.")
                    chosen = max(transcription_duration, ffprobe_duration)
        except Exception:
            pass

        # Финальные логи
        if chosen and chosen > 0:
            logger.logger.info(f"[VALIDATION] duration sources: transcription={transcription_duration:.2f}, ffprobe={ffprobe_duration:.2f}, chosen=ctx.video_duration={chosen:.2f}")
            logger.logger.info(f"Duration: {chosen:.2f} seconds")
        else:
            logger.logger.error("[DURATION] Не удалось надежно определить длительность видео. Установлено 0.0s")
            chosen = 0.0

        # Записываем единое значение в transcription_data
        transcription_data['duration'] = float(chosen)
        return float(chosen)

    def _analyze_moments(self, transcription_data: Dict[str, Any]) -> List[FilmMoment]:
        """Анализ моментов через ИИ (Film Mode v2: оконный сбор для длинных фильмов)"""
        try:
            duration = float(transcription_data.get('duration', 0.0) or 0.0)
        except Exception:
            duration = 0.0

        try:
            # АКТИВИРУЕМ ОКОННЫЙ РЕЖИМ ДЛЯ ВСЕХ ФИЛЬМОВ > 10 МИНУТ (было 45 минут)
            if duration >= 10 * 60:  # 10 минут вместо 45
                logger.logger.info(f"[WINDOW] Активирован оконный режим извлечения кандидатов (duration={duration:.2f}s)")
                moments = self._extract_film_moments_windowed(transcription_data)
                logger.logger.info(f"[WINDOW] Найдено {len(moments)} кандидатов после дедупликации")
                return moments

            # Короткие видео: прежняя монолитная логика
            segments_legacy = transcription_data.get('segments', [])
            segments_dict = []
            for seg in segments_legacy:
                if isinstance(seg, (list, tuple)) and len(seg) >= 3:
                    segments_dict.append({
                        'text': str(seg[0]),
                        'start': float(seg[1]),
                        'end': float(seg[2])
                    })
                elif isinstance(seg, dict):
                    segments_dict.append(seg)

            logger.logger.info("Формирование текста транскрибации через build_transcription_prompt...")
            transcription_text = build_transcription_prompt(segments_dict)
            logger.logger.info(f"✅ Текст транскрибации сформирован: {len(transcription_text)} символов")

            logger.logger.info("Анализ моментов через LLM (монолитный вызов)...")
            moments = self._extract_film_moments(transcription_text)
            logger.logger.info(f"Найдено {len(mомents)} потенциальных моментов")
            return moments
        except Exception as e:
            logger.logger.error(f"Ошибка при анализе моментов: {e}")
            return []

    def _extract_film_moments(self, transcription: str) -> List[FilmMoment]:
        """Извлечение моментов фильма через LLM с ключевыми словами"""
        system_instruction = f"""
        Ты — эксперт по анализу видео контента для создания вирусных shorts. Проанализируй предоставленную транскрибацию и выдели лучшие моменты двух типов:

        1. COMBO (10-20 сек): Склейка 2-4 коротких кусков из одной сцены в хронологическом порядке для создания мини-дуги
        2. SINGLE (30-60 сек): Один самодостаточный момент с микро-аркой (завязка → нарастание → развязка)

        Для КАЖДОГО момента выдели 3-7 ключевых слов, которые характеризуют его суть и потенциал для вирусности.

        Верни ТОЛЬКО JSON-массив объектов с полями:
        - moment_type: "COMBO" или "SINGLE"
        - start_time: число (секунды)
        - end_time: число (секунды)
        - text: текст момента
        - context: краткое описание почему этот момент подходит
        - keywords: массив строк с ключевыми словами (3-7 слов)

        Для COMBO также добавь:
        - segments: массив суб-сегментов с start/end/text

        Найди до {max(self.film_config.max_moments, self.film_config.target_shorts_count)} лучших моментов.
        """

        try:
            logger.logger.info(f"Отправка запроса к LLM для анализа моментов (модель: {self.film_config.llm_model})")
            logger.logger.debug(f"Длина транскрибации: {len(transcription)} символов")

            logger.logger.info("Создание конфигурации генерации через make_generation_config...")
            generation_config = make_generation_config(system_instruction, temperature=0.3)
            logger.logger.info("✅ Конфигурация генерации создана")

            logger.logger.info("Отправка запроса к LLM через call_llm_with_film_mode_retry...")
            response = call_llm_with_film_mode_retry(
                system_instruction=None,
                content=transcription,
                generation_config=generation_config,
                model=self.film_config.llm_model,
                max_api_attempts=5,
            )

            if not response or not response.text:
                logger.logger.warning("LLM не вернул ответ при анализе моментов")
                return []

            # Парсинг JSON ответа
            response_text = response.text.strip()
            logger.logger.debug(f"Сырой ответ LLM: {response_text[:500]}...")

            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]

            response_text = response_text.strip()

            try:
                moments_data = json.loads(response_text)
            except json.JSONDecodeError as e:
                logger.logger.error(f"Ошибка парсинга JSON от LLM: {e}")
                logger.logger.error(f"Текст для парсинга: {response_text[:200]}...")
                return []

            if not isinstance(moments_data, list):
                logger.logger.error(f"LLM вернул не массив, а {type(moments_data)}")
                return []

            moments = []
            for i, item in enumerate(moments_data):
                try:
                    if not isinstance(item, dict):
                        logger.logger.warning(f"Элемент {i} не является словарем, пропускаю")
                        continue

                    moment = FilmMoment(
                        moment_type=item.get('moment_type', 'SINGLE'),
                        start_time=float(item.get('start_time', 0)),
                        end_time=float(item.get('end_time', 0)),
                        text=item.get('text', ''),
                        context=item.get('context', ''),
                        segments=item.get('segments', []),
                        keywords=item.get('keywords', [])
                    )
                    moments.append(moment)
                    logger.logger.debug(f"Обработан момент {i+1}: {moment.moment_type} {moment.start_time:.1f}-{moment.end_time:.1f}")

                except Exception as e:
                    logger.logger.warning(f"Ошибка при обработке момента {i}: {e}")
                    continue

            logger.logger.info(f"Успешно извлечено {len(moments)} моментов из {len(moments_data)}")
            return moments

        except Exception as e:
            logger.logger.error(f"Ошибка при извлечении моментов через LLM: {e}")
            import traceback
            logger.logger.error(f"Traceback: {traceback.format_exc()}")
            return []

    def _rank_moments(self, moments: List[FilmMoment]) -> List[RankedMoment]:
        """Ранжирование моментов с динамическим порогом и покрытием по таймлайнам"""
        # Конфигурация ранжирования
        rc = getattr(self.film_config, "ranking", {}) or {}
        min_thr_cfg = float(rc.get("min_quality_threshold", getattr(self.film_config, "min_quality_score", 0.5)))
        soft_min = float(rc.get("soft_min_quality", 0.35))
        allow_fb = bool(rc.get("allow_fallback", True))
        fb_top_n = int(max(1, rc.get("fallback_top_n", 12)))
        max_best_cfg = int(max(1, rc.get("max_best_moments", 30)))
        target_n = int(max(1, getattr(self.film_config, "target_shorts_count", max_best_cfg)))
        bucket_min = int(max(1, getattr(self.film_config, "diversity_bucket_minutes", 5)))
        bucket_sec = bucket_min * 60
        gen_top_k = int(max(1, getattr(self.film_config, "generator_top_k", target_n)))

        # Подсчет и сортировка по score (с учетом ключевых слов)
        scored: List[RankedMoment] = []
        for moment in moments or []:
            scores = self._calculate_moment_scores(moment)
            total_score = sum(
                scores.get(name, 0.0) * self.film_config.ranking_weights.get(name, 0.0)
                for name in self.film_config.ranking_weights.keys()
            )
            # Бонус за количество и качество ключевых слов
            keyword_bonus = len(moment.keywords or []) * 0.1
            total_score += keyword_bonus
            scored.append(RankedMoment(moment=moment, scores=scores, total_score=total_score, rank=0))

        scored.sort(key=lambda x: x.total_score, reverse=True)
        M = len(scored)
        logger.logger.info(f"Кандидатов: {M}, min_thr_cfg={min_thr_cfg}, soft={soft_min}, target_n={target_n}, max_best_cfg={max_best_cfg}, generator_top_k={gen_top_k}")

        if M == 0:
            return []

        # Динамический порог: p75
        totals = [rm.total_score for rm in scored]
        totals_sorted = sorted(totals)
        try:
            p75_idx = max(0, min(len(totals_sorted) - 1, int(round(0.75 * (len(totals_sorted) - 1)))))
            p75 = float(totals_sorted[p75_idx])
        except Exception:
            p75 = min_thr_cfg
        min_thr_dyn = max(min_thr_cfg, p75)
        logger.logger.info(f"[RANK] dynamic threshold p75={p75:.3f} -> min_thr_dyn={min_thr_dyn:.3f}")

        selected = [rm for rm in scored if rm.total_score >= min_thr_dyn]

        strategy = "dynamic_p75"
        if not selected and allow_fb:
            # Fallback: берем top-N
            K = max(1, fb_top_n)
            selected = scored[:K]
            strategy = "fallback_topN"
            logger.logger.warning(f"Fallback top-N активирован: выбранных={len(selected)} из {M}")

        # Ограничение предварительное
        if selected:
            selected = selected[:max_best_cfg]

        # Покрытие по таймлайнам (diversity buckets) с round-robin
        if selected:
            buckets: Dict[int, List[RankedMoment]] = {}
            for rm in selected:
                try:
                    b = int(max(0, rm.moment.start_time) // bucket_sec)
                except Exception:
                    b = 0
                buckets.setdefault(b, []).append(rm)

            # внутри каждого бакета уже по убыванию total_score
            for b in buckets.values():
                b.sort(key=lambda x: x.total_score, reverse=True)

            covered: List[RankedMoment] = []
            keys = sorted(buckets.keys())
            # Итоговый лимит
            limit_k = min(gen_top_k, target_n, max_best_cfg, len(selected))
            idx = 0
            while len(covered) < limit_k:
                progressed = False
                for k in keys:
                    bucket_list = buckets.get(k, [])
                    if idx < len(bucket_list):
                        covered.append(bucket_list[idx])
                        progressed = True
                        if len(covered) >= limit_k:
                            break
                if not progressed:
                    # все бакеты исчерпаны на данном idx
                    break
                idx += 1
            selected = covered

        # Присвоение рангов
        for i, rm in enumerate(selected):
            rm.rank = i + 1

        first_score = selected[0].total_score if selected else 0.0
        last_score = selected[-1].total_score if selected else 0.0
        logger.logger.info(f"Выбрано {len(selected)} моментов (strategy={strategy}), первый score={first_score:.3f}, последний score={last_score:.3f}")
        logger.logger.info(f"[OK] Ranking: candidates={M}, dynamic_thr={min_thr_dyn:.3f}, selected={len(selected)} (strategy={strategy})")
        if len(selected) >= 1:
            logger.logger.info("[VALIDATION] Ranking produced N>=1: OK")

        try:
            self._last_ranking_info = {
                "selection_strategy": strategy,
                "candidates": M,
                "selected": len(selected),
                "threshold": float(min_thr_dyn),
                "soft": float(soft_min),
            }
        except Exception:
            pass

        return selected

    def _calculate_moment_scores(self, moment: FilmMoment) -> Dict[str, float]:
        """Расчет оценок момента по совпадениям ключевых слов (новая система ранжирования)"""
        scores: Dict[str, float] = {}

        # Получаем эталонные ключевые слова из конфигурации
        ref_keywords = getattr(self.film_config, 'reference_keywords', {})

        # Нормализуем ключевые слова момента (приводим к нижнему регистру)
        moment_keywords = [kw.lower().strip() for kw in (moment.keywords or []) if kw and kw.strip()]

        # Подсчитываем совпадения для каждой категории
        for category, reference_words in ref_keywords.items():
            if category in self.film_config.ranking_weights:
                # Нормализуем эталонные ключевые слова
                ref_words_lower = [w.lower().strip() for w in reference_words]

                # Считаем совпадения
                matches = 0
                for moment_kw in moment_keywords:
                    # Проверяем точное совпадение или частичное вхождение
                    for ref_kw in ref_words_lower:
                        if ref_kw in moment_kw or moment_kw in ref_kw:
                            matches += 1
                            break  # Одно ключевое слово момента может соответствовать только одной категории

                # Нормализуем оценку (максимум 10 за совпадения)
                scores[category] = min(matches * 2.0, 10.0)

        # Длина текста как бонус (короткие моменты лучше для shorts)
        duration = moment.end_time - moment.start_time
        if 10 <= duration <= 60:  # Идеальная длительность для shorts
            scores['duration_bonus'] = 4.0
        elif 5 <= duration <= 120:  # Приемлемая длительность
            scores['duration_bonus'] = 2.0
        else:
            scores['duration_bonus'] = 0.0

        # Бонус за разнообразие типов моментов
        if moment.moment_type == 'COMBO':
            scores['combo_bonus'] = 3.0  # COMBO моменты более ценны
        else:
            scores['combo_bonus'] = 0.0

        # Базовый скор за наличие ключевых слов
        if moment_keywords:
            scores['content_bonus'] = min(len(moment_keywords) * 0.5, 3.0)
        else:
            scores['content_bonus'] = 0.0

        # Штраф за визуальную зависимость
        text = (moment.text or "").lower()
        visual_keywords = ['визуально', 'зрительно', 'видно', 'картинка', 'изображение']
        visual_count = sum(1 for keyword in visual_keywords if keyword in text)
        scores['visual_penalty'] = -visual_count * 0.2

        # Расширенные метрики: pace/silence из контекста транскрипции
        try:
            td = getattr(self, "_ctx_transcription_data", None) or {}
            pace, sil = self._compute_pace_silence_scores(moment, td)
            scores['pace_score'] = max(0.0, min(10.0, float(pace)))
            scores['silence_penalty'] = max(0.0, min(10.0, float(sil)))
        except Exception:
            scores.setdefault('pace_score', 0.0)
            scores.setdefault('silence_penalty', 0.0)

        # Финальная нормализация к шкале 0-10
        for key in list(scores.keys()):
            try:
                scores[key] = min(max(float(scores[key]), -2), 10)  # Разрешаем небольшие отрицательные значения
            except Exception:
                scores[key] = 0.0

        return scores

    def _trim_boring_segments(self, ranked_moments: List[RankedMoment], transcription_data: Dict[str, Any]) -> List[RankedMoment]:
        """Обрезка скучных секунд"""
        try:
            trimmed_moments = []

            for rm in ranked_moments:
                moment = rm.moment

                # Поиск скучных сегментов в моменте
                boring_segments = self._detect_boring_segments_in_moment(moment, transcription_data)

                if boring_segments:
                    # Обрезка скучных сегментов
                    trimmed_moment = self._apply_trimming(moment, boring_segments)
                    if trimmed_moment:
                        rm.moment = trimmed_moment
                        logger.logger.debug(f"Обрезан момент {rm.rank}: {len(boring_segments)} скучных сегментов")

                trimmed_moments.append(rm)

            return trimmed_moments

        except Exception as e:
            logger.logger.error(f"Ошибка при обрезке скучных сегментов: {e}")
            return ranked_moments

    def _detect_boring_segments_in_moment(self, moment: FilmMoment, transcription_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Интеллектуальное обнаружение скучных сегментов в моменте с использованием ИИ-анализа пауз"""
        try:
            # Импортируем анализатор пауз
            from Components.PauseAnalysis import analyze_pauses_in_moment

            # Анализируем паузы в рамках момента
            pause_analyses = analyze_pauses_in_moment(
                moment.start_time,
                moment.end_time,
                transcription_data
            )

            boring_segments = []

            # Преобразуем результаты анализа пауз в формат boring_segments
            for pause_analysis in pause_analyses:
                if pause_analysis.should_trim:
                    boring_segments.append({
                        'start': pause_analysis.start_time,
                        'end': pause_analysis.end_time,
                        'reason': f'intelligent_{pause_analysis.category}',
                        'confidence': pause_analysis.confidence,
                        'importance_score': pause_analysis.importance_score,
                        'should_trim': pause_analysis.should_trim,
                        'ai_reasoning': pause_analysis.reasoning,
                        'duration': pause_analysis.duration
                    })

            # Если ИИ-анализ не дал результатов или отключен, используем легаси-метод
            if not boring_segments and not getattr(self.film_config, 'intelligent_pause_analysis', {}).get('enabled', False):
                logger.logger.info("ИИ-анализ пауз отключен или не дал результатов, используем легаси-метод")
                boring_segments = self._detect_boring_segments_legacy(moment, transcription_data)

            logger.logger.debug(f"Обнаружено {len(boring_segments)} скучных сегментов в моменте "
                              f"({moment.start_time:.1f}s-{moment.end_time:.1f}s)")

            return boring_segments

        except Exception as e:
            logger.logger.error(f"Ошибка при интеллектуальном анализе пауз: {e}. "
                               "Откатываемся на легаси-метод.")
            # Fallback на легаси-метод при ошибках
            return self._detect_boring_segments_legacy(moment, transcription_data)

    def _detect_boring_segments_legacy(self, moment: FilmMoment, transcription_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Старая логика обнаружения скучных сегментов (резервная)"""
        boring_segments = []
        threshold = self.film_config.pause_threshold

        segments = transcription_data.get('segments', [])

        for seg in segments:
            if not isinstance(seg, (list, tuple)) or len(seg) < 3:
                continue

            seg_start = float(seg[1])
            seg_end = float(seg[2])
            seg_text = str(seg[0]).strip()

            # Проверка на ПЕРЕСЕЧЕНИЕ с моментом (не полное вхождение!)
            # Сегмент должен пересекаться с моментом хотя бы частично
            if not (seg_end > moment.start_time and seg_start < moment.end_time):
                continue

            # Ограничиваем сегмент границами момента для корректного анализа
            effective_start = max(seg_start, moment.start_time)
            effective_end = min(seg_end, moment.end_time)
            duration = effective_end - effective_start

            # Критерии скучного сегмента
            # Длинные паузы
            if duration > threshold:
                boring_segments.append({
                    'start': effective_start,
                    'end': effective_end,
                    'reason': 'long_pause',
                    'duration': duration
                })
                continue

            # Филлеры
            filler_words = self.film_config.filler_words
            if any(filler in seg_text.lower() for filler in filler_words):
                boring_segments.append({
                    'start': effective_start,
                    'end': effective_end,
                    'reason': 'filler_words',
                    'text': seg_text
                })

        return boring_segments

    def _apply_trimming(self, moment: FilmMoment, boring_segments: List[Dict[str, Any]]) -> Optional[FilmMoment]:
        """Интеллектуальное применение обрезки к моменту с учетом ИИ-анализа пауз"""
        if not boring_segments:
            return moment

        # Сортировка скучных сегментов по времени
        boring_segments.sort(key=lambda x: x['start'])

        # Создание новых границ момента, исключая скучные сегменты
        new_segments = []
        current_start = moment.start_time

        for boring in boring_segments:
            # Добавляем сегмент до скучного
            if current_start < boring['start']:
                new_segments.append({
                    'start': current_start,
                    'end': boring['start'],
                    'text': 'content'
                })

            # Переходим к следующему после скучного
            current_start = boring['end']

        # Добавляем оставшийся сегмент
        if current_start < moment.end_time:
            new_segments.append({
                'start': current_start,
                'end': moment.end_time,
                'text': 'content'
            })

        # Фильтруем слишком короткие сегменты
        new_segments = [s for s in new_segments if s['end'] - s['start'] > 1.0]

        if not new_segments:
            return None

        # Обновляем момент
        total_duration = sum(s['end'] - s['start'] for s in new_segments)

        # Проверяем минимальную длительность
        min_duration = self.film_config.combo_duration[0] if moment.moment_type == 'COMBO' else self.film_config.single_duration[0]
        if total_duration < min_duration:
            return None

        # Создаем новый момент с объединенными сегментами
        # Добавляем информацию об ИИ-анализе пауз в контекст
        ai_trimmed_count = sum(1 for seg in boring_segments if seg.get('reason', '').startswith('intelligent_'))
        legacy_trimmed_count = len(boring_segments) - ai_trimmed_count

        context_parts = [moment.context]
        if ai_trimmed_count > 0:
            context_parts.append(f"ИИ-обрезка: {ai_trimmed_count} пауз")
        if legacy_trimmed_count > 0:
            context_parts.append(f"Стандартная обрезка: {legacy_trimmed_count} пауз")

        return FilmMoment(
            moment_type=moment.moment_type,
            start_time=new_segments[0]['start'],
            end_time=new_segments[-1]['end'],
            text=moment.text,
            segments=moment.segments,
            context=f" ({'; '.join(context_parts)})"
        )

    def _generate_shorts_from_moments(self, video_path: str, ranked_moments: List[RankedMoment], transcription_data: Dict[str, Any]) -> List[str]:
        """Генерация шортов из найденных моментов"""
        logger.logger.info("=== НАЧАЛО ГЕНЕРАЦИИ ШОРТОВ ===")

        # 1. Логирование входных параметров
        logger.logger.info("--- ВХОДНЫЕ ПАРАМЕТРЫ ---")
        logger.logger.info(f"video_path: {video_path}")
        logger.logger.info(f"video_path существует: {os.path.exists(video_path)}")
        if os.path.exists(video_path):
            video_size = os.path.getsize(video_path) / (1024 * 1024)  # MB
            logger.logger.info(f"Размер видео файла: {video_size:.2f} MB")

        logger.logger.info(f"ranked_moments: {len(ranked_moments)} моментов")
        for i, rm in enumerate(ranked_moments):
            moment = rm.moment
            logger.logger.debug(f"  Момент {i+1}: {moment.moment_type} {moment.start_time:.2f}s-{moment.end_time:.2f}s, score={rm.total_score:.2f}")

        logger.logger.info(f"transcription_data ключи: {list(transcription_data.keys())}")
        if 'segments' in transcription_data:
            logger.logger.info(f"Количество сегментов транскрибации: {len(transcription_data['segments'])}")
        if 'duration' in transcription_data:
            logger.logger.info(f"Длительность видео по транскрибации: {transcription_data['duration']:.2f}s")
        # Валидация источников длительности для шортов
        try:
            _td = float(transcription_data.get('duration_from_transcription', 0.0) or 0.0)
            _fd = float(transcription_data.get('duration_from_ffprobe', 0.0) or 0.0)
            _ch = float(transcription_data.get('duration', 0.0) or 0.0)
            logger.logger.info(f"[VALIDATION] duration sources: transcription={_td:.2f}, ffprobe={_fd:.2f}, chosen=ctx.video_duration={_ch:.2f}")
        except Exception:
            pass

        # 2. Валидация входных данных
        logger.logger.info("--- ВАЛИДАЦИЯ ВХОДНЫХ ДАННЫХ ---")
        if not video_path or not os.path.exists(video_path):
            logger.logger.error(f"❌ Видео файл не найден: {video_path}")
            return []

        if not ranked_moments:
            logger.logger.warning("⚠️ Нет моментов для обработки")
            return []

        if not transcription_data:
            logger.logger.warning("⚠️ Нет данных транскрибации")
            return []

        logger.logger.info("✅ Валидация входных данных пройдена")

        generated_shorts = []
        video_id = os.path.splitext(os.path.basename(video_path))[0]
        logger.logger.info(f"Извлечен video_id: {video_id}")

        # 3. Получение размеров видео для валидации
        logger.logger.info("--- ПОЛУЧЕНИЕ РАЗМЕРОВ ВИДЕО ---")
        try:
            logger.logger.debug(f"Вызов get_video_dimensions для файла: {video_path}")
            initial_width, initial_height = get_video_dimensions(video_path)
            logger.logger.info(f"✅ Размеры видео получены: {initial_width}x{initial_height}")
            if not initial_width or not initial_height:
                logger.logger.error("❌ Не удалось получить корректные размеры видео (один из размеров равен 0 или None)")
                return []
        except Exception as e:
            logger.logger.error(f"❌ Ошибка при получении размеров видео: {e}")
            import traceback
            logger.logger.error(f"Traceback: {traceback.format_exc()}")
            return []

        # 4. Выбор топ моментов для обработки
        logger.logger.info("--- ВЫБОР ТОП МОМЕНТОВ ---")
        top_k = int(max(1, getattr(self.film_config, "generator_top_k", getattr(self.film_config, "target_shorts_count", 30))))
        top_moments = ranked_moments[:top_k]
        logger.logger.info(f"Выбрано топ-{len(top_moments)} моментов из {len(ranked_moments)} (максимум {top_k})")

        # 5. Дополнительная валидация моментов
        logger.logger.info("--- ДОПОЛНИТЕЛЬНАЯ ВАЛИДАЦИЯ МОМЕНТОВ ---")

        # Извлекаем только FilmMoment объекты для валидации
        moments_to_validate = [rm.moment for rm in top_moments]
        video_duration = transcription_data.get('duration', None)

        # Вызываем метод валидации
        valid_moments_data = self._validate_moment_data(moments_to_validate, video_duration)

        # Создаем список RankedMoment только из валидных моментов
        valid_moments = []
        for rm in top_moments:
            if rm.moment in valid_moments_data:
                valid_moments.append(rm)

        if not valid_moments:
            logger.logger.error("❌ Нет валидных моментов для генерации шортов после всех проверок")
            return []

        top_moments = valid_moments
        logger.logger.info(f"✅ После валидации: {len(top_moments)} моментов готовы к обработке")

        # 6. Создание контекста обработки
        logger.logger.info("--- СОЗДАНИЕ КОНТЕКСТА ОБРАБОТКИ ---")
        processing_context = self._create_processing_context_for_moment(
            video_path, video_id, transcription_data, initial_width, initial_height
        )
        logger.logger.info("✅ Контекст обработки создан")

        # 7. Основной цикл обработки моментов
        logger.logger.info("--- НАЧАЛО ОБРАБОТКИ МОМЕНТОВ ---")
        for i, rm in enumerate(top_moments):
            try:
                moment = rm.moment
                logger.logger.info(f"=== ОБРАБОТКА МОМЕНТА {i+1}/{len(top_moments)} ===")
                logger.logger.info(f"Момент {rm.rank}: таймкод {moment.start_time:.2f}s - {moment.end_time:.2f}s")
                logger.logger.info(f"Длительность момента: {moment.end_time - moment.start_time:.2f}s")
                logger.logger.info(f"Тип момента: {moment.moment_type}")
                logger.logger.info(f"Балл: {rm.total_score:.2f}")
                logger.logger.info(f"Текст: {moment.text[:200]}...")
                if len(moment.text) > 200:
                    logger.logger.debug(f"Полный текст: {moment.text}")

                # Финальная проверка корректности таймкодов
                if moment.start_time >= moment.end_time:
                    logger.logger.error(f"❌ Некорректные таймкоды: start={moment.start_time} >= end={moment.end_time}")
                    continue

                duration = moment.end_time - moment.start_time
                if duration < 1.0:
                    logger.logger.warning(f"⚠️ Момент слишком короткий: {duration:.2f}s (минимум 1.0s)")

                # Создаем структуру highlight для совместимости с process_highlight
                highlight_item = {
                    'start': moment.start_time,
                    'end': moment.end_time,
                    'caption_with_hashtags': f"Film Moment {rm.rank}: {moment.text[:100]}...",
                    'segment_text': moment.text,
                    '_seq': i + 1,
                    '_total': len(top_moments)
                }
                logger.logger.debug(f"Создан highlight_item: {highlight_item}")

                # Генерируем шорт
                logger.logger.info(f"Вызов _process_moment_to_short для момента {rm.rank}")
                short_path = self._process_moment_to_short(processing_context, highlight_item, i + 1)

                if short_path:
                    generated_shorts.append(short_path)
                    logger.logger.info(f"✅ УСПЕШНО сгенерирован шорт: {short_path}")

                    # Проверяем, что файл действительно создан
                    if os.path.exists(short_path):
                        file_size = os.path.getsize(short_path) / (1024 * 1024)  # MB
                        logger.logger.info(f"✅ Файл шорта создан: {file_size:.2f} MB")
                    else:
                        logger.logger.warning(f"⚠️ Файл шорта не найден после создания: {short_path}")
                else:
                    logger.logger.error(f"❌ НЕ УДАЛОСЬ сгенерировать шорт для момента {rm.rank}")

            except Exception as e:
                logger.logger.error(f"❌ ОШИБКА при генерации шорта для момента {rm.rank}: {e}")
                import traceback
                logger.logger.error(f"Traceback: {traceback.format_exc()}")
                continue

        # 8. Завершение генерации шортов
        logger.logger.info("=== ЗАВЕРШЕНИЕ ГЕНЕРАЦИИ ШОРТОВ ===")
        logger.logger.info(f"Всего сгенерировано шортов: {len(generated_shorts)}")

        if generated_shorts:
            logger.logger.info("Список сгенерированных шортов:")
            for i, short_path in enumerate(generated_shorts, 1):
                exists = os.path.exists(short_path)
                size = os.path.getsize(short_path) / (1024 * 1024) if exists else 0
                logger.logger.info(f"  {i}. {short_path} ({size:.2f} MB, существует: {exists})")
        else:
            logger.logger.warning("⚠️ Ни один шорт не был сгенерирован")

        logger.logger.info("✅ Генерация шортов из моментов завершена")
        return generated_shorts

    def _validate_moment_data(self, moments: List[FilmMoment], video_duration: float = None) -> List[FilmMoment]:
        """Валидация данных моментов с подробным логированием"""
        logger.logger.info("=== ВАЛИДАЦИЯ ДАННЫХ МОМЕНТОВ ===")
        logger.logger.info(f"Вход: {len(moments)} моментов, длительность видео: {video_duration}")

        if not moments:
            logger.logger.warning("⚠️ Список моментов пуст")
            return []

        valid_moments = []
        invalid_count = 0

        for i, moment in enumerate(moments):
            logger.logger.debug(f"Валидация момента {i+1}: {moment.moment_type} {moment.start_time:.2f}s-{moment.end_time:.2f}s")

            is_valid = True
            issues = []

            # Проверка типа момента
            if moment.moment_type not in ['COMBO', 'SINGLE']:
                issues.append(f"неверный тип момента: {moment.moment_type}")
                is_valid = False

            # Проверка таймкодов
            if moment.start_time < 0:
                issues.append(f"отрицательное время начала: {moment.start_time}")
                is_valid = False

            if moment.end_time <= moment.start_time:
                issues.append(f"некорректный интервал: {moment.start_time} >= {moment.end_time}")
                is_valid = False

            # Проверка длительности
            duration = moment.end_time - moment.start_time
            if duration < 1.0:
                issues.append(f"слишком короткий: {duration:.2f}s (минимум 1.0s)")
                is_valid = False
            elif duration > 120.0:  # максимум 2 минуты
                issues.append(f"слишком длинный: {duration:.2f}s (максимум 120.0s)")
                is_valid = False

            # Проверка границ видео
            if video_duration and moment.end_time > video_duration:
                issues.append(f"выходит за границы видео: {moment.end_time:.2f}s > {video_duration:.2f}s")
                # Корректируем вместо инвалидации
                moment.end_time = video_duration
                logger.logger.info(f"  Скорректирован конец момента до {moment.end_time:.2f}s")

            # Проверка текста
            if not moment.text or not moment.text.strip():
                issues.append("пустой текст")
                is_valid = False

            # Проверка сегментов для COMBO
            if moment.moment_type == 'COMBO':
                if not moment.segments:
                    issues.append("COMBO момент без суб-сегментов")
                    is_valid = False
                elif len(moment.segments) < 2:
                    issues.append(f"COMBO момент с недостаточным количеством сегментов: {len(moment.segments)}")
                    is_valid = False

            if is_valid:
                valid_moments.append(moment)
                logger.logger.debug(f"✅ Момент {i+1} прошел валидацию")
            else:
                invalid_count += 1
                logger.logger.warning(f"❌ Момент {i+1} отклонен: {', '.join(issues)}")

        logger.logger.info(f"Результат валидации: {len(valid_moments)} валидных, {invalid_count} отклоненных из {len(moments)}")
        return valid_moments

    def _extract_video_segment(self, input_path: str, output_path: str, start_time: float, end_time: float, width: int, height: int) -> bool:
        """Извлечение сегмента видео с подробным логированием"""
        logger.logger.info("=== ИЗВЛЕЧЕНИЕ СЕГМЕНТА ВИДЕО ===")
        logger.logger.info(f"Входной файл: {input_path}")
        logger.logger.info(f"Выходной файл: {output_path}")
        logger.logger.info(f"Таймкоды: {start_time:.2f}s - {end_time:.2f}s")
        logger.logger.info(f"Размеры: {width}x{height}")

        # 1. Валидация входных параметров
        logger.logger.debug("Валидация входных параметров...")
        if not input_path or not os.path.exists(input_path):
            logger.logger.error(f"❌ Входной файл не найден: {input_path}")
            return False

        if not output_path:
            logger.logger.error("❌ Не указан выходной файл")
            return False

        if start_time < 0:
            logger.logger.error(f"❌ Отрицательное время начала: {start_time}")
            return False

        if end_time <= start_time:
            logger.logger.error(f"❌ Некорректный интервал: {start_time} >= {end_time}")
            return False

        duration = end_time - start_time
        if duration < 0.1:
            logger.logger.error(f"❌ Слишком короткий сегмент: {duration:.2f}s")
            return False

        logger.logger.info(f"✅ Валидация параметров пройдена, длительность сегмента: {duration:.2f}s")

        # 2. Проверка входного файла
        try:
            input_size = os.path.getsize(input_path) / (1024 * 1024)  # MB
            logger.logger.info(f"Размер входного файла: {input_size:.2f} MB")
        except Exception as e:
            logger.logger.warning(f"Не удалось получить размер входного файла: {e}")

        # 3. Вызов crop_video
        logger.logger.info("Вызов crop_video для извлечения сегмента...")
        try:
            success = crop_video(input_path, output_path, start_time, end_time, width, height)

            if success:
                logger.logger.info("✅ crop_video выполнен успешно")
            else:
                logger.logger.error("❌ crop_video вернул False")
                return False

        except Exception as e:
            logger.logger.error(f"❌ Исключение при вызове crop_video: {e}")
            import traceback
            logger.logger.error(f"Traceback: {traceback.format_exc()}")
            return False

        # 4. Проверка результата
        logger.logger.info("Проверка созданного файла...")
        if not os.path.exists(output_path):
            logger.logger.error(f"❌ Выходной файл не найден: {output_path}")
            return False

        try:
            output_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
            logger.logger.info(f"✅ Выходной файл создан: {output_path} ({output_size:.2f} MB)")

            # Проверка на пустой файл
            if output_size < 0.1:  # менее 100KB
                logger.logger.error(f"❌ Выходной файл слишком маленький: {output_size:.2f} MB")
                try:
                    os.remove(output_path)
                    logger.logger.info(f"Удален пустой файл: {output_path}")
                except Exception as e:
                    logger.logger.warning(f"Не удалось удалить пустой файл: {e}")
                return False

            logger.logger.info("✅ Сегмент видео успешно извлечен и проверен")
            return True

        except Exception as e:
            logger.logger.error(f"❌ Ошибка при проверке выходного файла: {e}")
            return False

    def _add_captions_to_short(self, input_video: str, output_video: str, transcription_segments: List[List], start_time: float, end_time: float, style_cfg=None) -> bool:
        """Добавление субтитров к видео с подробным логированием"""
        logger.logger.info("=== ДОБАВЛЕНИЕ СУБТИТРОВ К ВИДЕО ===")
        logger.logger.info(f"Входное видео: {input_video}")
        logger.logger.info(f"Выходное видео: {output_video}")
        logger.logger.info(f"Таймкоды: {start_time:.2f}s - {end_time:.2f}s")
        logger.logger.info(f"Количество сегментов транскрибации: {len(transcription_segments)}")

        # 1. Валидация входных параметров
        logger.logger.debug("Валидация входных параметров...")
        if not input_video or not os.path.exists(input_video):
            logger.logger.error(f"❌ Входное видео не найдено: {input_video}")
            return False

        if not output_video:
            logger.logger.error("❌ Не указан путь выходного видео")
            return False

        if not transcription_segments:
            logger.logger.warning("⚠️ Нет сегментов транскрибации для субтитров")
            # Всё равно пытаемся создать видео без субтитров
            logger.logger.info("Попытка создать видео без субтитров...")

        logger.logger.info(f"✅ Валидация параметров пройдена")

        # 2. Фильтрация релевантных сегментов
        if transcription_segments:
            logger.logger.info("Фильтрация релевантных сегментов...")
            relevant_segments = []
            for seg in transcription_segments:
                try:
                    # Обработка нормализованного формата [text, start, end]
                    if isinstance(seg, (list, tuple)) and len(seg) >= 3:
                        seg_start = float(seg[1])
                        seg_end = float(seg[2])
                        # Проверяем пересечение с моментом
                        if seg_end > start_time and seg_start < end_time:
                            relevant_segments.append(seg)
                    # Обработка dict формата (если вдруг попал)
                    elif isinstance(seg, dict):
                        seg_start = float(seg.get("start", 0.0))
                        seg_end = float(seg.get("end", 0.0))
                        if seg_end > start_time and seg_start < end_time:
                            # Конвертируем в список для совместимости
                            text = str(seg.get("text", ""))
                            relevant_segments.append([text, seg_start, seg_end])
                    else:
                        logger.logger.debug(f"Пропускаем сегмент неизвестного формата: {type(seg)}")
                except Exception as e:
                    logger.logger.debug(f"Ошибка при обработке сегмента: {e}, пропускаем")
                    continue

            logger.logger.info(f"Релевантных сегментов: {len(relevant_segments)} из {len(transcription_segments)}")
            if relevant_segments:
                logger.logger.debug(f"Примеры релевантных сегментов: {relevant_segments[:2]}")

            transcription_segments = relevant_segments
        else:
            logger.logger.warning("⚠️ Сегменты транскрибации отсутствуют")

        # 3. Вызов burn_captions
        logger.logger.info("Вызов burn_captions...")
        try:
            # Создаем временный файл для аудио (как в оригинальном коде)
            temp_audio = input_video  # Используем входное видео как источник аудио

            success = burn_captions(
                input_video, temp_audio, transcription_segments,
                start_time, end_time, output_video, style_cfg=style_cfg
            )

            if success:
                logger.logger.info("✅ burn_captions выполнен успешно")
            else:
                logger.logger.error("❌ burn_captions вернул False")
                return False

        except Exception as e:
            logger.logger.error(f"❌ Исключение при вызове burn_captions: {e}")
            import traceback
            logger.logger.error(f"Traceback: {traceback.format_exc()}")
            return False

        # 4. Проверка результата
        logger.logger.info("Проверка созданного видео с субтитрами...")
        if not os.path.exists(output_video):
            logger.logger.error(f"❌ Выходное видео не найдено: {output_video}")
            return False

        try:
            output_size = os.path.getsize(output_video) / (1024 * 1024)  # MB
            logger.logger.info(f"✅ Видео с субтитрами создано: {output_video} ({output_size:.2f} MB)")

            # Проверка на пустой файл
            if output_size < 0.1:  # менее 100KB
                logger.logger.error(f"❌ Выходное видео слишком маленькое: {output_size:.2f} MB")
                try:
                    os.remove(output_video)
                    logger.logger.info(f"Удален пустой файл: {output_video}")
                except Exception as e:
                    logger.logger.warning(f"Не удалось удалить пустой файл: {e}")
                return False

            logger.logger.info("✅ Субтитры успешно добавлены к видео")
            return True

        except Exception as e:
            logger.logger.error(f"❌ Ошибка при проверке выходного видео: {e}")
            return False

    def _get_video_duration_from_transcription(self, ctx) -> Optional[float]:
        """Получение длительности видео из данных транскрибации с улучшенной логикой"""
        try:
            # 1. Сначала пробуем получить из word_level_transcription
            if ctx.word_level_transcription and 'segments' in ctx.word_level_transcription:
                segments = ctx.word_level_transcription['segments']
                if segments:
                    # Берем время окончания последнего сегмента
                    last_segment = segments[-1]
                    if isinstance(last_segment, dict) and 'end' in last_segment:
                        duration = float(last_segment['end'])
                        logger.logger.debug(f"Длительность из word_level_transcription: {duration:.2f}s")
                        return duration

            # 2. Пробуем из обычных segments
            if ctx.transcription_segments:
                # Ищем последний сегмент с корректными данными
                for seg in reversed(ctx.transcription_segments):
                    if isinstance(seg, (list, tuple)) and len(seg) >= 3:
                        try:
                            end_time = float(seg[2])
                            if end_time > 0:
                                logger.logger.debug(f"Длительность из transcription_segments: {end_time:.2f}s")
                                return end_time
                        except (ValueError, IndexError):
                            continue

            # 3. Fallback: пытаемся получить через ffprobe
            try:
                duration = self._get_video_duration(ctx.video_path)
                if duration and duration > 0:
                    logger.logger.debug(f"Длительность через ffprobe: {duration:.2f}s")
                    return duration
            except Exception as e:
                logger.logger.warning(f"Не удалось получить длительность через ffprobe: {e}")

            logger.logger.warning("Не удалось определить длительность видео из доступных источников")
            return None

        except Exception as e:
            logger.logger.error(f"Ошибка при определении длительности видео: {e}")
            return None

    def _create_processing_context_for_moment(self, video_path: str, video_id: str, transcription_data: Dict[str, Any], width: int, height: int):
        """Создание контекста обработки для генерации шорта"""
        # Импортируем необходимые классы
        from Components.Database import VideoDatabase

        class MockProcessingContext:
            def __init__(self, video_path, video_id, transcription_data, width, height, config):
                self.video_path = video_path
                self.video_id = video_id
                self.transcription_segments = transcription_data.get('segments', [])
                self.word_level_transcription = transcription_data.get('word_level', {})
                self.initial_width = width
                self.initial_height = height
                self.cfg = config
                self.db = VideoDatabase()
                self.outputs = []
                # Единая длительность видео для всего пайплайна
                try:
                    self.video_duration = float(transcription_data.get('duration', 0.0) or 0.0)
                except Exception:
                    self.video_duration = 0.0
                # Источники длительности (для диагностики)
                try:
                    self.duration_from_transcription = float(transcription_data.get('duration_from_transcription', 0.0) or 0.0)
                except Exception:
                    self.duration_from_transcription = 0.0
                try:
                    self.duration_from_ffprobe = float(transcription_data.get('duration_from_ffprobe', 0.0) or 0.0)
                except Exception:
                    self.duration_from_ffprobe = 0.0

        return MockProcessingContext(video_path, video_id, transcription_data, width, height, self.config)

    def _process_moment_to_short(self, ctx, highlight_item, seq: int) -> Optional[str]:
        """Обработка момента в шорт (ИСПРАВЛЕНО для поддержки анимированных субтитров)"""
        logger.logger.info(f"--- НАЧАЛО ОБРАБОТКИ МОМЕНТА {seq} ---")

        final_output = None
        temp_segment = None
        cropped_vertical = None

        try:
            # 1. Извлечение и валидация таймкодов
            start = float(highlight_item["start"])
            stop = float(highlight_item["end"])
            adjusted_stop = math.ceil(stop)
            logger.logger.info(f"Таймкоды: {start:.2f}s - {adjusted_stop:.2f}s (округлено с {stop:.2f}s)")

            if adjusted_stop <= start:
                logger.logger.error(f"❌ Некорректные таймкоды: start={start} >= end={adjusted_stop}")
                return None

            # 2. Определение путей файлов
            base_name = os.path.splitext(os.path.basename(ctx.video_path))[0]
            # Используем централизованную функцию для имен
            final_output, _ = build_short_output_name(base_name, seq, ctx.cfg.processing.shorts_dir)
            
            output_base = f"{base_name}_film_moment_{seq}"
            temp_segment = os.path.join(ctx.cfg.processing.videos_dir, f"{output_base}_temp.mp4")
            cropped_vertical = os.path.join(ctx.cfg.processing.videos_dir, f"{output_base}_vertical.mp4")

            os.makedirs(ctx.cfg.processing.videos_dir, exist_ok=True)
            os.makedirs(ctx.cfg.processing.shorts_dir, exist_ok=True)

            logger.logger.info("Пути файлов:")
            logger.logger.info(f"  Временный сегмент: {temp_segment}")
            logger.logger.info(f"  Вертикальный кроп: {cropped_vertical}")
            logger.logger.info(f"  Финальный шорт: {final_output}")

            # --- ШАГ 1: Извлечение сегмента видео ---
            extract_success = self._extract_video_segment(
                ctx.video_path, temp_segment, start, adjusted_stop, ctx.initial_width, ctx.initial_height
            )
            if not extract_success:
                logger.logger.error(f"❌ Не удалось извлечь сегмент для момента {seq}")
                self._cleanup_temp_files([temp_segment], f" после неудачного извлечения сегмента {seq}")
                return None

            # --- ШАГ 2: Создание вертикального кропа ---
            crop_mode = ctx.cfg.processing.crop_mode
            crop_function = crop_to_70_percent_with_blur if crop_mode == "70_percent_blur" else crop_to_vertical_average_face
            
            vert_crop_path = self._safe_file_operation(
                f"создание вертикального кропа для момента {seq}",
                crop_function,
                temp_segment, cropped_vertical
            )
            if not vert_crop_path:
                logger.logger.error(f"❌ Не удалось создать вертикальный кроп для момента {seq}")
                self._cleanup_temp_files([temp_segment], f" после неудачного кропа {seq}")
                return None
            
            # --- ШАГ 3: ДОБАВЛЕНИЕ СУБТИТРОВ (ИСПРАВЛЕНО) ---
            captioning_success = False
            use_animated = ctx.cfg.processing.use_animated_captions

            logger.logger.info(f"Режим субтитров: {'Анимированные' if use_animated else 'Статичные (ASS)'}")

            if use_animated:
                # Логика для анимированных субтитров
                # Подготовка данных на уровне слов для сегмента
                transcription_result = prepare_words_for_segment(
                    ctx.word_level_transcription, start, adjusted_stop
                )

                if transcription_result and transcription_result.get("segments"):
                    # Получение метаданных для выделения (тон, ключевые слова, эмодзи)
                    segment_text = highlight_item.get('segment_text', '')
                    meta = compute_tone_and_keywords(segment_text) if segment_text else {}
                    
                    cfg_emoji = getattr(ctx.cfg.captions, "emoji", None)
                    if cfg_emoji and getattr(cfg_emoji, "enabled", False) and segment_text:
                        tone_val = meta.get("tone", "neutral")
                        max_per = int(getattr(cfg_emoji, "max_per_short", 0) or 0)
                        emojis = compute_emojis_for_segment(segment_text, tone_val, max_per)
                        meta["emojis"] = emojis

                    captioning_success = animate_captions(
                        cropped_vertical,
                        temp_segment,  # Источник аудио
                        transcription_result,
                        final_output,
                        style_cfg=ctx.cfg.captions,
                        highlight_meta=meta
                    )
                else:
                    logger.logger.warning("Нет данных на уровне слов для анимации, пропуск.")

            else:
                # Логика для статичных субтитров (ASS)
                captioning_success = burn_captions(
                    cropped_vertical,
                    temp_segment, # Источник аудио
                    ctx.transcription_segments,
                    start,
                    adjusted_stop,
                    final_output,
                    style_cfg=ctx.cfg.captions
                )
            
            # --- ФИНАЛИЗАЦИЯ ---
            if captioning_success:
                logger.logger.info(f"✅ Успешно обработан момент {seq}. Финальный файл: {final_output}")
                ctx.db.add_highlight(
                    ctx.video_id, start, adjusted_stop, final_output,
                    segment_text=highlight_item.get('segment_text', ''),
                    caption_with_hashtags=highlight_item.get('caption_with_hashtags', '')
                )
                return final_output
            else:
                logger.logger.error(f"❌ Не удалось добавить субтитры для момента {seq}")
                return None

        except Exception as e:
            logger.logger.error(f"❌ КРИТИЧЕСКАЯ ОШИБКА при обработке момента {seq}: {e}")
            import traceback
            logger.logger.error(f"Traceback: {traceback.format_exc()}")
            return None
        finally:
            # Очистка временных файлов
            self._cleanup_temp_files([temp_segment, cropped_vertical], f" после обработки момента {seq}")

    def _safe_file_operation(self, operation_name: str, operation_func, *args, **kwargs) -> Optional[Any]:
        """Безопасное выполнение файловой операции с graceful degradation"""
        try:
            logger.logger.debug(f"Выполнение операции: {operation_name}")
            result = operation_func(*args, **kwargs)
            logger.logger.debug(f"✅ Операция {operation_name} выполнена успешно")
            return result
        except Exception as e:
            logger.logger.warning(f"⚠️ Операция {operation_name} завершилась с ошибкой: {e}")
            logger.logger.warning(f"Продолжаем выполнение несмотря на ошибку в {operation_name}")
            return None

    def _cleanup_temp_files(self, files_to_clean: List[str], context: str = "") -> None:
        """Безопасная очистка временных файлов"""
        if not files_to_clean:
            return

        logger.logger.info(f"Очистка временных файлов{context}...")
        for temp_file in files_to_clean:
            if temp_file and os.path.exists(temp_file):
                try:
                    file_size = os.path.getsize(temp_file) / (1024 * 1024)  # MB
                    os.remove(temp_file)
                    logger.logger.info(f"✅ Удален временный файл: {temp_file} ({file_size:.2f} MB)")
                except Exception as e:
                    logger.logger.warning(f"⚠️ Не удалось удалить временный файл {temp_file}: {e}")

    def _create_result(self, video_path: str, ranked_moments: List[RankedMoment], transcription_data: Dict[str, Any], generated_shorts: List[str] = None) -> FilmAnalysisResult:
        """Создание финального результата анализа"""
        video_id = os.path.splitext(os.path.basename(video_path))[0]
        duration = transcription_data.get('duration', 0)

        # Формирование keep_ranges
        k = int(max(1, getattr(self.film_config, "generator_top_k", getattr(self.film_config, "target_shorts_count", 30))))
        top_n = min(k, len(ranked_moments))
        keep_ranges = []
        for rm in ranked_moments[:top_n]:
            keep_ranges.append({
                'start': rm.moment.start_time,
                'end': rm.moment.end_time,
                'type': rm.moment.moment_type,
                'score': round(rm.total_score, 2),
                'text': rm.moment.text[:200] + '...' if len(rm.moment.text) > 200 else rm.moment.text,
                'keywords': rm.moment.keywords or []
            })

        # Формирование scores
        scores = []
        for rm in ranked_moments[:top_n]:
            score_dict = {
                'moment_id': f"{rm.moment.moment_type.lower()}_{rm.rank}",
                'total': round(rm.total_score, 2),
                'keywords': rm.moment.keywords or []
            }
            score_dict.update({k: round(v, 2) for k, v in rm.scores.items()})
            scores.append(score_dict)

        # Превью текста
        total_duration = sum(r['end'] - r['start'] for r in keep_ranges)
        preview_text = f"Фильм содержит {len(keep_ranges)} лучших моментов длительностью {total_duration:.1f} минут из оригинальных {duration/60:.1f} минут видео."

        # Риски
        risks = []
        if len(keep_ranges) < 3:
            risks.append("Мало подходящих моментов найдено")
        if duration > 3600:  # > 1 час
            risks.append("Длинное видео - возможны проблемы с обработкой")
        if not keep_ranges:
            risks.append("Не найдено ни одного подходящего момента")

        # Метаданные
        if generated_shorts is None:
            generated_shorts = []

        info = getattr(self, "_last_ranking_info", {}) or {}
        rc = getattr(self.film_config, "ranking", {}) or {}
        try:
            min_thr = rc.get("min_quality_threshold", getattr(self.film_config, "min_quality_score", 0.5))
        except Exception:
            min_thr = getattr(self.film_config, "min_quality_score", 0.5)
        soft_min = rc.get("soft_min_quality", 0.35)

        metadata = {
            'processed_at': datetime.now().isoformat(),
            'model_version': self.config.llm.model_name,
            'total_segments_analyzed': len(transcription_data.get('segments', [])),
            'video_duration': duration,
            'video_duration_sec': duration,
            'moments_found': len(ranked_moments),
            'shorts_generated': len(generated_shorts),
            'selection_strategy': info.get('selection_strategy', 'quality_threshold'),
            'ranking_system': 'keyword_matching',  # Новая система ранжирования
            'thresholds': {
                'min_quality_threshold': float(min_thr),
                'soft_min_quality': float(soft_min),
            },
            'film_config': {
                'min_quality_score': self.film_config.min_quality_score,
                'generate_shorts': self.film_config.generate_shorts,
                'pause_threshold': self.film_config.pause_threshold
            }
        }

        return FilmAnalysisResult(
            video_id=video_id,
            duration=duration,
            keep_ranges=keep_ranges,
            scores=scores,
            preview_text=preview_text,
            risks=risks,
            metadata=metadata,
            generated_shorts=generated_shorts
        )
# ========== Film Mode v2 helpers (windowed extraction, dedupe, extended scoring) ==========

    def _iter_windows(self, duration_sec: float, win_min: int, overlap_min: int):
        """Итератор окон (start, end) в секундах по длительности видео."""
        try:
            W = max(60.0, float(win_min) * 60.0)
        except Exception:
            W = 12 * 60.0
        try:
            O = max(0.0, float(overlap_min) * 60.0)
        except Exception:
            O = 3 * 60.0

        if duration_sec is None or duration_sec <= 0:
            return
        if W <= 0:
            W = min(12 * 60.0, max(60.0, duration_sec))

        step = max(1.0, W - O)
        t = 0.0
        while t < duration_sec:
            start = t
            end = min(duration_sec, t + W)
            if end - start >= 10.0:  # пропускаем слишком короткие окна
                yield (start, end)
            if end >= duration_sec:
                break
            t += step

    def _extract_film_moments_windowed(self, transcription_data: Dict[str, Any]) -> List[FilmMoment]:
        """
        Оконное извлечение кандидатов: идем по таймлайну окнами и для каждого окна просим LLM
        вернуть до K лучших моментов в рамках этого окна.
        """
        moments: List[FilmMoment] = []
        try:
            duration = float(transcription_data.get('duration', 0.0) or 0.0)
        except Exception:
            duration = 0.0

        if duration <= 0.0:
            logger.logger.warning("duration<=0 для windowed-извлечения — возвращаю пустой список")
            return []

        # Готовим segments в dict-формате для build_transcription_prompt
        segments_legacy = transcription_data.get('segments', []) or []
        segs_dict: List[Dict[str, Any]] = []
        for seg in segments_legacy:
            try:
                if isinstance(seg, (list, tuple)) and len(seg) >= 3:
                    segs_dict.append({'text': str(seg[0]), 'start': float(seg[1]), 'end': float(seg[2])})
                elif isinstance(seg, dict):
                    st = float(seg.get('start', 0.0) or 0.0)
                    en = float(seg.get('end', 0.0) or 0.0)
                    tx = str(seg.get('text', '') or '')
                    segs_dict.append({'text': tx, 'start': st, 'end': en})
            except Exception:
                continue

        win_min = getattr(self.film_config, "window_minutes", 12)
        ov_min = getattr(self.film_config, "window_overlap_minutes", 3)
        k_per_win = int(max(1, getattr(self.film_config, "max_moments_per_window", 12)))
        # Увеличиваем лимит на окно для достижения целевых 30 шортов
        target_per_window = max(k_per_win, self.film_config.target_shorts_count // 4)  # минимум 4 окна
        k_per_win = min(target_per_window, 20)  # но не больше 20 на окно

        # Общая системная инструкция для окна (с выделением ключевых слов)
        def _build_window_system_instruction(w_start: float, w_end: float) -> str:
            return f"""
        Ты — эксперт по анализу фильмов для создания вирусных Shorts. Твоя задача — в пределах окна [{w_start:.2f}s, {w_end:.2f}s] найти лучшие моменты двух типов и вернуть ТОЛЬКО JSON-массив:

        Типы моментов:
        - COMBO (10–20 сек): 2–4 суб-сегмента одной сцены в хронологическом порядке
        - SINGLE (30–60 сек): самодостаточный момент с мини-аркой (завязка → нарастание → развязка)

        Для КАЖДОГО момента выдели 3-7 ключевых слов, которые характеризуют его суть и потенциал для вирусности.

        Формат каждого объекта:
        - moment_type: "COMBO" | "SINGLE"
        - start_time: секунды (абсолютные таймкоды фильма)
        - end_time: секунды
        - text: краткий текст/реплики момента
        - context: почему момент подходит (кратко)
        - keywords: массив строк с ключевыми словами (3-7 слов)
        Для COMBO:
        - segments: массив объектов {{"start":sec,"end":sec,"text":"..."}}

        Условия:
        - Верни до {k_per_win} лучших моментов ТОЛЬКО в границах окна.
        - Таймкоды указывай абсолютные, соответствующие фильму.
        - Строго JSON без пояснений.
        - Стремись найти максимум качественных моментов для достижения цели в 30 шортов.
        """

        # Идем по окнам
        total_windows = 0
        for w_start, w_end in self._iter_windows(duration, win_min, ov_min):
            total_windows += 1
            # Подтягиваем сегменты окна
            win_segments = []
            for s in segs_dict:
                try:
                    if s['end'] > w_start and s['start'] < w_end:
                        win_segments.append(s)
                except Exception:
                    continue

            if not win_segments:
                logger.logger.debug(f"[WINDOW] пустое окно без сегментов: {w_start:.2f}-{w_end:.2f}")
                continue

            try:
                # Текст окна
                trans_text = build_transcription_prompt(win_segments)
                system_instruction = _build_window_system_instruction(w_start, w_end)
                generation_config = make_generation_config(system_instruction, temperature=self.film_config.llm_temperature)

                response = call_llm_with_film_mode_retry(
                    system_instruction=None,
                    content=trans_text,
                    generation_config=generation_config,
                    model=self.film_config.llm_model,
                    max_api_attempts=5,
                )
                if not response or not getattr(response, "text", None):
                    logger.logger.warning(f"[WINDOW] пустой ответ LLM на окно {w_start:.2f}-{w_end:.2f}")
                    continue

                resp = response.text.strip()
                if resp.startswith("```json"):
                    resp = resp[7:].strip()
                if resp.endswith("```"):
                    resp = resp[:-3].strip()

                try:
                    arr = json.loads(resp)
                except json.JSONDecodeError as je:
                    logger.logger.warning(f"[WINDOW] JSONDecodeError в окне {w_start:.2f}-{w_end:.2f}: {je}")
                    logger.logger.debug(f"RAW: {resp[:300]}...")
                    continue

                if not isinstance(arr, list):
                    logger.logger.warning(f"[WINDOW] LLM вернул не массив в окне {w_start:.2f}-{w_end:.2f}")
                    continue

                for i, item in enumerate(arr):
                    try:
                        if not isinstance(item, dict):
                            continue
                        st = float(item.get('start_time', 0.0) or 0.0)
                        en = float(item.get('end_time', 0.0) or 0.0)
                        # Жестко клиппим в границы видео
                        st = max(0.0, min(st, duration))
                        en = max(0.0, min(en, duration))
                        if en <= st:
                            continue
                        mtype = str(item.get('moment_type', 'SINGLE') or 'SINGLE').upper()
                        txt = str(item.get('text', '') or '')
                        ctx = str(item.get('context', '') or '')
                        segs = item.get('segments', [])
                        keywords = item.get('keywords', [])
                        # Валидация по типу
                        if mtype == 'COMBO':
                            if not isinstance(segs, list) or len(segs) < getattr(self.film_config, "min_combo_segments", 2):
                                # если плохие sub-сегменты — конвертируем в SINGLE
                                mtype = 'SINGLE'
                        # Приводим sub-сегменты к ожидаемому формату (если есть)
                        norm_segments: List[Dict[str, Any]] = []
                        if isinstance(segs, list):
                            for ss in segs:
                                try:
                                    if isinstance(ss, dict):
                                        sst = float(ss.get('start', st))
                                        sse = float(ss.get('end', en))
                                        sst = max(0.0, min(sst, duration))
                                        sse = max(0.0, min(sse, duration))
                                        if sse > sst:
                                            norm_segments.append({'start': sst, 'end': sse, 'text': str(ss.get('text', '') or '')})
                                except Exception:
                                    continue

                        fm = FilmMoment(
                            moment_type=mtype,
                            start_time=st,
                            end_time=en,
                            text=txt,
                            segments=norm_segments,
                            context=ctx,
                            keywords=keywords
                        )
                        moments.append(fm)
                    except Exception:
                        continue

            except Exception as e:
                logger.logger.warning(f"[WINDOW] ошибка при обработке окна {w_start:.2f}-{w_end:.2f}: {e}")
                continue

        logger.logger.info(f"[WINDOW] собрано сырых кандидатов: {len(moments)} из {total_windows} окон")

        # Дедупликация/слияние
        deduped = self._dedupe_and_merge_moments(moments)
        logger.logger.info(f"[WINDOW] после дедупликации: {len(deduped)}")
        return deduped

    def _interval_iou(self, a0: float, a1: float, b0: float, b1: float) -> float:
        """IoU для одномерных интервалов [a0, a1] и [b0, b1]."""
        try:
            if a1 <= a0 or b1 <= b0:
                return 0.0
            inter = max(0.0, min(a1, b1) - max(a0, b0))
            uni = max(a1, b1) - min(a0, b0)
            if uni <= 0:
                return 0.0
            return inter / uni
        except Exception:
            return 0.0

    def _dedupe_and_merge_moments(self, candidates: List[FilmMoment]) -> List[FilmMoment]:
        """
        Простая NMS-подобная дедупликация по времени с выбором лучшего кандидата.
        Критерий "лучше": больший total_score (оценим через текущую систему скоринга),
        при равенстве — более подходящая длительность (ближе к центру допустимого интервала).
        """
        if not candidates:
            return []

        thr = float(getattr(self.film_config, "dedupe_iou_threshold", 0.5) or 0.5)
        # Предварительная оценка total_score для сортировки (с учетом ключевых слов)
        scored_list: List[RankedMoment] = []
        for c in candidates:
            try:
                sc = self._calculate_moment_scores(c)
                total = sum(sc.get(k, 0.0) * self.film_config.ranking_weights.get(k, 0.0) for k in sc.keys())
                # Бонус за количество ключевых слов
                keyword_bonus = len(c.keywords or []) * 0.1
                total += keyword_bonus
                scored_list.append(RankedMoment(moment=c, scores=sc, total_score=total, rank=0))
            except Exception:
                scored_list.append(RankedMoment(moment=c, scores={}, total_score=0.0, rank=0))

        # Сортируем по total_score по убыванию, чтобы первыми оставить лучших
        scored_list.sort(key=lambda x: x.total_score, reverse=True)

        kept: List[RankedMoment] = []
        for rm in scored_list:
            ok = True
            for kept_rm in kept:
                iou = self._interval_iou(rm.moment.start_time, rm.moment.end_time, kept_rm.moment.start_time, kept_rm.moment.end_time)
                if iou >= thr:
                    ok = False
                    break
            if ok:
                kept.append(rm)

        # Возвращаем списком FilmMoment (без рангов)
        return [rm.moment for rm in kept]

    def _compute_pace_silence_scores(self, moment: FilmMoment, transcription_data: Dict[str, Any]) -> tuple[float, float]:
        """
        Возвращает (pace_score_0_10, silence_penalty_0_10):
        - pace_score: плотность слов/сек (нормирована на [0..10] при cap=4.0 слов/сек)
        - silence_penalty: доля длинных пауз в моменте -> [0..10]
        """
        try:
            st = float(moment.start_time)
            en = float(moment.end_time)
            dur = max(1e-6, en - st)
        except Exception:
            return (0.0, 0.0)

        # Плотность слов/сек по legacy segments
        segments = transcription_data.get('segments', []) or []
        words = 0
        overlapped_dur = 0.0
        for seg in segments:
            try:
                if isinstance(seg, (list, tuple)) and len(seg) >= 3:
                    sst = float(seg[1]); sse = float(seg[2])
                    if sse > st and sst < en:
                        ov = max(0.0, min(sse, en) - max(sst, st))
                        if ov > 0:
                            txt = str(seg[0]) or ""
                            # Простейшая оценка количества слов: split по пробелам
                            wcnt = len([w for w in txt.strip().split() if w])
                            words += wcnt
                            overlapped_dur += ov
            except Exception:
                continue

        pace = 0.0
        if overlapped_dur > 0:
            pace_wps = float(words) / overlapped_dur
            # Нормируем: 0 -> 0, 4 слов/сек -> 10, cap
            pace = max(0.0, min(10.0, (pace_wps / 4.0) * 10.0))

        # Silence penalty: используем интеллектуальный детектор скучных сегментов
        try:
            boring = self._detect_boring_segments_in_moment(moment, transcription_data) or []
            boring_total = 0.0
            ai_trimmed_duration = 0.0

            for b in boring:
                try:
                    b0 = float(b.get('start', st)); b1 = float(b.get('end', st))
                    duration = max(0.0, b1 - b0)
                    boring_total += duration

                    # Отслеживаем длительность ИИ-обрезанных пауз
                    if b.get('reason', '').startswith('intelligent_'):
                        ai_trimmed_duration += duration
                except Exception:
                    continue

            frac = max(0.0, min(1.0, boring_total / dur))
            silence_penalty = min(10.0, frac * 10.0)

            # Логируем информацию об ИИ-анализе пауз для скоринга
            if boring:
                ai_fraction = ai_trimmed_duration / boring_total if boring_total > 0 else 0
                logger.logger.debug(f"Silence analysis for moment {st:.1f}s-{en:.1f}s: "
                                  f"total_boring={boring_total:.2f}s ({frac:.1%}), "
                                  f"ai_trimmed={ai_trimmed_duration:.2f}s ({ai_fraction:.1%})")
        except Exception as e:
            logger.logger.debug(f"Error in silence penalty calculation: {e}")
            silence_penalty = 0.0

        return (pace, silence_penalty)

    def _create_empty_result(self, video_path: str) -> FilmAnalysisResult:
        """Создание пустого результата при ошибке"""
        video_id = os.path.splitext(os.path.basename(video_path))[0]

        return FilmAnalysisResult(
            video_id=video_id,
            duration=0,
            keep_ranges=[],
            scores=[],
            preview_text="Не удалось проанализировать видео",
            risks=["Ошибка обработки видео"],
            metadata={'error': True, 'ranking_system': 'keyword_matching'},
            generated_shorts=[]
        )


def scan_movies_folder() -> List[str]:
    """
    Сканирует папку movies и возвращает список видео файлов.
    Поддерживаемые форматы: .mp4, .avi, .mkv, .mov, .wmv

    Returns:
        List[str]: Список путей к видео файлам
    """
    movies_dir = os.path.join(os.getcwd(), "movies")
    supported_extensions = {'.mp4', '.avi', '.mkv', '.mov', '.wmv'}

    if not os.path.exists(movies_dir):
        logger.logger.warning(f"Папка movies не найдена: {movies_dir}")
        return []

    video_files = []
    try:
        for file in os.listdir(movies_dir):
            if os.path.isfile(os.path.join(movies_dir, file)):
                _, ext = os.path.splitext(file.lower())
                if ext in supported_extensions:
                    video_files.append(os.path.join(movies_dir, file))
    except Exception as e:
        logger.logger.error(f"Ошибка при сканировании папки movies: {e}")
        return []

    return sorted(video_files)


def display_movie_selection(video_files: List[str]) -> None:
    """
    Отображает список видео файлов с номерами для выбора.

    Args:
        video_files: Список путей к видео файлам
    """
    if not video_files:
        print("\n📁 Папка movies пуста или не содержит поддерживаемых видео файлов.")
        print("Поддерживаемые форматы: .mp4, .avi, .mkv, .mov, .wmv")
        print("Поместите видео файлы в папку 'movies' в корне проекта.")
        return

    print(f"\n🎬 Найдено {len(video_files)} видео файлов в папке movies:")
    print("-" * 60)

    for i, file_path in enumerate(video_files, 1):
        file_name = os.path.basename(file_path)
        file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB

        # Получаем информацию о длительности, если возможно
        duration_str = ""
        try:
            from Components.Edit import get_video_dimensions
            # Для получения длительности можно использовать ffprobe
            duration = _get_video_duration_quick(file_path)
            if duration and duration > 0:
                duration_str = f" ({duration:.1f} мин)"
        except:
            pass

        print("2d")

    print("-" * 60)
    print("0. Вернуться в главное меню")
    print("URL. Ввести YouTube URL или путь к файлу вручную")


def select_movie_by_number(video_files: List[str]) -> Optional[str]:
    """
    Позволяет пользователю выбрать видео файл по номеру.

    Args:
        video_files: Список путей к видео файлам

    Returns:
        Optional[str]: Выбранный путь к файлу или None при отмене
    """
    if not video_files:
        return None

    while True:
        try:
            choice = input("\nВведите номер видео (1-{}) или 0 для отмены: ".format(len(video_files))).strip()

            if choice == "0":
                return None

            if choice.upper() == "URL":
                # Возвращаем специальный маркер для ручного ввода
                return "URL_INPUT"

            choice_num = int(choice)

            if 1 <= choice_num <= len(video_files):
                selected_file = video_files[choice_num - 1]

                # Проверяем, что файл все еще существует
                if not os.path.exists(selected_file):
                    print(f"❌ Файл не найден: {os.path.basename(selected_file)}")
                    print("Файл мог быть удален или перемещен.")
                    return None

                file_name = os.path.basename(selected_file)
                file_size = os.path.getsize(selected_file) / (1024 * 1024)  # MB
                print(f"\n✅ Выбрано: {file_name} ({file_size:.1f} MB)")
                return selected_file
            else:
                print(f"❌ Неверный номер. Введите число от 1 до {len(video_files)}")

        except ValueError:
            print("❌ Неверный ввод. Введите число или 'URL' для ручного ввода")
        except KeyboardInterrupt:
            print("\n\nОтмена выбора.")
            return None
        except Exception as e:
            print(f"❌ Ошибка при выборе файла: {e}")
            return None


def _get_video_duration_quick(video_path: str) -> Optional[float]:
    """
    Быстрое получение длительности видео через ffprobe.

    Args:
        video_path: Путь к видео файлу

    Returns:
        Optional[float]: Длительность в минутах или None при ошибке
    """
    try:
        import subprocess
        import json

        cmd = [
            "ffprobe",
            "-v", "quiet",
            "-print_format", "json",
            "-show_format",
            video_path
        ]

        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)

        if result.returncode == 0:
            data = json.loads(result.stdout)
            duration = float(data['format']['duration'])
            return duration / 60  # в минутах
        else:
            return None
    except Exception:
        return None


def analyze_film_main(url: Optional[str] = None, local_path: Optional[str] = None) -> FilmAnalysisResult:
    """
    Основная функция для анализа фильма.
    Точка входа для интеграции с main.py
    """
    try:
        config = get_config()
        analyzer = FilmAnalyzer(config)
        return analyzer.analyze_film(url, local_path)
    except Exception as e:
        logger.logger.error(f"Ошибка в analyze_film_main: {e}")
        # Возвращаем пустой результат
        return FilmAnalysisResult(
            video_id="error",
            duration=0,
            keep_ranges=[],
            scores=[],
            preview_text=f"Ошибка анализа: {str(e)}",
            risks=[str(e)],
            metadata={'error': True}
        )
</file>

<file path="Components/LanguageTasks.py">
from google import genai
from typing import TypedDict, List, Optional, Any, Tuple, Dict
import json
import os
import re # Import regex for parsing transcription
import time
from dotenv import load_dotenv
from google.genai import types
from Components.config import get_config

# Optional imports for Google API exceptions (rate limit handling)
try:
    from google.api_core.exceptions import ResourceExhausted as GoogleResourceExhausted  # type: ignore
except Exception:
    GoogleResourceExhausted = None  # type: ignore

try:
    from google.api_core import exceptions as google_exceptions  # type: ignore
except Exception:
    google_exceptions = None  # type: ignore

load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise ValueError(
        "Google API key not found. Make sure it is defined in the .env file."
    )

client = genai.Client(
        api_key=GOOGLE_API_KEY,
    )
# Load configuration (cached)
cfg = get_config()
# Consider using a more capable model if generating descriptions needs more nuance
# model = genai.GenerativeModel("gemini-1.5-flash") # Example alternative
model = cfg.llm.model_name

def build_transcription_prompt(segments: list[dict]) -> str:
    """
    Собирает строку транскрипции для LLM из списка сегментов.

    Вход:
    - segments: список словарей со следующими ключами (минимально необходимые):
        - "start": float
        - "end": float
        - "text": str
        - опционально: "speaker" | "name" | "id" — будет использовано вместо "Speaker", если непусто.

    Формат каждой строки:
    "[{start:.2f}] SpeakerName: {text} [{end:.2f}]"

    Возврат:
    - Одна большая строка с символом новой строки после каждой записи. Побочных эффектов нет.
    """
    lines: list[str] = []
    for seg in (segments or []):
        try:
            if isinstance(seg, dict):
                start = float(seg.get("start", 0.0))
                end = float(seg.get("end", 0.0))
                text = str(seg.get("text", "") or "")
                speaker_val = None
                for key in ("speaker", "name", "id"):
                    v = seg.get(key, None)
                    if v is not None and str(v).strip():
                        speaker_val = str(v).strip()
                        break
            else:
                start = float(getattr(seg, "start", 0.0))
                end = float(getattr(seg, "end", 0.0))
                text = str(getattr(seg, "text", "") or "")
                speaker_val = None
                for key in ("speaker", "name", "id"):
                    if hasattr(seg, key):
                        v = getattr(seg, key)
                        if v is not None and str(v).strip():
                            speaker_val = str(v).strip()
                            break

            speaker_label = speaker_val if speaker_val else "Speaker"
            line = f"[{start:.2f}] {speaker_label}: {text.strip()} [{end:.2f}]"
            lines.append(line)
        except Exception:
            # Любые странности сегмента — пропускаем строку, не прерывая пайплайн
            continue
    return "\n".join(lines) + ("\n" if lines else "")
# --- Rate limit handling utilities and wrapper ---

def parse_retry_delay_seconds(error: Exception | str) -> Optional[int]:
    """
    Пытается извлечь задержку повторной попытки (в секундах) из текста ошибки.
    Поддерживаемые форматы:
    - Retry-After: 28
    - retry-after: 28
    - "retryDelay": "28s"
    - retryDelay: 28s
    Возвращает целое количество секунд или None, если не удалось распарсить.
    """
    text = ""
    try:
        if isinstance(error, Exception):
            parts = [str(error), repr(error)]
            for attr in ("message", "details", "args"):
                val = getattr(error, attr, None)
                if val:
                    parts.append(str(val))
            text = " | ".join(parts)
        else:
            text = str(error)
    except Exception:
        text = str(error)

    patterns = [
        r'(?i)(?:retry[- ]?after|retryDelay)"?:?\s*"?(\d+)\s*s?',  # Retry-After: 28  or retryDelay: "28s"
        r'(?i)"retryDelay"\s*:\s*"?(\d+)\s*s"?'                    # "retryDelay": "28s"
    ]
    for pat in patterns:
        m = re.search(pat, text)
        if m:
            try:
                return int(m.group(1))
            except Exception:
                continue
    return None


def _is_resource_exhausted_error(err: Exception) -> bool:
    """Возвращает True, если ошибка соответствует лимиту API (ResourceExhausted)."""
    try:
        if 'GoogleResourceExhausted' in globals() and GoogleResourceExhausted is not None and isinstance(err, GoogleResourceExhausted):  # type: ignore
            return True
    except Exception:
        pass
    try:
        if 'google_exceptions' in globals() and google_exceptions is not None:
            ResExh = getattr(google_exceptions, "ResourceExhausted", None)
            if ResExh is not None and isinstance(err, ResExh):
                return True
    except Exception:
        pass
    text = f"{type(err).__name__}: {err}"
    return ("ResourceExhausted" in text) or ("RESOURCE_EXHAUSTED" in text) or ("rate limit" in text.lower())


def call_llm_with_retry(
    system_instruction: Optional[str],
    content: List | str,
    generation_config,
    model: Optional[str] = None,
    max_api_attempts: int = 3,
):
    """
    Выполняет вызов client.models.generate_content с централизованной обработкой лимитов API.

    Логирование:
    - При перехвате лимита и наличии retryDelay:
      "Лимит API обработан. Выполняю паузу на X секунд перед попыткой #Y."
    - Если retryDelay извлечь не удалось:
      "Не удалось извлечь retryDelay. Попытки прекращены."

    Стратегия:
    - Повторяет запрос не более max_api_attempts раз, делая паузу X секунд, если retryDelay присутствует.
    - При отсутствии retryDelay немедленно прекращает дальнейшие попытки и пробрасывает исключение.
    - Другие исключения пробрасываются без изменений.
    """
    model_to_use = model or globals().get("model")
    # Нормализуем contents
    if isinstance(content, list):
        contents = content
    else:
        contents = [types.Content(role="user", parts=[types.Part.from_text(text=str(content))])]

    last_err: Optional[Exception] = None

    for api_try in range(1, max_api_attempts + 1):
        try:
            # system_instruction ожидается внутри generation_config; параметр system_instruction оставлен для совместимости.
            return client.models.generate_content(
                model=model_to_use,
                contents=contents,
                config=generation_config,
            )
        except Exception as e:
            last_err = e
            if _is_resource_exhausted_error(e):
                delay = parse_retry_delay_seconds(e)
                if delay is None:
                    print("Не удалось извлечь retryDelay. Попытки прекращены.")
                    raise
                if api_try < max_api_attempts:
                    print(f"Лимит API обработан. Выполняю паузу на {delay} секунд перед попыткой #{api_try+1}.")
                    time.sleep(delay)
                    continue
                # Достигнут лимит попыток API — пробрасываем исключение без дополнительного лога.
                raise
            else:
                raise

    if last_err is not None:
        raise last_err


def call_llm_with_film_mode_retry(
    system_instruction: Optional[str],
    content: List | str,
    generation_config,
    model: Optional[str] = None,
    max_api_attempts: int = 5,
    base_delay: float = 2.0,
    max_delay: float = 60.0,
):
    """
    Улучшенная версия call_llm_with_retry для Film Mode с поддержкой 503 UNAVAILABLE ошибок.

    Особенности:
    - Увеличено количество попыток до 5
    - Экспоненциальная задержка с jitter для 503 ошибок
    - Специальная обработка ResourceExhausted и 503 ошибок
    - Более детальное логирование для отладки
    """
    model_to_use = model or globals().get("model")

    # Нормализуем contents
    if isinstance(content, list):
        contents = content
    else:
        contents = [types.Content(role="user", parts=[types.Part.from_text(text=str(content))])]

    last_err: Optional[Exception] = None

    for api_try in range(1, max_api_attempts + 1):
        try:
            print(f"[FILM_MODE_RETRY] Попытка {api_try}/{max_api_attempts} для модели {model_to_use}")

            # system_instruction ожидается внутри generation_config
            response = client.models.generate_content(
                model=model_to_use,
                contents=contents,
                config=generation_config,
            )

            print(f"[FILM_MODE_RETRY] Успешный ответ на попытке {api_try}")
            return response

        except Exception as e:
            last_err = e
            error_text = str(e).lower()
            error_type = type(e).__name__

            print(f"[FILM_MODE_RETRY] Ошибка на попытке {api_try}: {error_type} - {str(e)[:200]}...")

            # Обработка ResourceExhausted (429 Too Many Requests)
            if _is_resource_exhausted_error(e):
                delay = parse_retry_delay_seconds(e)
                if delay is None:
                    delay = min(base_delay * (2 ** (api_try - 1)), max_delay)
                    print(f"[FILM_MODE_RETRY] Используем экспоненциальную задержку: {delay} сек")

                if api_try < max_api_attempts:
                    print(f"[FILM_MODE_RETRY] ResourceExhausted: пауза {delay} сек перед попыткой {api_try+1}")
                    time.sleep(delay)
                    continue
                else:
                    print("[FILM_MODE_RETRY] Превышено максимальное количество попыток для ResourceExhausted")
                    raise

            # Обработка 503 UNAVAILABLE ошибок
            elif "503" in error_text or "unavailable" in error_text or "service unavailable" in error_text:
                delay = min(base_delay * (2 ** (api_try - 1)), max_delay)
                # Добавляем jitter для предотвращения одновременных повторных попыток
                import random
                jitter = random.uniform(0.1, 1.0)
                delay += jitter

                if api_try < max_api_attempts:
                    print(f"[FILM_MODE_RETRY] 503 UNAVAILABLE: пауза {delay:.1f} сек перед попыткой {api_try+1}")
                    time.sleep(delay)
                    continue
                else:
                    print("[FILM_MODE_RETRY] Превышено максимальное количество попыток для 503 UNAVAILABLE")
                    raise

            # Обработка других ошибок (500, 502, etc.)
            elif any(code in error_text for code in ["500", "502", "504", "internal", "bad gateway", "gateway timeout"]):
                delay = min(base_delay * (2 ** (api_try - 1)), max_delay)

                if api_try < max_api_attempts:
                    print(f"[FILM_MODE_RETRY] Серверная ошибка: пауза {delay:.1f} сек перед попыткой {api_try+1}")
                    time.sleep(delay)
                    continue
                else:
                    print("[FILM_MODE_RETRY] Превышено максимальное количество попыток для серверной ошибки")
                    raise

            # Для остальных ошибок (400, 401, etc.) - не повторяем
            else:
                print(f"[FILM_MODE_RETRY] Необрабатываемая ошибка {error_type}, прекращаем попытки")
                raise

    # Если дошли сюда - значит все попытки исчерпаны
    if last_err is not None:
        print(f"[FILM_MODE_RETRY] Все {max_api_attempts} попыток исчерпаны, последняя ошибка: {type(last_err).__name__}")
        raise last_err

# Вспомогательная функция: безопасная сборка конфигурации генерации с поддержкой Thinking (если доступно в SDK)
def make_generation_config(system_instruction_text: str, temperature: float = 0.2) -> types.GenerateContentConfig:
    """
    Собирает GenerateContentConfig согласно документации google-genai:
    - system_instruction: строка (может быть Content, но здесь — строка).
    - response_mime_type='application/json' для строгого JSON.
    - thinking_config: types.ThinkingConfig(thinking_budget=-1, include_thoughts=False) для Gemini 2.5.
      Если класс отсутствует или не поддерживается текущей версией SDK — используем конфигурацию без thinking_config.
    """
    base_kwargs = dict(
        temperature=temperature,
        response_mime_type="application/json",
        system_instruction=system_instruction_text,
    )

    # Предпочтительный путь (Python): вложенный thinking_config с корректными snake_case полями.
    ThinkingConfig = getattr(types, "ThinkingConfig", None)
    if ThinkingConfig is not None:
        try:
            return types.GenerateContentConfig(
                **base_kwargs,
                thinking_config=ThinkingConfig(
                    thinking_budget=-1,      # динамическое мышление
                    include_thoughts=False,  # не включать конспекты мыслей в ответ
                ),
            )
        except Exception:
            # Если валидация SDK не пропускает thinking_config, откатываемся на конфиг без мышления.
            print("Предупреждение: ThinkingConfig недоступен/не совместим; продолжаю без thinking.")
            return types.GenerateContentConfig(**base_kwargs)

    # Фолбэк: нет ThinkingConfig в текущем SDK — работаем без мышления.
    return types.GenerateContentConfig(**base_kwargs)


class Message(TypedDict):
    role: str
    content: str


class HighlightSegment(TypedDict):
    start: float
    end: float

# New type for the enriched highlight data
class EnrichedHighlightData(TypedDict):
    start: float
    end: float
    caption_with_hashtags: str
    segment_text: str # Store the text used for generation
    title: Optional[str]
    description: Optional[str]
    hashtags: Optional[List[str]]


def validate_highlight(highlight: HighlightSegment) -> bool:
    """Validate a single highlight segment's time duration and format."""
    try:
        if not all(key in highlight for key in ["start", "end"]):
            print(f"Validation Fail: Missing 'start' or 'end' key in {highlight}")
            return False

        start = float(highlight["start"])
        end = float(highlight["end"])
        duration = end - start

        # Check for valid duration (configured range)
        min_duration = float(cfg.llm.highlight_min_sec)
        max_duration = float(cfg.llm.highlight_max_sec)

        if not (min_duration <= duration <= max_duration):
            print(f"Validation Fail: Duration {duration:.2f}s out of range [~{min_duration:.0f}s, ~{max_duration:.0f}s] for {highlight}")
            return False

        # Check for valid ordering
        if start >= end:
            print(f"Validation Fail: Start time {start} >= end time {end} for {highlight}")
            return False

        return True
    except (ValueError, TypeError) as e:
        print(f"Validation Fail: Invalid type or value in {highlight} - {e}")
        return False


def validate_highlights(highlights: List[HighlightSegment]) -> bool:
    """Validate all highlights and check for overlaps."""
    if not highlights:
        print("Validation: No highlights provided.")
        return False

    # Validate each individual highlight (already checks duration)
    if not all(validate_highlight(h) for h in highlights):
        # Specific errors printed within validate_highlight
        print("Validation: One or more highlights failed individual checks.")
        return False

    # Check for overlapping segments
    sorted_highlights = sorted(highlights, key=lambda x: float(x["start"]))
    for i in range(len(sorted_highlights) - 1):
        if float(sorted_highlights[i]["end"]) > float(
            sorted_highlights[i + 1]["start"]
        ):
            print(f"Validation Fail: Overlap detected between {sorted_highlights[i]} and {sorted_highlights[i+1]}")
            return False

    return True


def extract_highlights(
    transcription: str, max_attempts: int = 3
) -> List[HighlightSegment]:
    """Extracts highlight time segments from transcription, validates, checks overlaps, with retry logic."""
    # System instruction based on Google AI Studio code
    system_instruction_text = f"""
Ты — креатор коротких видео для соцсетей. По предоставленной транскрипции выдели как можно больше непересекающихся отрезков, которые подойдут для увлекательных коротких роликов. Отдавай приоритет разнообразию валидных сегментов.
Верни ТОЛЬКО JSON-массив объектов. Каждый объект обязан содержать ключи "start" и "end" (на английском) с точными временными метками начала и конца сегмента из транскрипта. Никакого текста, объяснений или форматирования вне JSON.

Критерии выбора:
• Ключевые мысли, объяснения, вопросы, выводы и вообще наиболее «цепляющие» места.
• Старайся включать завершённые мысли/предложения.
• Предпочтительны моменты с понятным контекстом, но при необходимости соблюдай ограничение по времени.
• Естественные паузы и переходы — плюс, но не обязательны.

Требования к длительности:
• Длительность каждого сегмента (end - start) СТРОГО ОТ {cfg.llm.highlight_min_sec} ДО {cfg.llm.highlight_max_sec} секунд (включительно).
• Сегменты не должны перекрываться.
• Найди и верни от 10 до {cfg.llm.max_highlights} валидных сегментов — не больше.
• От 30 до 60 секунд.ф

Точность таймкодов:
• Используй ИМЕННО те таймкоды, что присутствуют/логически вытекают из транскрипта.
• Нельзя придумывать или править таймкоды.

Пример JSON-вывода:
[
  {{"start": "8.96", "end": "42.20"}},
  {{"start": "115.08", "end": "156.12"}},
  {{"start": "1381.68", "end": "1427.40"}}
]

• Главная цель — найти несколько сегментов со сроком строго {cfg.llm.highlight_min_sec}–{cfg.llm.highlight_max_sec} секунд.
• Убедись, что таймкоды соответствуют фактическим маркерам/границам смысла.
• Сегменты не должны пересекаться.
    """

    # Define generation config based on AI Studio code
    generation_config = make_generation_config(system_instruction_text, temperature=cfg.llm.temperature_highlights)

    effective_attempts = cfg.llm.max_attempts_highlights if max_attempts == 3 else max_attempts
    for attempt in range(effective_attempts):
        print(f"\nПопытка {attempt + 1}: генерация и валидация тайм-сегментов для хайлайтов...")
        try:
            # Structure the prompt and system instruction for generate_content
            user_prompt_text = f"Transcription:\n{transcription}"
            contents = [types.Content(role="user", parts=[types.Part.from_text(text=user_prompt_text)])]
            # Use the global model but with the new config
            response = call_llm_with_retry(
                system_instruction=None,
                content=contents,
                generation_config=generation_config,
                model=model,
            )

            # Basic safety check for response content
            if not response or not response.text:
                 print(f"Неудача на попытке {attempt + 1}: пустой ответ от LLM.")
                 continue

            # Extract JSON from response
            response_text = response.text
            # Handle potential markdown code blocks
            match = re.search(r"```json\s*([\s\S]*?)\s*```", response_text)
            if match:
                json_string = match.group(1).strip()
            else:
                 # Assume the whole text is JSON if no markdown block found
                 json_string = response_text.strip()

            raw_highlights = json.loads(json_string)

            if not isinstance(raw_highlights, list):
                 print(f"Неудача на попытке {attempt + 1}: ответ LLM не является JSON‑массивом.")
                 print(f"Сырой ответ LLM: {response.text}")
                 continue

            # Filter the highlights: Keep only those passing individual validation
            valid_highlights = [h for h in raw_highlights if validate_highlight(h)]

            if not valid_highlights:
                print("No highlights passed individual duration/format validation in this attempt.")
                continue # Try next attempt

            # Check for overlaps ONLY among the valid duration highlights
            # Sort again just to be sure, although validate_highlights also sorts internally for its check
            sorted_highlights = sorted(valid_highlights, key=lambda x: float(x["start"]))
            overlaps_found = False
            for i in range(len(sorted_highlights) - 1):
                if float(sorted_highlights[i]["end"]) > float(sorted_highlights[i + 1]["start"]):
                    print(f"Обнаружено пересечение между {sorted_highlights[i]} и {sorted_highlights[i+1]}")
                    overlaps_found = True
                    break # No need to check further overlaps in this attempt

            if overlaps_found:
                print("Проверка на пересечения провалена для этой попытки.")
                continue # Try next attempt

            # If we reach here, we have a non-empty list of valid, non-overlapping highlights
            print(f"Успех на попытке {attempt + 1}. Найдено валидных сегментов: {len(sorted_highlights)}.")
            # Apply max_highlights cap from config
            try:
                max_h = int(cfg.llm.max_highlights)
                if max_h > 0:
                    return sorted_highlights[:max_h]
            except Exception:
                pass
            return sorted_highlights # Return the validated and sorted list

        except json.JSONDecodeError:
             print(f"Неудача на попытке {attempt + 1}: некорректный JSON от LLM.")
             if 'response_text' in locals(): print(f"Сырой ответ LLM: {response_text}")
             continue
        except Exception as e:
            if _is_resource_exhausted_error(e):
                # Обертка уже залогировала причину; прекращаем дальнейшие попытки этой функции
                break
            print(f"Неудача на попытке {attempt + 1}: непредвиденная ошибка: {str(e)}")
            if 'response_text' in locals():
                print(f"Сырой ответ LLM при ошибке: {response_text}")
            continue

    print("Достигнуто максимальное число попыток извлечения сегментов. Возвращаю пустой список.")
    return []


# --- New Functions ---

def extract_text_for_segment(transcription: str, start_time: float, end_time: float) -> str:
    """Extracts speaker text from transcription within a given time range."""
    segment_text = []
    # Regex to capture timestamp and text, robust to formats like:
    # [0.00] Speaker: Text [8.96]
    # [8.96] Text
    # [12.32] Speaker: Text
    # It captures the start time and the main text content.
    line_pattern = re.compile(r"^\s*\[\s*(\d+\.\d+)\s*\]\s*(.*?)(?:\s*\[\d+\.\d+\s*\])?$")

    lines = transcription.strip().splitlines() # Use splitlines for robustness
    for i, line in enumerate(lines):
        match = line_pattern.match(line)
        if match:
            try:
                timestamp = float(match.group(1))
                text_content = match.group(2).strip()

                # Remove speaker prefix like "Speaker X:" if present
                text_content = re.sub(r"^[Ss]peaker\s*\d*:\s*", "", text_content).strip()

                # Include lines starting within the time range
                if timestamp < end_time and timestamp >= start_time:
                    if text_content: # Avoid adding empty lines
                         segment_text.append(text_content)
            except (ValueError, IndexError):
                # Ignore lines that don't match the expected format
                continue
        # else: Line doesn't match pattern, ignore

    return "\n".join(segment_text)


def generate_description_and_hashtags(segment_text: str, max_attempts: int = 3) -> Optional[str]:
    """Generates a description with appended hashtags for a text segment using LLM."""
    if not segment_text or not segment_text.strip():
        print("Skipping description generation: Empty segment text provided.")
        return None

    system_prompt = """
    Тебе дан текстовый фрагмент короткого видеоролика (обычно 30–60 секунд).
    Твоя задача: вернуть ОДИН JSON-объект со строкой:
    1) Короткое, ёмкое и вовлекающее описание (1–2 предложения) содержимого клипа.
    2) Затем через пробел — 3–5 релевантных хэштегов, слитно, в нижнем регистре, начинаются с #, без пробелов.
    
    Формат ответа (строго, на английском ключе):
    {
        "caption_with_hashtags": "Твоё описание. #пример1 #пример2 #пример3"
    }
    
    Правила:
    - В ответе не должно быть ничего, кроме указанного JSON-объекта.
    - Хэштеги отражают тему клипа, без пробелов, латиницей/кириллицей допустимо.
    
    Пример ввода:
    "Одна из самых интересных областей — обработка видео. Посмотрим, как ИИ может автоматически находить хайлайты."
    
    Пример вывода:
    {
        "caption_with_hashtags": "Как ИИ находит лучшие моменты в видео — кратко и по делу! #ии #видео #хайлайты #машинноевобучение"
    }
    
    Верни ТОЛЬКО JSON-объект.
    """

    # Define generation config based on AI Studio code
    generation_config = make_generation_config(system_prompt, temperature=cfg.llm.temperature_metadata)

    effective_attempts = cfg.llm.max_attempts_metadata if max_attempts == 3 else max_attempts
    for attempt in range(effective_attempts):
        try:
            # Structure the prompt and system instruction for generate_content
            user_prompt_text = f"Segment Text:\n{segment_text}"
            contents = [types.Content(role="user", parts=[types.Part.from_text(text=user_prompt_text)])]
            
            # Use the global model with the new config
            response = call_llm_with_retry(
                system_instruction=None,
                content=contents,
                generation_config=generation_config,
                model=model,
            )

            if not response or not response.text:
                print(f"Неудача на попытке {attempt + 1}: пустой ответ от LLM для описания.")
                continue

            # Extract JSON from response
            response_text = response.text
            match = re.search(r"```json\s*([\s\S]*?)\s*```", response_text)
            if match:
                json_string = match.group(1).strip()
            else:
                json_string = response_text.strip()

            data = json.loads(json_string)

            # Validate the structure and types
            if not isinstance(data, dict) or \
               "caption_with_hashtags" not in data or \
               not isinstance(data["caption_with_hashtags"], str):
                print(f"Неудача на попытке {attempt + 1}: некорректная структура или типы в JSON-ответе.")
                print(f"Сырой ответ LLM: {response_text}")
                continue

            return data["caption_with_hashtags"].strip()

        except json.JSONDecodeError:
            print(f"Неудача на попытке {attempt + 1}: некорректный JSON от LLM для описания.")
            if 'response_text' in locals(): print(f"Сырой ответ LLM: {response_text}")
            continue
        except Exception as e:
            if _is_resource_exhausted_error(e):
                break
            print(f"Неудача на попытке {attempt + 1}: непредвиденная ошибка при генерации описания: {str(e)}")
            if 'response_text' in locals():
                print(f"Сырой ответ LLM при ошибке: {response_text}")
            continue

    print("Достигнуто максимальное число попыток генерации описания/хэштегов. Возвращаю None.")
    return None

# --- Batch metadata generation ---
BATCH_METADATA_SYSTEM_PROMPT = """Ты — эксперт по SMM и продвижению на YouTube, специализирующийся на вирусных Shorts. Тебе на вход подается JSON-массив текстовых фрагментов из видео, каждый с уникальным `id`. Твоя задача — для каждого фрагмента создать оптимальный набор метаданных для максимального вовлечения и охвата.

Правила:
1. Твой ответ должен быть ИСКЛЮЧИТЕЛЬНО одним валидным JSON-массивом. Никакого текста до или после.
2. Для каждого входного объекта с `id` ты должен сгенерировать объект в выходном массиве с тем же `id` и тремя полями: `title`, `description` и `hashtags`.
3. title (заголовок): 40–70 символов, интригующий, задает вопрос или создает предвкушение. Обязательно использовать ключевые слова из текста.
4. description (описание): до 150 символов, кратко раскрывает суть, допускается призыв к действию.
5. hashtags (хэштеги): массив из 3–5 строк; первым ВСЕГДА `#shorts`; остальные — максимально релевантны теме фрагмента.

Пример Входа:
[{"id":"seg_1","text":"Сегодня обсудим, как автоматически находить лучшие моменты в видео..."}]

Пример Выхода:
[{"id":"seg_1","title":"Нейросеть находит лучшие моменты в видео?","description":"Смотрите, как ИИ анализирует ролики для создания шортсов.","hashtags":["#shorts","#ИИ","#нейросети","#видеомонтаж"]}]"""

def _build_retry_prompt(
    validation_tracker: Dict[str, Dict[str, Any]],
    items_to_retry: Optional[List[dict]] = None,
    *,
    max_snippet_len: int = 500
) -> str:
    """
    Строит user prompt для повторной отправки проблемных элементов.

    Параметры:
    - validation_tracker: словарь статусов вида {id: {"status","data","reason","original_item"}}
    - items_to_retry: список исходных элементов {"id","text"} для повтора; если None — берутся со статусами pending/failed
    - max_snippet_len: ограничение длины включаемого текста

    Возвращает:
    - Строку, которую следует передать как пользовательский prompt.
    """
    try:
        if items_to_retry is None:
            items_to_retry = []
            for _id, st in validation_tracker.items():
                if st and st.get("status") in ("pending", "failed"):
                    orig = st.get("original_item") or {}
                    if "id" not in orig:
                        orig = {**orig, "id": _id}
                    items_to_retry.append(orig)
    except Exception:
        items_to_retry = items_to_retry or []

    lines: List[str] = []
    lines.append("Нужно исправить ошибки для указанных id. Верни строго JSON-массив объектов с корректированными данными для этих id.")
    lines.append("")
    lines.append("Требования к каждому объекту ответа:")
    lines.append("• Сохраняй поле id без изменений.")
    lines.append("• Поля: title, description, hashtags.")
    lines.append("• title: 40–70 символов (после тримминга).")
    lines.append("• description: максимум 150 символов.")
    lines.append("• hashtags: массив из 3–5 строк; первый элемент строго '#shorts'; все элементы начинаются с '#'.")
    lines.append("")
    lines.append("Проблемные элементы (id, причина и исходный текст):")
    lines.append("")

    for it in items_to_retry:
        _id = str(it.get("id", ""))
        st = validation_tracker.get(_id, {}) or {}
        reason = st.get("reason") or "Причина не указана — см. требования валидации."
        text = str(it.get("text", "") or "")
        snippet = text.strip()
        if len(snippet) > max_snippet_len:
            snippet = snippet[:max_snippet_len].rstrip() + "..."
        lines.append(f"- id: {_id}")
        lines.append(f"  Причина ошибки: {reason}")
        lines.append("  Исходный текст:")
        lines.append(f'  """{snippet}"""')
        lines.append("")

    lines.append("Верни ТОЛЬКО JSON-массив следующего вида без пояснений:")
    lines.append('[{"id":"<id>","title":"...","description":"...","hashtags":["#shorts","..."]}]')
    return "\n".join(lines)

def generate_metadata_batch(items: list[dict], max_attempts: int = 3) -> list[dict]:
    """
    Пакетная генерация метаданных (title, description, hashtags) для сегментов с таргетированными повторами.

    Параметры:
    - items: список словарей вида {"id": str, "text": str}
    - max_attempts: максимальное число итераций (первая — весь батч, далее — только проблемные)

    Возврат:
    - список объектов {"id": str, "title": str, "description": str, "hashtags": list[str]} в порядке исходных items.

    Валидация (правила НЕ изменены):
    - title: 40–70 символов (после trim)
    - description: длина ≤ 150 символов
    - hashtags: массив длиной 3–5; первый элемент — "#shorts"; все элементы — строки, начинающиеся с "#".
    """
    if not items:
        print("Пакетная генерация метаданных: входных сегментов = 0")
        return []

    print(f"Пакетная генерация метаданных: входных сегментов = {len(items)}")

    # Первая итерация — как и раньше: системный промпт из константы
    generation_config_first = make_generation_config(
        BATCH_METADATA_SYSTEM_PROMPT,
        temperature=cfg.llm.temperature_metadata,
    )

    def _clean_space(s: str) -> str:
        return re.sub(r"\s+", " ", s or "").strip()

    def _extract_json_array(text: str) -> str:
        if not isinstance(text, str):
            return ""
        t = text.strip()
        m = re.search(r"```json\s*([\s\S]*?)\s*```", t)
        if m:
            return m.group(1).strip()
        m = re.search(r"```\s*([\s\S]*?)\s*```", t)
        if m:
            return m.group(1).strip()
        m = re.search(r'"""([\s\S]*?)"""', t)
        if m:
            return m.group(1).strip()
        start = t.find("[")
        end = t.rfind("]")
        if start != -1 and end != -1 and start < end:
            return t[start:end+1].strip()
        return t

    def _validate_item(obj: dict, expected_id: str) -> Tuple[bool, Optional[str], Optional[dict]]:
        """
        Проверяет объект метаданных согласно неизменённым правилам.

        Возвращает:
        - success: bool
        - reason: текст причины ошибки (рус.), если неуспех
        - valid_data: нормализованный объект при успехе
        """
        if not isinstance(obj, dict):
            return False, "Элемент ответа должен быть JSON-объектом (dict).", None
        obj_id = str(obj.get("id", ""))
        if obj_id != expected_id:
            return False, f"Неверный id: ожидалось '{expected_id}', получено '{obj_id}'.", None

        title = _clean_space(str(obj.get("title", "")))
        description = _clean_space(str(obj.get("description", "")))
        hashtags = obj.get("hashtags", [])

        if not (40 <= len(title) <= 70):
            return False, f"Некорректная длина title: {len(title)} символов; требуется 40–70.", None
        if len(description) > 150:
            return False, f"Слишком длинный description: {len(description)} символов; максимум 150.", None
        if not isinstance(hashtags, list):
            return False, "hashtags должен быть списком из 3–5 строк.", None
        if not (3 <= len(hashtags) <= 5):
            return False, f"Некорректное количество хэштегов: {len(hashtags)}; требуется 3–5.", None
        if any(not isinstance(h, str) or not h.startswith("#") for h in hashtags):
            return False, "Все хэштеги должны быть строками и начинаться с '#'.", None
        if len(hashtags) == 0 or hashtags[0] != "#shorts":
            return False, "Первый хэштег должен быть '#shorts'.", None

        return True, None, {
            "id": expected_id,
            "title": title,
            "description": description,
            "hashtags": hashtags,
        }

    # Подготовка входных элементов и трекера валидации
    items_prepared: List[dict] = []
    ordered_ids: List[str] = []
    validation_tracker: Dict[str, Dict[str, Any]] = {}

    for idx, it in enumerate(items):
        safe = dict(it or {})
        id_val = safe.get("id")
        if id_val is None or str(id_val).strip() == "":
            id_val = f"item_{idx+1}"
            safe["id"] = id_val
        id_str = str(id_val)
        if "text" in safe:
            safe["text"] = str(safe["text"])
        items_prepared.append(safe)
        ordered_ids.append(id_str)
        validation_tracker[id_str] = {
            "status": "pending",       # "pending" | "success" | "failed"
            "data": None,              # валидные данные при успехе
            "reason": None,            # последняя причина отказа
            "original_item": safe,     # исходный элемент {"id","text",...}
        }

    N = len(items_prepared)
    effective_attempts = cfg.llm.max_attempts_metadata if max_attempts == 3 else max_attempts

    attempt = 0
    while attempt < effective_attempts:
        attempt += 1

        # Определяем поднабор для текущей попытки
        if attempt == 1:
            to_send = items_prepared
        else:
            to_send = [
                validation_tracker[_id]["original_item"]
                for _id in ordered_ids
                if validation_tracker[_id]["status"] in ("pending", "failed")
            ]

        if not to_send:
            # Все уже успешно провалидированы
            break

        print(f"Попытка {attempt} из {effective_attempts} для batch-метаданных ({len(to_send)} элементов)")
        try:
            if attempt == 1:
                # Первая отправка — как раньше: весь вход и основной модель
                user_payload = json.dumps(to_send, ensure_ascii=False)
                contents = [types.Content(role="user", parts=[types.Part.from_text(text=user_payload)])]
                gen_cfg = generation_config_first
                model_to_use = model
            else:
                # Повторы — только проблемные элементы, со вспомогательным prompt и лёгкой моделью
                retry_prompt = _build_retry_prompt(validation_tracker, to_send)
                contents = [types.Content(role="user", parts=[types.Part.from_text(text=retry_prompt)])]
                # Системный промпт остаётся тем же, чтобы зафиксировать схему ответа
                gen_cfg = make_generation_config(BATCH_METADATA_SYSTEM_PROMPT, temperature=cfg.llm.temperature_metadata)
                model_to_use = "gemini-2.5-flash-lite"

            response = call_llm_with_retry(
                system_instruction=None,  # system_instruction уже в gen_cfg
                content=contents,
                generation_config=gen_cfg,
                model=model_to_use,
            )

            if not response or not getattr(response, "text", None):
                print(f"Неудача на попытке {attempt}: пустой ответ от LLM.")
                continue

            raw_text = response.text
            json_str = _extract_json_array(raw_text)
            data = json.loads(json_str)

            if not isinstance(data, list):
                print(f"Неудача на попытке {attempt}: ответ LLM не является JSON‑массивом.")
                continue

            # Соберём кандидатов по id
            out_by_id: Dict[str, Any] = {}
            for obj in data:
                if isinstance(obj, dict) and "id" in obj:
                    out_by_id[str(obj["id"])] = obj

            validated_this_attempt = 0
            expected_ids = [str(it.get("id")) for it in to_send]

            for expected_id in expected_ids:
                candidate = out_by_id.get(expected_id)
                if candidate is None:
                    validation_tracker[expected_id]["status"] = "failed"
                    validation_tracker[expected_id]["data"] = None
                    validation_tracker[expected_id]["reason"] = f"Модель не вернула объект для id '{expected_id}'."
                    continue

                ok, reason, normalized = _validate_item(candidate, expected_id)
                if ok and normalized:
                    validation_tracker[expected_id]["status"] = "success"
                    validation_tracker[expected_id]["data"] = normalized
                    validation_tracker[expected_id]["reason"] = None
                    validated_this_attempt += 1
                else:
                    validation_tracker[expected_id]["status"] = "failed"
                    validation_tracker[expected_id]["data"] = None
                    validation_tracker[expected_id]["reason"] = reason or "Нарушение правил валидации."

            total_success = sum(1 for st in validation_tracker.values() if st["status"] == "success")
            print(f"Batch-метаданные: успешно сгенерировано за попытку {validated_this_attempt}, всего валидных {total_success} из {N}")

            if total_success == N:
                break

        except json.JSONDecodeError:
            print(f"Неудача на попытке {attempt}: некорректный JSON от LLM для batch-метаданных.")
            if 'raw_text' in locals():
                print(f"Сырой ответ LLM: {raw_text}")
            continue
        except Exception as e:
            if _is_resource_exhausted_error(e):
                # Внутренняя обёртка уже залогировала и управляла паузами; прекращаем дальнейшие итерации
                break
            print(f"Неудача на попытке {attempt}: непредвиденная ошибка при batch-метаданных: {e}")
            if 'raw_text' in locals():
                print(f"Сырой ответ LLM при ошибке: {raw_text}")
            continue

    # Итоговая сборка по исходному порядку; для неуспехов — плейсхолдеры
    if any(st["status"] != "success" for st in validation_tracker.values()):
        print("Batch-метаданные: не все элементы прошли валидацию, для оставшихся будут использованы плейсхолдеры")

    results: List[dict] = []
    for _id in ordered_ids:
        st = validation_tracker[_id]
        if st["status"] == "success" and st["data"]:
            results.append(st["data"])
        else:
            results.append({"id": _id, "title": "", "description": "", "hashtags": ["#shorts"]})

    return results

# --- Updated Main Function ---

def GetHighlights(transcription: str) -> List[EnrichedHighlightData]:
    """
    Main function to get multiple highlight segments from transcription,
    each enriched with LLM-generated metadata (title, description, hashtags).
    Backward-compatible: caption_with_hashtags is preserved.
    """
    enriched_highlights = []
    try:
        # Clean and validate the input transcription
        if not transcription or not transcription.strip():
            print("Ошибка: передана пустая транскрипция.")
            return []

        # 1. Extract highlight time segments
        highlight_segments = extract_highlights(transcription.strip())

        if not highlight_segments:
            print("Не удалось извлечь валидные тайм‑сегменты для хайлайтов.")
            return []

        # 2. Extract text and prepare batch items
        items = []
        mapping = []

        for idx, segment in enumerate(highlight_segments, start=1):
            # Convert string timestamps to floats first
            try:
                start_time = float(segment["start"])
                end_time = float(segment["end"])
            except ValueError:
                print(f"Предупреждение: не удалось преобразовать таймкоды в float для сегмента {segment}. Пропускаю.")
                continue

            # Extract text for this segment
            segment_text = extract_text_for_segment(transcription, start_time, end_time)

            if not segment_text.strip():
                print("Предупреждение: для этого сегмента не извлечён текст. Пропускаю генерацию метаданных.")
                continue

            seg_id = f"seg_{len(items)+1}"
            items.append({"id": seg_id, "text": segment_text})
            mapping.append((seg_id, start_time, end_time, segment_text))

        if not items:
            print("Не удалось подготовить ни одного текстового сегмента для пакетной генерации.")
            return []

        print(f"\nПерехожу к пакетной генерации метаданных для {len(items)} сегментов...")
        batch_meta = generate_metadata_batch(items)
        meta_by_id = {str(m.get("id")): m for m in (batch_meta or []) if isinstance(m, dict)}

        # 3. Assemble enriched highlights
        for seg_id, start_time, end_time, segment_text in mapping:
            meta = meta_by_id.get(seg_id, {})
            title = str(meta.get("title", "") or "").strip()
            description = str(meta.get("description", "") or "").strip()
            hashtags = meta.get("hashtags", None)
            if not isinstance(hashtags, list) or not all(isinstance(h, str) for h in hashtags):
                hashtags = ["#shorts"]

            # Build backward-compatible caption_with_hashtags
            base_caption = ""
            if title and description:
                base_caption = f"{title} — {description}"
            elif title:
                base_caption = title
            elif description:
                base_caption = description
            caption_with_hashtags = base_caption
            if hashtags:
                caption_with_hashtags = f"{base_caption}\n{' '.join(hashtags)}" if base_caption else " ".join(hashtags)

            enriched_data: EnrichedHighlightData = {
                "start": start_time,
                "end": end_time,
                "segment_text": segment_text,
                "caption_with_hashtags": caption_with_hashtags,
                "title": title or None,
                "description": description or None,
                "hashtags": hashtags or None,
            }
            enriched_highlights.append(enriched_data)

        if not enriched_highlights:
            print("Не удалось обогатить ни один хайлайт метаданными.")
            return []

        print(f"\nУспешно обогащено хайлайтов: {len(enriched_highlights)}.")
        return enriched_highlights

    except Exception as e:
        print(f"Ошибка в GetHighlights: {str(e)}")
        import traceback
        traceback.print_exc()
        return []


if __name__ == "__main__":
    example_transcription = """
    [0.0] Speaker 1: Welcome to our discussion about artificial intelligence.
    [15.5] Speaker 1: Today we'll explore the fascinating world of machine learning.
    [30.2] Speaker 2: One of the most exciting applications is in video processing.
    [45.8] Speaker 1: Let's look at how AI can automatically generate video highlights.
    [60.0] Speaker 2: This technology is revolutionizing content creation, making it faster and easier.
    [75.5] Speaker 1: We're seeing it used widely in social media, entertainment, and even education platforms to deliver personalized content.
    [90.2] Speaker 2: The ability for AI to not just cut clips but understand context and find truly engaging moments is key.
    [105.8] Speaker 1: It's changing how we create and consume digital content daily. Think about personalized news feeds.
    [120.0] Speaker 2: Let's dive into some specific examples of tools available now.
    [135.0] Speaker 1: Good idea. Tool number one uses advanced natural language processing...
    """

    final_highlights = GetHighlights(example_transcription)
    if final_highlights:
        print("\n--- Итоговые обогащённые хайлайты ---")
        for i, highlight in enumerate(final_highlights, 1):
            print(f"Хайлайт {i}:")
            print(f"  Время: {highlight['start']:.2f}s - {highlight['end']:.2f}s")
            print(f"  Текст: {highlight['segment_text'][:100]}...") # Фрагмент текста
            print(f"  Подпись: {highlight['caption_with_hashtags']}")
            print("-" * 10)
    else:
        print("\nНе найдено валидных обогащённых хайлайтов.")

# --- Utility: tone/keywords heuristic ---
def compute_tone_and_keywords(text: str) -> dict:
    """
    Простая эвристика для определения «тона» фрагмента и набора ключевых слов.
    Возвращает:
        {
          "tone": "urgency" | "drama" | "positive" | "neutral",
          "keywords": ["слово1", "слово2", ...]  # нижний регистр, без пунктуации
        }
    Правила:
      - Подсчёт вхождений индикаторов по категориям + бонус за '!' для интенсивности.
      - При равенстве/отсутствии индикаторов — tone="neutral".
      - Ключевые слова: 1–4 "жёлуди" (нижний регистр), из найденных индикаторных слов,
        приоритизация по частоте и длине, со стоп-словами.
    """
    try:
        if not text or not str(text).strip():
            return {"tone": "neutral", "keywords": []}

        raw = str(text)
        s = raw.lower()

        # Индикаторы (стеммы/подстроки допустимы)
        indicators = {
            "urgency": {"срочно", "быстро", "скорее", "внимание", "опасн", "атак", "пожар", "беги"},
            "drama": {"плач", "потеря", "больно", "умер", "траг", "слёзы", "страх", "кровь"},
            "positive": {"круто", "супер", "ура", "рад", "люблю", "счаст", "класс", "смешно"},
        }
        emoji_to_tone = {"🔥": "urgency", "❤️": "positive", "😂": "positive", "💀": "drama"}

        # Подсчёт вхождений индикаторов (подстрочные совпадения, чтобы ловить стеммы)
        scores = {"urgency": 0, "drama": 0, "positive": 0}
        for tone, tokens in indicators.items():
            sc = 0
            for tok in tokens:
                try:
                    sc += s.count(tok)  # подстрочные совпадения
                except Exception:
                    continue
            scores[tone] += sc

        # Эмодзи-влияние
        for emo, tone_name in emoji_to_tone.items():
            if emo in raw:
                scores[tone_name] += raw.count(emo)

        # Бонус за '!' как прокси интенсивности -> скорее в сторону urgency
        exclam = raw.count("!")
        if exclam > 0:
            scores["urgency"] += exclam

        # Выбор тона по максимуму; при равенстве или нуле — neutral
        best_tone = "neutral"
        try:
            max_val = max(scores.values()) if scores else 0
            if max_val > 0:
                # если несколько с одинаковым максимумом — neutral
                winners = [k for k, v in scores.items() if v == max_val]
                best_tone = winners[0] if len(winners) == 1 else "neutral"
        except Exception:
            best_tone = "neutral"

        # --- Извлечение ключевых слов ---
        # Токенизация: русские/латинские буквы и цифры, остальное — разделители
        import re
        tokens = re.findall(r"[а-яёa-z0-9]+", s, flags=re.IGNORECASE)
        if not tokens:
            return {"tone": best_tone, "keywords": []}

        stop = {"и", "в", "на", "это", "как", "что", "я", "мы", "они", "он", "она", "а", "но", "или", "да"}
        # Частоты
        freq: dict[str, int] = {}
        for t in tokens:
            if t in stop:
                continue
            freq[t] = freq.get(t, 0) + 1

        # Кандидаты — те токены, которые соприкасаются с любым индикаторным стеммом
        indicator_stems = set().union(*indicators.values())
        candidates = []
        for word, cnt in freq.items():
            if any(stem in word for stem in indicator_stems):
                candidates.append((word, cnt, len(word)))

        # Сортировка по убыванию: частота, длина; затем лексикографически
        candidates.sort(key=lambda x: (-x[1], -x[2], x[0]))

        # Выбрать до 4 уникальных по слову
        kw = []
        seen = set()
        for w, _, _ in candidates:
            if w not in seen:
                seen.add(w)
                kw.append(w)
            if len(kw) >= 4:
                break

        return {"tone": best_tone, "keywords": kw}
    except Exception:
        # На всякий случай — фолбэк: полный бэкомпат
        return {"tone": "neutral", "keywords": []}

# --- Utility: emoji heuristics ---
def compute_emojis_for_segment(text: str, tone: str, max_count: int) -> list[str]:
    """
    emoji: heuristics and placement
    Автономная эвристика выбора эмодзи на основе тона и ключевых слов.

    Правила:
    - Если max_count <= 0 -> [].
    - Карта по тону:
        urgency: ["🔥", "⚠️", "💥"]
        drama: ["💔", "😢", "💀"]
        positive: ["😂", "✨", "🎉", "😊"]
        neutral: []
    - Ключевые слова (рус. стеммы/подстроки):
        смех/юмор -> приоритет "😂" | триггеры: {"смешн","ха","лол","ахаха"}
        экшн/напряжение -> "🔥","⚠️" | триггеры: {"срочно","беги","атак","взрыв","пожар"}
        радость/успех -> "🎉","✨" | триггеры: {"ура","побед","круто","супер","класс"}
    - Сначала кандидаты по ключевым словам (в указанном порядке групп), затем добиваем из карты тона.
    - Удаляем дубликаты, обрезаем до max_count.
    - Никаких внешних вызовов, зависимостей — чистая локальная функция.
    """
    try:
        if max_count is None:
            max_count = 0
        try:
            max_count = int(max_count)
        except Exception:
            max_count = 0

        if max_count <= 0:
            return []

        s = str(text or "")
        s_lower = s.lower()

        tone_map = {
            "urgency": ["🔥", "⚠️", "💥"],
            "drama": ["💔", "😢", "💀"],
            "positive": ["😂", "✨", "🎉", "😊"],
            "neutral": [],
        }

        # Ключевые слова (стеммы/подстроки)
        kw_laughter = {"смешн", "ха", "лол", "ахаха"}
        kw_action   = {"срочно", "беги", "атак", "взрыв", "пожар"}
        kw_joy      = {"ура", "побед", "круто", "супер", "класс"}

        candidates: list[str] = []

        # Смех/юмор — добавляем "😂" один раз, если найдено совпадение
        if any(tok in s_lower for tok in kw_laughter):
            candidates.append("😂")

        # Экшн/напряжение — "🔥" и "⚠️" (в таком порядке)
        if any(tok in s_lower for tok in kw_action):
            candidates.extend(["🔥", "⚠️"])

        # Радость/успех — "🎉" и "✨"
        if any(tok in s_lower for tok in kw_joy):
            candidates.extend(["🎉", "✨"])

        # Добиваем из карты тона
        t = str(tone or "neutral").lower().strip()
        tone_candidates = tone_map.get(t, [])
        candidates.extend(tone_candidates)

        # Уникализация с сохранением порядка
        seen = set()
        result: list[str] = []
        for emo in candidates:
            if emo not in seen:
                seen.add(emo)
                result.append(emo)
            if len(result) >= max_count:
                break

        return result[:max_count]
    except Exception:
        # На всякий случай — "тихий" фолбэк
        return []
</file>

<file path="Components/Logger.py">
"""
Улучшенная система логирования и мониторинга ресурсов для проекта обработки видео.
Включает детальное логирование, мониторинг GPU/CPU, прогресс-бары и ротацию логов.
"""

import time
import logging
import psutil
import GPUtil
from functools import wraps
from typing import Optional, Dict, Any, Callable
from pathlib import Path
import json
from datetime import datetime
import threading
from concurrent.futures import ThreadPoolExecutor
import torch
from dataclasses import dataclass, field
from contextlib import contextmanager
from tqdm import tqdm
import os
from logging.handlers import RotatingFileHandler
import sys


@dataclass
class PerformanceMetrics:
    """Метрики производительности для операций"""
    operation_name: str
    start_time: float
    end_time: Optional[float] = None
    cpu_usage_start: float = 0.0
    cpu_usage_end: float = 0.0
    memory_usage_start: int = 0
    memory_usage_end: int = 0
    gpu_usage_start: Optional[Dict[str, float]] = None
    gpu_usage_end: Optional[Dict[str, float]] = None
    success: bool = True
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def duration(self) -> float:
        """Длительность операции в секундах"""
        if self.end_time is None:
            return time.time() - self.start_time
        return self.end_time - self.start_time

    @property
    def cpu_delta(self) -> float:
        """Изменение CPU использования в процентах"""
        return self.cpu_usage_end - self.cpu_usage_start

    @property
    def memory_delta(self) -> int:
        """Изменение памяти в байтах"""
        return self.memory_usage_end - self.memory_usage_start


class ResourceMonitor:
    """Мониторинг системных ресурсов"""

    def __init__(self):
        self.gpu_available = torch.cuda.is_available()
        self.gpu_count = torch.cuda.device_count() if self.gpu_available else 0

    def get_cpu_usage(self) -> float:
        """Получить текущее использование CPU в процентах"""
        return psutil.cpu_percent(interval=0.1)

    def get_memory_usage(self) -> int:
        """Получить текущее использование памяти в байтах"""
        return psutil.Process().memory_info().rss

    def get_gpu_usage(self) -> Optional[Dict[str, float]]:
        """Получить использование GPU"""
        if not self.gpu_available:
            return None

        try:
            gpus = GPUtil.getGPUs()
            gpu_data = {}
            for i, gpu in enumerate(gpus):
                gpu_data[f'gpu_{i}'] = {
                    'usage': gpu.load * 100,
                    'memory_used': gpu.memoryUsed,
                    'memory_total': gpu.memoryTotal,
                    'temperature': gpu.temperature
                }
            return gpu_data
        except Exception:
            return None

    def get_system_info(self) -> Dict[str, Any]:
        """Получить общую информацию о системе"""
        return {
            'cpu_count': psutil.cpu_count(),
            'cpu_count_logical': psutil.cpu_count(logical=True),
            'memory_total': psutil.virtual_memory().total,
            'gpu_available': self.gpu_available,
            'gpu_count': self.gpu_count,
            'torch_version': torch.__version__,
            'cuda_version': torch.version.cuda if self.gpu_available else None
        }


class AdvancedLogger:
    """Улучшенный логгер с мониторингом ресурсов и таймингами"""

    def __init__(self, name: str = "VideoProcessor", log_dir: str = "logs"):
        self.name = name
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)

        self.monitor = ResourceMonitor()
        self.active_operations: Dict[str, PerformanceMetrics] = {}
        self._lock = threading.Lock()

        # Настройка логгеров
        self._setup_loggers()

        # Thread pool для асинхронных операций
        self.executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix="Logger")

        # GPU-first настройки
        self._ensure_gpu_priority()

    def _setup_loggers(self):
        """Настройка системы логирования с ротацией"""
        # Основной логгер
        self.logger = logging.getLogger(f"{self.name}.main")
        self.logger.setLevel(logging.DEBUG)

        # Убираем существующие обработчики
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)

        # Файловый обработчик с ротацией
        log_file = self.log_dir / f"{self.name.lower()}.log"
        file_handler = RotatingFileHandler(
            log_file,
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5,
            encoding='utf-8'
        )
        file_handler.setLevel(logging.DEBUG)

        # Консольный обработчик
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)

        # Форматтеры
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s'
        )

        file_handler.setFormatter(file_formatter)
        console_handler.setFormatter(console_formatter)

        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)

        # Отдельный логгер для производительности
        self.perf_logger = logging.getLogger(f"{self.name}.performance")
        perf_file = self.log_dir / f"{self.name.lower()}_performance.log"
        perf_handler = RotatingFileHandler(
            perf_file,
            maxBytes=5 * 1024 * 1024,  # 5MB
            backupCount=3,
            encoding='utf-8'
        )
        perf_handler.setFormatter(file_formatter)
        self.perf_logger.addHandler(perf_handler)
        self.perf_logger.setLevel(logging.INFO)

        # Отдельный логгер для ошибок
        self.error_logger = logging.getLogger(f"{self.name}.errors")
        error_file = self.log_dir / f"{self.name.lower()}_errors.log"
        error_handler = RotatingFileHandler(
            error_file,
            maxBytes=5 * 1024 * 1024,  # 5MB
            backupCount=3,
            encoding='utf-8'
        )
        error_handler.setFormatter(file_formatter)
        self.error_logger.addHandler(error_handler)
        self.error_logger.setLevel(logging.ERROR)

    def _ensure_gpu_priority(self):
        """Обеспечить GPU-first подход"""
        if self.monitor.gpu_available:
            try:
                # Установить текущий GPU
                torch.cuda.set_device(0)

                # Оптимизации для GPU
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False

                # Очистить кэш GPU
                torch.cuda.empty_cache()

                self.logger.info(f"GPU-first режим активирован. Используется GPU: {torch.cuda.get_device_name(0)}")
            except Exception as e:
                self.logger.warning(f"Не удалось настроить GPU-first режим: {e}")
        else:
            self.logger.info("GPU не доступен, используется CPU режим")

    def start_operation(self, operation_name: str, metadata: Optional[Dict[str, Any]] = None) -> str:
        """Начать отслеживание операции"""
        operation_id = f"{operation_name}_{time.time()}_{threading.get_ident()}"

        with self._lock:
            metrics = PerformanceMetrics(
                operation_name=operation_name,
                start_time=time.time(),
                cpu_usage_start=self.monitor.get_cpu_usage(),
                memory_usage_start=self.monitor.get_memory_usage(),
                gpu_usage_start=self.monitor.get_gpu_usage(),
                metadata=metadata or {}
            )
            self.active_operations[operation_id] = metrics

        self.logger.info(f"🚀 Начата операция: {operation_name} (ID: {operation_id})")
        return operation_id

    def end_operation(self, operation_id: str, success: bool = True, error_message: Optional[str] = None):
        """Завершить отслеживание операции"""
        with self._lock:
            if operation_id not in self.active_operations:
                self.logger.warning(f"Операция {operation_id} не найдена")
                return

            metrics = self.active_operations[operation_id]
            metrics.end_time = time.time()
            metrics.cpu_usage_end = self.monitor.get_cpu_usage()
            metrics.memory_usage_end = self.monitor.get_memory_usage()
            metrics.gpu_usage_end = self.monitor.get_gpu_usage()
            metrics.success = success
            metrics.error_message = error_message

        # Логирование результатов
        self._log_operation_results(metrics)

        # Удаление из активных операций
        with self._lock:
            del self.active_operations[operation_id]

    def _log_operation_results(self, metrics: PerformanceMetrics):
        """Логирование результатов операции"""
        duration = metrics.duration
        cpu_delta = metrics.cpu_delta
        memory_delta = metrics.memory_delta / 1024 / 1024  # MB

        status_icon = "✅" if metrics.success else "❌"
        status_text = "УСПЕХ" if metrics.success else "ОШИБКА"

        # Основное логирование
        self.logger.info(
            ".2f"
            ".1f"
            ".1f"
            f"{status_icon} {status_text}"
        )

        # Детальное логирование производительности
        perf_data = {
            'operation': metrics.operation_name,
            'duration': duration,
            'cpu_usage_start': metrics.cpu_usage_start,
            'cpu_usage_end': metrics.cpu_usage_end,
            'cpu_delta': cpu_delta,
            'memory_start_mb': metrics.memory_usage_start / 1024 / 1024,
            'memory_end_mb': metrics.memory_usage_end / 1024 / 1024,
            'memory_delta_mb': memory_delta,
            'success': metrics.success,
            'timestamp': datetime.now().isoformat(),
            'metadata': metrics.metadata
        }

        if metrics.gpu_usage_start and metrics.gpu_usage_end:
            perf_data['gpu_usage_start'] = metrics.gpu_usage_start
            perf_data['gpu_usage_end'] = metrics.gpu_usage_end

        if not metrics.success and metrics.error_message:
            perf_data['error'] = metrics.error_message

        # Асинхронная запись в файл
        self.executor.submit(self._write_performance_log, perf_data)

        # Логирование ошибок отдельно
        if not metrics.success and metrics.error_message:
            self.error_logger.error(
                f"Ошибка в операции {metrics.operation_name}: {metrics.error_message}"
            )

    def _write_performance_log(self, perf_data: Dict[str, Any]):
        """Асинхронная запись лога производительности"""
        try:
            log_entry = json.dumps(perf_data, ensure_ascii=False, indent=2)
            self.perf_logger.info(f"PERFORMANCE_DATA: {log_entry}")
        except Exception as e:
            self.logger.error(f"Ошибка записи лога производительности: {e}")

    @contextmanager
    def operation_context(self, operation_name: str, metadata: Optional[Dict[str, Any]] = None):
        """Контекстный менеджер для операций"""
        operation_id = self.start_operation(operation_name, metadata)
        try:
            yield operation_id
            self.end_operation(operation_id, success=True)
        except Exception as e:
            self.end_operation(operation_id, success=False, error_message=str(e))
            raise

    def create_progress_bar(self, total: int, desc: str = "", unit: str = "it") -> tqdm:
        """Создать прогресс-бар с мониторингом ресурсов"""
        return tqdm(
            total=total,
            desc=desc,
            unit=unit,
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] '
                      '{postfix}]'
        )

    def log_system_info(self):
        """Логирование информации о системе"""
        system_info = self.monitor.get_system_info()
        self.logger.info("=== ИНФОРМАЦИЯ О СИСТЕМЕ ===")
        for key, value in system_info.items():
            self.logger.info(f"{key}: {value}")
        self.logger.info("=" * 30)

    def get_active_operations_count(self) -> int:
        """Получить количество активных операций"""
        with self._lock:
            return len(self.active_operations)

    def cleanup(self):
        """Очистка ресурсов"""
        self.executor.shutdown(wait=True)
        # Очистка GPU памяти если возможно
        if self.monitor.gpu_available:
            try:
                torch.cuda.empty_cache()
            except Exception:
                pass


# Глобальный экземпляр логгера
logger = AdvancedLogger()


def timed_operation(operation_name: str, metadata: Optional[Dict[str, Any]] = None):
    """Декоратор для отслеживания операций"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            with logger.operation_context(operation_name, metadata):
                return func(*args, **kwargs)
        return wrapper
    return decorator


def log_function_call(func: Callable) -> Callable:
    """Декоратор для логирования вызовов функций"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        func_name = f"{func.__module__}.{func.__name__}" if hasattr(func, '__module__') else func.__name__
        logger.logger.debug(f"Вызов функции: {func_name}")
        start_time = time.time()

        try:
            result = func(*args, **kwargs)
            duration = time.time() - start_time
            logger.logger.debug(".3f")
            return result
        except Exception as e:
            duration = time.time() - start_time
            logger.logger.error(".3f")
            raise
    return wrapper
</file>

<file path="Components/Paths.py">
from typing import Tuple
import os
from pathlib import Path
from Components.config import get_config

def build_short_output_name(base_name: str, idx: int, shorts_dir: str) -> Tuple[str, str]:
    """
    Формирует уникальные пути для итогового short-файла и соответствующего временного файла анимации.

    Итоговый шаблон:
    shorts/{base_name}_highlight_{idx:02d}_final.mp4

    Временный файл для анимации:
    {final_path}_temp_anim.mp4
    """
    file_name = f"{base_name}_highlight_{idx:02d}_final.mp4"
    final_path = os.path.join(shorts_dir, file_name)
    temp_anim_path = f"{final_path}_temp_anim.mp4"
    return final_path, temp_anim_path


def resolve_path(*parts: str) -> str:
    """
    Возвращает абсолютный путь, собранный относительно base_dir из конфига.

    Пример:
      resolve_path("a", "b", "c.txt") -> "<ABS_BASE_DIR>/a/b/c.txt"

    Всегда нормализует и резолвит путь (Path(...).resolve()).
    """
    cfg = get_config()
    base_dir = Path(cfg.paths.base_dir)
    abs_path = (base_dir.joinpath(*[str(p) for p in parts])).resolve()
    return str(abs_path)


def fonts_path(font_filename: str) -> str:
    """
    Возвращает абсолютный путь к шрифту, собранный от base_dir/fonts_dir.

    Пример:
      fonts_path("Montserrat-Bold.ttf") -> "<ABS_BASE_DIR>/<fonts_dir>/Montserrat-Bold.ttf"

    Всегда нормализует и резолвит путь (Path(...).resolve()).
    """
    cfg = get_config()
    base_dir = Path(cfg.paths.base_dir)
    fonts_root = (base_dir / cfg.paths.fonts_dir).resolve()
    return str((fonts_root / font_filename).resolve())
</file>

<file path="Components/PauseAnalysis.py">
"""
Интеллектуальный анализатор пауз для обрезки видео.
Использует ИИ для классификации пауз и определения их важности.
"""

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple
import json
import hashlib
import os
from datetime import datetime, timedelta

from Components.LanguageTasks import call_llm_with_retry, make_generation_config
from Components.config import get_config, IntelligentPauseAnalysisConfig
from Components.Logger import logger


@dataclass
class PauseAnalysis:
    """Результат анализа одной паузы"""
    start_time: float
    end_time: float
    duration: float
    category: str  # "structural", "filler", "emphasis", "breathing"
    confidence: float  # 0.0 to 1.0
    importance_score: float  # -1.0 to 1.0 (отрицательный = менее важный)
    should_trim: bool
    reasoning: str
    context_before: str = ""
    context_after: str = ""


@dataclass
class PauseAnalysisResult:
    """Результат анализа всех пауз в транскрипции"""
    pauses: List[PauseAnalysis] = field(default_factory=list)
    total_pauses_analyzed: int = 0
    ai_processed_pauses: int = 0
    legacy_processed_pauses: int = 0
    cache_hits: int = 0
    processing_time_seconds: float = 0.0
    errors: List[str] = field(default_factory=list)


class IntelligentPauseAnalyzer:
    """
    Интеллектуальный анализатор пауз с использованием ИИ.

    Основные возможности:
    - Классификация пауз по категориям (структурные, заполнители, для эффекта, дыхательные)
    - Определение важности пауз на основе контекста
    - Использование gemini-2.5-flash-lite для простых задач
    - Кеширование результатов анализа
    - Оптимизация под API ограничения
    """

    def __init__(self, config: IntelligentPauseAnalysisConfig):
        self.config = config
        self.cache = {}  # Простой in-memory кеш
        self._load_cache_from_disk()

    def analyze_pauses_in_transcription(self, transcription_data: Dict[str, Any]) -> PauseAnalysisResult:
        """
        Основной метод анализа пауз в транскрипции.

        Args:
            transcription_data: Данные транскрипции с сегментами

        Returns:
            PauseAnalysisResult: Результаты анализа всех пауз
        """
        import time
        start_time = time.time()

        result = PauseAnalysisResult()

        try:
            # Извлекаем паузы из транскрипции
            pauses = self._extract_pauses_from_transcription(transcription_data)
            result.total_pauses_analyzed = len(pauses)

            if not pauses:
                logger.logger.info("Анализ пауз: паузы не найдены в транскрипции")
                return result

            logger.logger.info(f"Анализ пауз: найдено {len(pauses)} пауз для анализа")

            # Обрабатываем паузы с использованием ИИ или легаси-метода
            analyzed_pauses = []

            if self.config.enabled:
                # Используем ИИ-анализ
                analyzed_pauses = self._analyze_pauses_with_ai(pauses, transcription_data, result)
            else:
                # Используем легаси-метод
                analyzed_pauses = self._analyze_pauses_legacy(pauses, transcription_data, result)

            result.pauses = analyzed_pauses
            result.processing_time_seconds = time.time() - start_time

            # Сохраняем кеш на диск
            self._save_cache_to_disk()

            logger.logger.info(f"Анализ пауз завершен: {len(analyzed_pauses)} пауз обработано за {result.processing_time_seconds:.2f}с")
            logger.logger.info(f"ИИ-обработка: {result.ai_processed_pauses}, Легаси: {result.legacy_processed_pauses}, Кеш: {result.cache_hits}")

        except Exception as e:
            error_msg = f"Ошибка при анализе пауз: {str(e)}"
            logger.logger.error(error_msg)
            result.errors.append(error_msg)
            result.processing_time_seconds = time.time() - start_time

        return result

    def _extract_pauses_from_transcription(self, transcription_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Извлекает паузы из данных транскрипции.

        Returns:
            List[Dict]: Список пауз с start_time, end_time, duration
        """
        pauses = []
        segments = transcription_data.get('segments', [])

        if not segments:
            return pauses

        # Сортируем сегменты по времени
        sorted_segments = sorted(segments, key=lambda x: x.get('start', 0) if isinstance(x, dict) else 0)

        for i in range(len(sorted_segments) - 1):
            current_segment = sorted_segments[i]
            next_segment = sorted_segments[i + 1]

            # Извлекаем времена окончания и начала
            if isinstance(current_segment, dict) and isinstance(next_segment, dict):
                current_end = current_segment.get('end', 0)
                next_start = next_segment.get('start', 0)

                if next_start > current_end:
                    duration = next_start - current_end
                    pauses.append({
                        'start_time': current_end,
                        'end_time': next_start,
                        'duration': duration,
                        'context_before': current_segment.get('text', ''),
                        'context_after': next_segment.get('text', '')
                    })

        # Фильтруем паузы по минимальной длительности
        min_pause_duration = 0.1  # 100ms минимум
        pauses = [p for p in pauses if p['duration'] >= min_pause_duration]

        return pauses

    def _analyze_pauses_with_ai(self, pauses: List[Dict[str, Any]], transcription_data: Dict[str, Any],
                               result: PauseAnalysisResult) -> List[PauseAnalysis]:
        """
        Анализирует паузы с использованием ИИ.
        """
        analyzed_pauses = []

        # Группируем паузы в батчи для оптимизации API
        batches = self._create_pause_batches(pauses)

        for batch in batches:
            try:
                batch_results = self._analyze_pause_batch(batch, transcription_data, result)
                analyzed_pauses.extend(batch_results)

                # Проверяем лимиты API
                if self._should_apply_rate_limit():
                    import time
                    delay = self.config.api_optimization.get('rate_limit_delay', 1.0)
                    logger.logger.debug(f"Применяем задержку API: {delay}с")
                    time.sleep(delay)

            except Exception as e:
                logger.logger.warning(f"Ошибка при анализе батча пауз: {e}")
                # Откатываемся на легаси-метод для этого батча
                if self.config.api_optimization.get('fallback_to_legacy', True):
                    logger.logger.info("Откатываемся на легаси-метод для батча")
                    legacy_results = self._analyze_pauses_legacy(batch, transcription_data, result)
                    analyzed_pauses.extend(legacy_results)

        return analyzed_pauses

    def _create_pause_batches(self, pauses: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
        """
        Группирует паузы в батчи для оптимизации API.
        """
        batch_size = self.config.batch_size
        return [pauses[i:i + batch_size] for i in range(0, len(pauses), batch_size)]

    def _analyze_pause_batch(self, batch: List[Dict[str, Any]], transcription_data: Dict[str, Any],
                           result: PauseAnalysisResult) -> List[PauseAnalysis]:
        """
        Анализирует батч пауз с использованием ИИ.
        """
        # Проверяем кеш
        cache_key = self._generate_cache_key(batch, transcription_data)
        if self.config.cache_enabled and cache_key in self.cache:
            cached_result = self._get_cached_result(cache_key)
            if cached_result:
                result.cache_hits += len(cached_result)
                logger.logger.debug(f"Кеш hit для батча из {len(cached_result)} пауз")
                return cached_result

        # Создаем промпт для ИИ
        prompt = self._create_pause_analysis_prompt(batch, transcription_data)

        try:
            # Используем gemini-2.5-flash-lite для простых задач
            generation_config = make_generation_config(
                system_instruction=self._get_pause_analysis_system_prompt(),
                temperature=self.config.temperature
            )

            response = call_llm_with_retry(
                system_instruction=None,
                content=prompt,
                generation_config=generation_config,
                model=self.config.model,
                max_api_attempts=self.config.max_attempts
            )

            if not response or not response.text:
                raise ValueError("Пустой ответ от ИИ")

            # Парсим ответ
            analysis_results = self._parse_pause_analysis_response(response.text)

            # Создаем объекты PauseAnalysis
            pause_analyses = []
            for i, analysis_data in enumerate(analysis_results):
                if i < len(batch):
                    pause_data = batch[i]
                    analysis = self._create_pause_analysis_from_data(pause_data, analysis_data)
                    pause_analyses.append(analysis)

            result.ai_processed_pauses += len(pause_analyses)

            # Кешируем результат
            if self.config.cache_enabled:
                self._cache_result(cache_key, pause_analyses)

            return pause_analyses

        except Exception as e:
            logger.logger.warning(f"Ошибка при ИИ-анализе пауз: {e}")
            # Откатываемся на легаси-метод
            if self.config.api_optimization.get('fallback_to_legacy', True):
                return self._analyze_pauses_legacy(batch, transcription_data, result)
            else:
                # Возвращаем пустые результаты
                return []

    def _create_pause_analysis_prompt(self, batch: List[Dict[str, Any]], transcription_data: Dict[str, Any]) -> str:
        """
        Создает промпт для анализа пауз.
        """
        prompt_parts = []

        # Добавляем контекст транскрипции
        prompt_parts.append("КОНТЕКСТ ТРАНСКРИПЦИИ:")
        segments = transcription_data.get('segments', [])[:50]  # Ограничиваем для экономии токенов
        for seg in segments:
            if isinstance(seg, dict):
                start = seg.get('start', 0)
                text = seg.get('text', '')
                prompt_parts.append(".2f")

        prompt_parts.append("\nПАУЗЫ ДЛЯ АНАЛИЗА:")
        for i, pause in enumerate(batch):
            prompt_parts.append(f"Пауза {i+1}:")
            prompt_parts.append(f"  Время: {pause['start_time']:.2f}s - {pause['end_time']:.2f}s")
            prompt_parts.append(f"  Длительность: {pause['duration']:.2f}s")
            prompt_parts.append(f"  Контекст до: {pause.get('context_before', '')[:100]}...")
            prompt_parts.append(f"  Контекст после: {pause.get('context_after', '')[:100]}...")
            prompt_parts.append("")

        return "\n".join(prompt_parts)

    def _get_pause_analysis_system_prompt(self) -> str:
        """
        Возвращает системный промпт для анализа пауз.
        """
        categories = self.config.pause_categories

        return f"""
Ты — эксперт по анализу речевых пауз в транскрипциях. Твоя задача — классифицировать паузы и определить, следует ли их обрезать.

КАТЕГОРИИ ПАУЗ:
- structural: Структурные паузы (конец предложения, смена темы) — НЕ ОБРЕЗАТЬ
- filler: Заполнители речи ("э-э", "м-м", паузы размышления) — ОБРЕЗАТЬ
- emphasis: Паузы для эффекта (драматические паузы) — АНАЛИЗИРОВАТЬ КОНТЕКСТ
- breathing: Дыхательные паузы — ОБРЕЗАТЬ при длительности > 1.5с

КРИТЕРИИ ОЦЕНКИ:
1. Длительность паузы
2. Контекст до и после паузы
3. Лингвистические маркеры
4. Цель паузы (структурная vs случайная)

ВЕРНИ ТОЛЬКО JSON-массив объектов с полями:
- category: одна из категорий выше
- confidence: уверенность от 0.0 до 1.0
- importance_score: от -1.0 (не важен) до 1.0 (очень важен)
- should_trim: true/false
- reasoning: краткое объяснение решения

Пример ответа:
[
  {{
    "category": "filler",
    "confidence": 0.9,
    "importance_score": -0.7,
    "should_trim": true,
    "reasoning": "Короткая пауза размышления между словами"
  }}
]
"""

    def _parse_pause_analysis_response(self, response_text: str) -> List[Dict[str, Any]]:
        """
        Парсит ответ ИИ с анализом пауз.
        """
        try:
            # Извлекаем JSON из ответа
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response_text)
            if json_match:
                json_str = json_match.group(1)
            else:
                # Ищем JSON-массив в тексте
                start = response_text.find('[')
                end = response_text.rfind(']') + 1
                if start != -1 and end > start:
                    json_str = response_text[start:end]
                else:
                    json_str = response_text.strip()

            data = json.loads(json_str)
            if isinstance(data, list):
                return data
            else:
                logger.logger.warning("ИИ вернул не массив в ответе анализа пауз")
                return []

        except Exception as e:
            logger.logger.warning(f"Ошибка парсинга ответа ИИ: {e}")
            return []

    def _create_pause_analysis_from_data(self, pause_data: Dict[str, Any], analysis_data: Dict[str, Any]) -> PauseAnalysis:
        """
        Создает объект PauseAnalysis из данных паузы и анализа.
        """
        return PauseAnalysis(
            start_time=pause_data['start_time'],
            end_time=pause_data['end_time'],
            duration=pause_data['duration'],
            category=analysis_data.get('category', 'unknown'),
            confidence=float(analysis_data.get('confidence', 0.5)),
            importance_score=float(analysis_data.get('importance_score', 0.0)),
            should_trim=bool(analysis_data.get('should_trim', False)),
            reasoning=analysis_data.get('reasoning', ''),
            context_before=pause_data.get('context_before', ''),
            context_after=pause_data.get('context_after', '')
        )

    def _analyze_pauses_legacy(self, pauses: List[Dict[str, Any]], transcription_data: Dict[str, Any],
                              result: PauseAnalysisResult) -> List[PauseAnalysis]:
        """
        Легаси-метод анализа пауз без ИИ.
        """
        analyzed_pauses = []

        for pause_data in pauses:
            duration = pause_data['duration']

            # Простая логика определения типа паузы
            if duration < 0.5:
                category = "filler"
                should_trim = True
                importance_score = -0.5
                reasoning = "Короткая пауза, вероятно заполнитель"
            elif duration > 2.0:
                category = "structural"
                should_trim = False
                importance_score = 0.8
                reasoning = "Длинная пауза, вероятно структурная"
            else:
                category = "breathing"
                should_trim = duration > 1.5
                importance_score = 0.0
                reasoning = "Средняя пауза, возможно дыхательная"

            analysis = PauseAnalysis(
                start_time=pause_data['start_time'],
                end_time=pause_data['end_time'],
                duration=duration,
                category=category,
                confidence=0.7,  # Фиксированная уверенность для легаси
                importance_score=importance_score,
                should_trim=should_trim,
                reasoning=reasoning,
                context_before=pause_data.get('context_before', ''),
                context_after=pause_data.get('context_after', '')
            )

            analyzed_pauses.append(analysis)

        result.legacy_processed_pauses += len(analyzed_pauses)
        return analyzed_pauses

    def _generate_cache_key(self, batch: List[Dict[str, Any]], transcription_data: Dict[str, Any]) -> str:
        """
        Генерирует ключ кеша для батча пауз.
        """
        # Создаем хеш на основе контекста пауз
        cache_data = {
            'pause_times': [(p['start_time'], p['end_time']) for p in batch],
            'transcription_hash': self._get_transcription_hash(transcription_data)
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()

    def _get_transcription_hash(self, transcription_data: Dict[str, Any]) -> str:
        """
        Генерирует хеш транскрипции для кеширования.
        """
        segments = transcription_data.get('segments', [])
        if not segments:
            return "empty"

        # Берем первые и последние несколько сегментов для хеша
        sample_segments = segments[:3] + segments[-3:] if len(segments) > 6 else segments
        sample_text = json.dumps(sample_segments, sort_keys=True)
        return hashlib.md5(sample_text.encode()).hexdigest()

    def _cache_result(self, key: str, results: List[PauseAnalysis]) -> None:
        """
        Кеширует результаты анализа.
        """
        if not self.config.cache_enabled:
            return

        cache_entry = {
            'results': [
                {
                    'start_time': p.start_time,
                    'end_time': p.end_time,
                    'duration': p.duration,
                    'category': p.category,
                    'confidence': p.confidence,
                    'importance_score': p.importance_score,
                    'should_trim': p.should_trim,
                    'reasoning': p.reasoning,
                    'context_before': p.context_before,
                    'context_after': p.context_after
                } for p in results
            ],
            'timestamp': datetime.now().isoformat(),
            'ttl_hours': self.config.cache_ttl_hours
        }

        self.cache[key] = cache_entry

    def _get_cached_result(self, key: str) -> Optional[List[PauseAnalysis]]:
        """
        Получает результат из кеша.
        """
        if key not in self.cache:
            return None

        cache_entry = self.cache[key]

        # Проверяем срок действия кеша
        timestamp = datetime.fromisoformat(cache_entry['timestamp'])
        ttl_hours = cache_entry.get('ttl_hours', 24)
        if datetime.now() - timestamp > timedelta(hours=ttl_hours):
            # Кеш устарел
            del self.cache[key]
            return None

        # Восстанавливаем объекты PauseAnalysis
        results = []
        for data in cache_entry['results']:
            analysis = PauseAnalysis(
                start_time=data['start_time'],
                end_time=data['end_time'],
                duration=data['duration'],
                category=data['category'],
                confidence=data['confidence'],
                importance_score=data['importance_score'],
                should_trim=data['should_trim'],
                reasoning=data['reasoning'],
                context_before=data.get('context_before', ''),
                context_after=data.get('context_after', '')
            )
            results.append(analysis)

        return results

    def _load_cache_from_disk(self) -> None:
        """
        Загружает кеш с диска.
        """
        try:
            cache_file = os.path.join(os.getcwd(), "pause_analysis_cache.json")
            if os.path.exists(cache_file):
                with open(cache_file, 'r', encoding='utf-8') as f:
                    self.cache = json.load(f)
                logger.logger.debug(f"Загружен кеш анализа пауз: {len(self.cache)} записей")
        except Exception as e:
            logger.logger.warning(f"Не удалось загрузить кеш анализа пауз: {e}")
            self.cache = {}

    def _save_cache_to_disk(self) -> None:
        """
        Сохраняет кеш на диск.
        """
        try:
            cache_file = os.path.join(os.getcwd(), "pause_analysis_cache.json")
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
            logger.logger.debug(f"Сохранен кеш анализа пауз: {len(self.cache)} записей")
        except Exception as e:
            logger.logger.warning(f"Не удалось сохранить кеш анализа пауз: {e}")

    def _should_apply_rate_limit(self) -> bool:
        """
        Определяет, нужно ли применять ограничение частоты запросов.
        """
        # Простая логика: применяем задержку после каждого батча
        return self.config.api_optimization.get('use_batch_processing', True)


# Вспомогательные функции
def create_intelligent_pause_analyzer() -> IntelligentPauseAnalyzer:
    """
    Создает экземпляр IntelligentPauseAnalyzer с настройками из конфигурации.
    """
    config = get_config()
    return IntelligentPauseAnalyzer(config.film_mode.intelligent_pause_analysis)


def analyze_pauses_in_moment(moment_start: float, moment_end: float, transcription_data: Dict[str, Any]) -> List[PauseAnalysis]:
    """
    Анализирует паузы в рамках конкретного момента видео.

    Args:
        moment_start: Начало момента (секунды)
        moment_end: Конец момента (секунды)
        transcription_data: Данные транскрипции

    Returns:
        List[PauseAnalysis]: Анализ пауз в рамках момента
    """
    analyzer = create_intelligent_pause_analyzer()
    result = analyzer.analyze_pauses_in_transcription(transcription_data)

    # Фильтруем паузы, которые пересекаются с моментом
    moment_pauses = []
    for pause in result.pauses:
        if pause.end_time > moment_start and pause.start_time < moment_end:
            # Ограничиваем паузу границами момента
            effective_start = max(pause.start_time, moment_start)
            effective_end = min(pause.end_time, moment_end)

            adjusted_pause = PauseAnalysis(
                start_time=effective_start,
                end_time=effective_end,
                duration=effective_end - effective_start,
                category=pause.category,
                confidence=pause.confidence,
                importance_score=pause.importance_score,
                should_trim=pause.should_trim,
                reasoning=pause.reasoning,
                context_before=pause.context_before,
                context_after=pause.context_after
            )
            moment_pauses.append(adjusted_pause)

    return moment_pauses
</file>

<file path="Components/Speaker.py">
import cv2
import numpy as np
import webrtcvad
import wave
import contextlib
from pydub import AudioSegment
import os

# Update paths to the model files
prototxt_path = "models/deploy.prototxt"
model_path = "models/res10_300x300_ssd_iter_140000_fp16.caffemodel"
temp_audio_path = "temp_audio.wav"

# Load DNN model
net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)

# Initialize VAD
vad = webrtcvad.Vad(2)  # Aggressiveness mode from 0 to 3

def voice_activity_detection(audio_frame, sample_rate=16000):
    return vad.is_speech(audio_frame, sample_rate)

def extract_audio_from_video(video_path, audio_path):
    audio = AudioSegment.from_file(video_path)
    audio = audio.set_frame_rate(16000).set_channels(1)
    audio.export(audio_path, format="wav")

def process_audio_frame(audio_data, sample_rate=16000, frame_duration_ms=30):
    n = int(sample_rate * frame_duration_ms / 1000) * 2  # 2 bytes per sample
    offset = 0
    while offset + n <= len(audio_data):
        frame = audio_data[offset:offset + n]
        offset += n
        yield frame

global Frames
Frames = [] # [x,y,w,h]

def detect_faces_and_speakers(input_video_path, output_video_path):
    # Return Frams:
    global Frames
    # Extract audio from the video
    extract_audio_from_video(input_video_path, temp_audio_path)

    # Read the extracted audio
    with contextlib.closing(wave.open(temp_audio_path, 'rb')) as wf:
        sample_rate = wf.getframerate()
        audio_data = wf.readframes(wf.getnframes())

    cap = cv2.VideoCapture(input_video_path)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))

    frame_duration_ms = 30  # 30ms frames
    audio_generator = process_audio_frame(audio_data, sample_rate, frame_duration_ms)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        h, w = frame.shape[:2]
        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))
        net.setInput(blob)
        detections = net.forward()

        audio_frame = next(audio_generator, None)
        if audio_frame is None:
            break
        is_speaking_audio = voice_activity_detection(audio_frame, sample_rate)
        MaxDif = 0
        Add = []
        for i in range(detections.shape[2]):
            confidence = detections[0, 0, i, 2]
            if confidence > 0.3:  # Confidence threshold
                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                (x, y, x1, y1) = box.astype("int")
                face_width = x1 - x
                face_height = y1 - y

                # Draw bounding box
                cv2.rectangle(frame, (x, y), (x1, y1), (0, 255, 0), 2)

                # Assuming lips are approximately at the bottom third of the face
                lip_distance = abs((y + 2 * face_height // 3) - (y1))
                Add.append([[x, y, x1, y1], lip_distance])

                MaxDif == max(lip_distance, MaxDif)
        for i in range(detections.shape[2]):
            confidence = detections[0, 0, i, 2]
            if confidence > 0.3:  # Confidence threshold
                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                (x, y, x1, y1) = box.astype("int")
                face_width = x1 - x
                face_height = y1 - y

                # Draw bounding box
                cv2.rectangle(frame, (x, y), (x1, y1), (0, 255, 0), 2)

                # Assuming lips are approximately at the bottom third of the face
                lip_distance = abs((y + 2 * face_height // 3) - (y1))
                print(lip_distance)

                # Combine visual and audio cues
                if lip_distance >= MaxDif and is_speaking_audio:  # Adjust the threshold as needed
                    cv2.putText(frame, "Active Speaker", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                if lip_distance >= MaxDif:
                    break

        Frames.append([x, y, x1, y1])

        out.write(frame)
        cv2.imshow('Frame', frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()
    os.remove(temp_audio_path)



if __name__ == "__main__":
    detect_faces_and_speakers()
    print(Frames)
    print(len(Frames))
    print(Frames[1:5])
</file>

<file path="Components/SpeakerDetection.py">
import cv2
import numpy as np
#Face Detection function
def detect_faces(video_file):
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    # Load the video
    cap = cv2.VideoCapture(video_file)

    faces = []

    # Detect and store unique faces
    while len(faces) < 5:
        ret, frame = cap.read()
        if ret:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            detected_faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

            # Iterate through the detected faces
            for face in detected_faces:
                # Check if the face is already in the list of faces
                if not any(np.array_equal(face, f) for f in faces):
                    faces.append(face)

            # Print the number of unique faces detected so far
            print(f"Number of unique faces detected: {len(faces)}")

    # Release the video capture object
    cap.release()

    # If faces detected, return the list of faces
    if len(faces) > 0:
        return faces
    
def crop_video(faces, input_file, output_file):
    try:
        if len(faces) > 0:
            # Constants for cropping
            CROP_RATIO = 0.9  # Adjust the ratio to control how much of the face is visible in the cropped video
            VERTICAL_RATIO = 9 / 16  # Aspect ratio for the vertical video

            # Read the input video
            cap = cv2.VideoCapture(input_file)

            # Get the frame dimensions
            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

            # Calculate the target width and height for cropping (vertical format)
            target_height = int(frame_height * CROP_RATIO)
            target_width = int(target_height * VERTICAL_RATIO)

            # Create a VideoWriter object to save the output video
            fourcc = cv2.VideoWriter_fourcc(*"mp4v")
            output_video = cv2.VideoWriter(output_file, fourcc, 30.0, (target_width, target_height))

            # Loop through each frame of the input video
            while True:
                ret, frame = cap.read()

                # If no more frames, break out of the loop
                if not ret:
                    break

                # Iterate through each detected face
                for face in faces:
                    # Unpack the face coordinates
                    x, y, w, h = face

                    # Calculate the crop coordinates
                    crop_x = max(0, x + (w - target_width) // 2)  # Adjust the crop region to center the face
                    crop_y = max(0, y + (h - target_height) // 2)
                    crop_x2 = min(crop_x + target_width, frame_width)
                    crop_y2 = min(crop_y + target_height, frame_height)

                    # Crop the frame based on the calculated crop coordinates
                    cropped_frame = frame[crop_y:crop_y2, crop_x:crop_x2]

                    # Resize the cropped frame to the target dimensions
                    resized_frame = cv2.resize(cropped_frame, (target_width, target_height))

                    # Write the resized frame to the output video
                    output_video.write(resized_frame)

            cap.release()
            output_video.release()

            print("Video cropped successfully.")
        else:
            print("No faces detected in the video.")
    except Exception as e:
        print(f"Error during video cropping: {str(e)}")


    return None
if __name__ == "__main__":
    input = r"Short.mp4"
    faces = detect_faces(input)
    print(faces)
    crop_video(faces, input, "Cropped.mp4")
    print("DONE")
</file>

<file path="Components/Transcription.py">
from faster_whisper import WhisperModel
import torch
import os
import psutil
import time
from Components.Logger import logger, timed_operation
import json
from pathlib import Path
from Components.config import get_config
from Components.Paths import resolve_path

from typing import Optional

# Старые функции транскрипции (`transcribeAudio`, `transcribe_word_level_full`)
# были удалены и заменены единой функцией `transcribe_unified`.
# Это изменение повышает производительность, избегая повторной загрузки модели
# и двойного прохода по аудиофайлу.

@timed_operation("transcribe_unified")
def transcribe_unified(audio_path, model):
    """
    Выполняет транскрипцию аудио одним вызовом и возвращает две структуры данных:
    сегменты и полную транскрипцию с разбивкой по словам.

    Это единая точка входа для транскрипции, заменяющая двойные вызовы
    transcribeAudio и transcribe_word_level_full.

    Args:
        audio_path (str): Путь к аудиофайлу.
        model (WhisperModel): Загруженная модель faster-whisper.

    Returns:
        tuple[list, dict]: Кортеж, содержащий:
        - list: Список сегментов в формате [[text, start, end], ...].
        - dict: Полная транскрипция на уровне слов в формате
                {'segments': [{'text': str, 'start': float, 'end': float, 'words': [...]}, ...]}.
    """
    logger.logger.info(f"Запуск единой транскрипции аудио: {audio_path}")

    # Логирование начальных системных ресурсов
    _log_system_resources("Начало транскрипции")

    segments_legacy = []
    word_level_transcription = {"segments": []}

    try:
        # Создаем прогресс-бар для транскрипции
        progress_bar = logger.create_progress_bar(
            total=100,  # Процентное представление
            desc="Транскрипция аудио",
            unit="%"
        )

        start_time = time.time()

        # Запуск транскрипции с параметрами
        logger.logger.info("Запуск модели faster-whisper для транскрипции...")
        segments_gen, info = model.transcribe(
            audio=audio_path,
            beam_size=5,
            language="ru",
            condition_on_previous_text=True,
            vad_filter=True,
            word_timestamps=True  # Ключевой параметр для получения слов
        )

        # Получаем информацию о длительности аудио
        audio_duration = getattr(info, 'duration', 0.0)
        logger.logger.info(f"Длительность аудио: {audio_duration:.2f} секунд")

        processed_segments = 0
        total_segments = 0

        # Предварительный подсчет общего количества сегментов для прогресс-бара
        segments_list = list(segments_gen)
        total_segments = len(segments_list)
        segments_gen = iter(segments_list)  # Сбрасываем генератор

        logger.logger.info(f"Обнаружено {total_segments} сегментов для обработки")

        for i, seg in enumerate(segments_gen):
            segment_start = time.time()

            # 1. Формируем структуру для транскрипции на уровне слов
            seg_dict = {
                "start": float(seg.start),
                "end": float(seg.end),
                "text": seg.text
            }
            words_arr = []
            if getattr(seg, "words", None):
                for w in seg.words:
                    words_arr.append({
                        "word": w.word,
                        "start": float(w.start),
                        "end": float(w.end)
                    })
            seg_dict["words"] = words_arr
            word_level_transcription["segments"].append(seg_dict)

            # 2. Формируем "старую" структуру сегментов
            segments_legacy.append([seg.text, float(seg.start), float(seg.end)])

            processed_segments += 1

            # Обновляем прогресс-бар
            progress = int((processed_segments / total_segments) * 100) if total_segments > 0 else 100
            progress_bar.update(max(0, progress - progress_bar.n))
            progress_bar.set_postfix({
                "Сегмент": f"{processed_segments}/{total_segments}",
                "Время": f"{seg.start:.1f}s-{seg.end:.1f}s",
                "Слова": len(words_arr)
            })

            # Логирование обработки сегмента
            segment_time = time.time() - segment_start
            logger.logger.debug(f"Обработан сегмент {processed_segments}/{total_segments}: "
                              f"время={segment_time:.3f}s, слова={len(words_arr)}, "
                              f"текст='{seg.text[:50]}...'")

            # Периодическое логирование системных ресурсов
            if processed_segments % 10 == 0 or processed_segments == total_segments:
                _log_system_resources(f"Обработка сегмента {processed_segments}/{total_segments}")

        progress_bar.close()

        total_time = time.time() - start_time
        logger.logger.info(f"Транскрипция завершена за {total_time:.2f} секунд")
        logger.logger.info(f"Обработано {len(segments_legacy)} сегментов, "
                          f"скорость: {audio_duration/total_time:.2f}x")

        # Финальное логирование ресурсов
        _log_system_resources("Завершение транскрипции")

        # Экспорт артефактов транскрипции на диск
        try:
            cfg = get_config()
            out_dir = resolve_path(cfg.processing.transcriptions_dir)
            base_name = Path(audio_path).stem
            export_transcription_artifacts(base_name, word_level_transcription, out_dir)
        except Exception as ex:
            logger.logger.error(f"Ошибка при сохранении транскрипции на диск: {ex}")
            raise

        return segments_legacy, word_level_transcription

    except Exception as e:
        logger.logger.error(f"Ошибка в единой функции транскрипции: {e}")
        import traceback
        traceback.print_exc()
        # Возвращаем пустые структуры в случае ошибки
        return [], {"segments": []}


def _format_timestamp_srt(seconds: float) -> str:
    try:
        if seconds is None:
            seconds = 0.0
        seconds = float(seconds)
    except Exception:
        seconds = 0.0
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    ms = int(round((seconds - int(seconds)) * 1000))
    if ms == 1000:
        s += 1
        ms = 0
    return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"


def _format_timestamp_vtt(seconds: float) -> str:
    try:
        if seconds is None:
            seconds = 0.0
        seconds = float(seconds)
    except Exception:
        seconds = 0.0
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    ms = int(round((seconds - int(seconds)) * 1000))
    if ms == 1000:
        s += 1
        ms = 0
    return f"{h:02d}:{m:02d}:{s:02d}.{ms:03d}"


def export_transcription_artifacts(base_name: str, word_level_transcription: dict, out_dir: str) -> dict:
    """
    Экспортирует артефакты транскрипции в форматы TXT, JSON, SRT, VTT в указанный каталог.

    Args:
        base_name (str): Базовое имя файла без расширения.
        word_level_transcription (dict): Полная транскрипция с ключом "segments".
        out_dir (str): Каталог для сохранения файлов.

    Returns:
        dict: Словарь путей: {"txt": ..., "json": ..., "srt": ..., "vtt": ...}
    """
    if not isinstance(base_name, str) or not base_name:
        raise ValueError("base_name must be a non-empty string")
    if not isinstance(word_level_transcription, dict):
        raise ValueError("word_level_transcription must be a dict")

    os.makedirs(out_dir, exist_ok=True)

    txt_path = os.path.join(out_dir, f"{base_name}.txt")
    json_path = os.path.join(out_dir, f"{base_name}.json")
    srt_path = os.path.join(out_dir, f"{base_name}.srt")
    vtt_path = os.path.join(out_dir, f"{base_name}.vtt")

    segments = word_level_transcription.get("segments", []) or []

    # TXT: плоский текст
    plain_lines = []
    for seg in segments:
        text = str(seg.get("text", "")).strip()
        if text:
            plain_lines.append(text)
    with open(txt_path, "w", encoding="utf-8") as f_txt:
        f_txt.write("\n".join(plain_lines))
    logger.logger.info(f"Saving transcription to: {txt_path}")

    # JSON: структура транскрипции
    payload = word_level_transcription
    with open(json_path, "w", encoding="utf-8") as f_json:
        json.dump(payload, f_json, ensure_ascii=False, indent=2)
    logger.logger.info(f"Saving transcription to: {json_path}")

    # SRT
    with open(srt_path, "w", encoding="utf-8") as f_srt:
        idx = 1
        for seg in segments:
            text = str(seg.get("text", "")).strip()
            if not text:
                continue
            start = float(seg.get("start", 0.0) or 0.0)
            end = float(seg.get("end", 0.0) or 0.0)
            f_srt.write(f"{idx}\n")
            f_srt.write(f"{_format_timestamp_srt(start)} --> {_format_timestamp_srt(end)}\n")
            f_srt.write(f"{text}\n\n")
            idx += 1
    logger.logger.info(f"Saving transcription to: {srt_path}")

    # VTT
    with open(vtt_path, "w", encoding="utf-8") as f_vtt:
        f_vtt.write("WEBVTT\n\n")
        for seg in segments:
            text = str(seg.get("text", "")).strip()
            if not text:
                continue
            start = float(seg.get("start", 0.0) or 0.0)
            end = float(seg.get("end", 0.0) or 0.0)
            f_vtt.write(f"{_format_timestamp_vtt(start)} --> {_format_timestamp_vtt(end)}\n")
            f_vtt.write(f"{text}\n\n")
    logger.logger.info(f"Saving transcription to: {vtt_path}")

    return {"txt": txt_path, "json": json_path, "srt": srt_path, "vtt": vtt_path}


def _log_system_resources(context: str = ""):
    """
    Логирует текущие системные ресурсы (CPU, память, GPU если доступен).

    Args:
        context (str): Контекст для логирования (например, "Начало транскрипции")
    """
    try:
        # CPU и память
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        memory_used = memory.used / (1024**3)  # GB
        memory_total = memory.total / (1024**3)  # GB

        resource_info = {
            "cpu_percent": f"{cpu_percent:.1f}%",
            "memory_percent": f"{memory_percent:.1f}%",
            "memory_used": f"{memory_used:.2f}GB",
            "memory_total": f"{memory_total:.2f}GB"
        }

        # GPU ресурсы если доступны
        try:
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                resource_info["gpu_count"] = gpu_count

                for i in range(gpu_count):
                    gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)  # GB
                    gpu_allocated = torch.cuda.memory_allocated(i) / (1024**3)  # GB
                    gpu_reserved = torch.cuda.memory_reserved(i) / (1024**3)  # GB

                    resource_info[f"gpu_{i}_memory"] = f"{gpu_allocated:.2f}/{gpu_reserved:.2f}/{gpu_memory:.2f}GB"

                    # Температура GPU если доступна
                    try:
                        import subprocess
                        result = subprocess.run(['nvidia-smi', '--query-gpu=temperature.gpu',
                                               '--format=csv,noheader,nounits'],
                                              capture_output=True, text=True)
                        if result.returncode == 0:
                            temp = result.stdout.strip().split('\n')[i]
                            resource_info[f"gpu_{i}_temp"] = f"{temp}°C"
                    except:
                        pass
        except Exception as e:
            logger.logger.debug(f"Не удалось получить GPU информацию: {e}")

        context_str = f" [{context}]" if context else ""
        logger.logger.info(f"Системные ресурсы{context_str}: {resource_info}")

    except Exception as e:
        logger.logger.warning(f"Ошибка при логировании системных ресурсов: {e}")


def _to_float(val, default=None):
    try:
        return float(val)
    except Exception:
        return default


def prepare_words_for_segment(full_word_level_transcription: dict, start: float, stop: float) -> dict:
    """
    Подготавливает подмножество слов для одного сегмента [start, stop] из глобальной
    словной транскрипции и нормализует их таймкоды к координатам сегмента.

    Вход:
    - full_word_level_transcription: dict в формате
      {
        "segments": [
          {
            "words": [
              {"start": float, "end": float, "text"/"word": str, ...},
              ...
            ],
            ...
          },
          ...
        ]
      }
    - start: абсолютное время начала сегмента, секунды (float)
    - stop:  абсолютное время конца сегмента, секунды (float)

    Логика отбора и нормализации:
    - Выбираются только те слова, которые пересекаются с интервалом [start, stop]:
      word.start < stop и word.end > start.
    - Нормализация в координаты сегмента:
      start_rel = max(0.0, word.start - start)
      end_rel   = max(start_rel, word.end - start)
    - end_rel дополнительно ограничивается длительностью сегмента (stop - start), чтобы
      «хвост» слова за границей сегмента был обрезан.
    - Слова без числовых start/end пропускаются.
    - Результат сортируется по (start_rel, end_rel).

    Возврат:
    Структура совместимая с animate_captions:
    {
      "segments": [{
        "start": 0.0,
        "end": stop - start,
        "text": "segment",
        "words": [
          {"start": start_rel, "end": end_rel, "text": word_text}, ...
        ]
      }]}
    
    Предположения:
    - Функция чистая: не изменяет входные данные, не имеет побочных эффектов.
    - При некорректном входе возвращается сегмент с пустым списком слов и корректной длительностью.
    """
    try:
        seg_duration = max(0.0, float(stop) - float(start))
    except Exception:
        # На всякий случай, если приведение типов не удалось
        seg_duration = max(0.0, (stop or 0.0) - (start or 0.0))

    words_out = []
    try:
        segments_wl = []
        if isinstance(full_word_level_transcription, dict):
            segments_wl = full_word_level_transcription.get("segments", []) or []
        for seg_wl in segments_wl:
            # Достаём список слов из dict или объекта с атрибутом .words
            seg_words = seg_wl.get("words", []) if isinstance(seg_wl, dict) else getattr(seg_wl, "words", []) or []
            if not seg_words:
                continue
            for w in seg_words:
                if isinstance(w, dict):
                    s = _to_float(w.get("start", None), None)
                    e = _to_float(w.get("end", None), None)
                    txt = w.get("text", w.get("word", ""))
                else:
                    s = _to_float(getattr(w, "start", None), None)
                    e = _to_float(getattr(w, "end", None), None)
                    txt = getattr(w, "text", getattr(w, "word", ""))
                # Пропускаем некорректные таймкоды
                if s is None or e is None:
                    continue
                # Фильтр пересечения [start, stop]
                if e > start and s < stop:
                    start_rel = max(0.0, s - start)
                    end_rel = max(start_rel, e - start)
                    # Обрезка по длительности сегмента
                    if end_rel > seg_duration:
                        end_rel = seg_duration
                        if end_rel < start_rel:
                            start_rel = end_rel
                    words_out.append({"start": start_rel, "end": end_rel, "text": str(txt)})
        # Сортировка стабильна для предсказуемости
        words_out.sort(key=lambda x: (x["start"], x["end"]))
    except Exception:
        # В случае неожиданных проблем вернём пустой список слов, сохранив длительность
        words_out = []

    return {
        "segments": [{
            "start": 0.0,
            "end": seg_duration,
            "text": "segment",
            "words": words_out
        }]}
# Блок if __name__ == "__main__" был удален, так как он
# использовал устаревшие функции для тестирования.
</file>

<file path="Components/YoutubeDownloader.py">
import os
import subprocess
import glob
import time


def download_youtube_video(url):
    """
    Скачивает видео с YouTube устойчивым способом через yt-dlp (Python API), с автоматическим фолбэком
    на pytubefix/pytube. Гарантируется итоговый MP4 без перекодирования (ffmpeg -c copy).
    Возвращает путь к мастер-файлу MP4 или None при ошибке.
    """
    try:
        out_dir = "videos"
        os.makedirs(out_dir, exist_ok=True)

        def _ffmpeg_merge(v, a, merged_final):
            tmp_out = merged_final + ".tmp"
            print("Автослияние дорожек через ffmpeg (без перекодирования, copy)…")
            merge_cmd = [
                "ffmpeg",
                "-hide_banner",
                "-loglevel", "info",
                "-y",
                "-i", v,
                "-i", a,
                "-map", "0:v:0",
                "-map", "1:a:0",
                "-c:v", "copy",
                "-c:a", "copy",
                "-movflags", "+faststart",
                "-shortest",
                tmp_out,
            ]
            print("Запуск команды слияния:", " ".join(merge_cmd))
            mres = subprocess.run(merge_cmd, capture_output=True, text=True)
            if mres.returncode != 0:
                print("Ошибка: ffmpeg завершился с ненулевым кодом возврата.")
                print(f"Код возврата: {mres.returncode}")
                if mres.stderr:
                    print("stderr ffmpeg:")
                    print(mres.stderr.strip())
                print("Команда:", " ".join(merge_cmd))
                print("Не удалось объединить дорожки. Проверьте ffmpeg в PATH.")
                return None
            try:
                if os.path.exists(merged_final):
                    os.remove(merged_final)
                os.replace(tmp_out, merged_final)
                return merged_final
            except Exception as ren_e:
                print(f"Ошибка при переименовании итогового файла: {ren_e}")
                return None

        def _attempt_local_merge(start_time):
            video_parts = [
                p for p in glob.glob(os.path.join(out_dir, "master-*.mp4"))
                if os.path.getmtime(p) >= (start_time - 1.0)
            ]
            audio_parts = [
                p for p in glob.glob(os.path.join(out_dir, "master-*.m4a"))
                if os.path.getmtime(p) >= (start_time - 1.0)
            ]

            def stem(path):
                base = os.path.basename(path)
                return os.path.splitext(base)[0]  # master-<id>

            video_map = {stem(p): p for p in video_parts}
            audio_map = {stem(p): p for p in audio_parts}
            common = sorted(
                set(video_map.keys()) & set(audio_map.keys()),
                key=lambda s: os.path.getmtime(video_map[s]),
                reverse=True
            )

            if common:
                key = common[0]
                v = video_map[key]
                a = audio_map[key]
                merged_final = os.path.join(out_dir, f"{key}.mp4")
                res = _ffmpeg_merge(v, a, merged_final)
                if res:
                    # Чистим части
                    try:
                        os.remove(v)
                    except Exception:
                        pass
                    try:
                        os.remove(a)
                    except Exception:
                        pass
                    print(f"Загрузка через yt-dlp завершена. Итоговый файл: {res}")
                    return res
            return None

        def _fallback_pytube(src_url):
            try:
                from pytubefix import YouTube
                lib = "pytubefix"
            except Exception:
                try:
                    from pytube import YouTube
                    lib = "pytube"
                except Exception as e_imp:
                    print(f"Фолбэк pytubefix/pytube недоступен: {e_imp}")
                    return None

            try:
                print(f"Фолбэк: {lib} — пытаюсь скачать прогрессивный MP4…")
                # Имитируем поведение WEB‑клиента (для pytubefix) и отключаем OAuth/кэш.
                if lib == "pytubefix":
                    yt = YouTube(src_url, client="WEB", use_oauth=False, allow_oauth_cache=False)
                else:
                    yt = YouTube(src_url, use_oauth=False, allow_oauth_cache=False)
                # Прогрессивный (видео+аудио в одном mp4)
                stream = yt.streams.filter(progressive=True, file_extension="mp4").order_by("resolution").desc().first()
                if stream:
                    filename = f"master-{yt.video_id}.mp4"
                    final_path = stream.download(output_path=out_dir, filename=filename)
                    print(f"Загрузка завершена через {lib}. Итоговый файл: {final_path}")
                    return final_path

                print("Прогрессивный MP4 недоступен. Пытаюсь скачать адаптивные потоки mp4+m4a…")
                vstream = yt.streams.filter(adaptive=True, only_video=True, file_extension="mp4").order_by("resolution").desc().first()
                astream = yt.streams.filter(adaptive=True, only_audio=True, mime_type="audio/mp4").order_by("abr").desc().first()
                if not vstream or not astream:
                    print("Не удалось найти подходящие адаптивные потоки mp4 (видео) и m4a (аудио) для слияния.")
                    return None

                vpath = vstream.download(output_path=out_dir, filename=f"master-{yt.video_id}-video")
                apath = astream.download(output_path=out_dir, filename=f"master-{yt.video_id}-audio")
                merged_final = os.path.join(out_dir, f"master-{yt.video_id}.mp4")
                res = _ffmpeg_merge(vpath, apath, merged_final)
                if res:
                    try:
                        os.remove(vpath)
                    except Exception:
                        pass
                    try:
                        os.remove(apath)
                    except Exception:
                        pass
                    print(f"Загрузка завершена через {lib}. Итоговый файл: {res}")
                    return res
                return None
            except Exception as e_fb:
                print(f"Ошибка фолбэка {lib}: {e_fb}")
                return None

        start_time = time.time()

        # Попытка 1: yt-dlp (Python API)
        try:
            import yt_dlp as ytdlp
        except Exception as imp_err:
            print(f"yt-dlp (Python API) недоступен: {imp_err}")
            print("Перехожу к фолбэку pytubefix/pytube…")
            return _fallback_pytube(url)

        try:
            print("Использую yt-dlp (Python API) для устойчивой загрузки (mp4+m4a, без перекодирования)…")
            format_str = "bv*[ext=mp4][vcodec^=avc1]+ba[ext=m4a]/bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4]/b"
            out_tmpl = os.path.join(out_dir, "master-%(id)s.%(ext)s")
            ydl_opts = {
                "format": format_str,
                "outtmpl": out_tmpl,
                "merge_output_format": "mp4",
                "noplaylist": True,
                "prefer_ffmpeg": True,
                "quiet": True,
                "no_warnings": True,
                "retries": 3,
                # yt-dlp по умолчанию использует -c copy для merge; добавим faststart
                "postprocessor_args": ["-movflags", "+faststart"],
            }
            with ytdlp.YoutubeDL(ydl_opts) as ydl:
                _ = ydl.extract_info(url, download=True)

            # Проверяем итоговый mp4
            mp4s = [
                p for p in glob.glob(os.path.join(out_dir, "master-*.mp4"))
                if os.path.getmtime(p) >= (start_time - 1.0)
            ]
            if mp4s:
                mp4s.sort(key=os.path.getmtime, reverse=True)
                final_path = mp4s[0]
                print(f"Загрузка через yt-dlp завершена. Итоговый файл: {final_path}")
                return final_path

            # Пробуем локальное слияние, если остались отдельные дорожки
            merged = _attempt_local_merge(start_time)
            if merged:
                return merged

            print("yt-dlp не создал финальный MP4 и не найдены пары mp4+m4a для слияния.")
            print("Перехожу к фолбэку pytubefix/pytube…")
            return _fallback_pytube(url)

        except Exception as e_ydl:
            print(f"Ошибка при загрузке через yt-dlp (Python API): {e_ydl}")
            print("Перехожу к фолбэку pytubefix/pytube…")
            return _fallback_pytube(url)

    except Exception as e:
        print(f"Непредвиденная ошибка при скачивании: {e}")
        print("Не удалось скачать видео. Проверьте доступность ffmpeg в PATH.")
        return None


if __name__ == "__main__":
    youtube_url = input("Enter YouTube video URL: ")
    downloaded_file = download_youtube_video(youtube_url)
    if downloaded_file:
        print(f"\nDownload finished. File available at: {downloaded_file}")
    else:
        print("\nDownload failed.")
</file>

<file path="models/deploy.prototxt">
input: "data"
input_shape {
  dim: 1
  dim: 3
  dim: 300
  dim: 300
}

layer {
  name: "data_bn"
  type: "BatchNorm"
  bottom: "data"
  top: "data_bn"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "data_scale"
  type: "Scale"
  bottom: "data_bn"
  top: "data_bn"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1_h"
  type: "Convolution"
  bottom: "data_bn"
  top: "conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 32
    pad: 3
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "msra"
      variance_norm: FAN_OUT
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "conv1_bn_h"
  type: "BatchNorm"
  bottom: "conv1_h"
  top: "conv1_h"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "conv1_scale_h"
  type: "Scale"
  bottom: "conv1_h"
  top: "conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1_relu"
  type: "ReLU"
  bottom: "conv1_h"
  top: "conv1_h"
}
layer {
  name: "conv1_pool"
  type: "Pooling"
  bottom: "conv1_h"
  top: "conv1_pool"
  pooling_param {
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "layer_64_1_conv1_h"
  type: "Convolution"
  bottom: "conv1_pool"
  top: "layer_64_1_conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_bn2_h"
  type: "BatchNorm"
  bottom: "layer_64_1_conv1_h"
  top: "layer_64_1_conv1_h"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "layer_64_1_scale2_h"
  type: "Scale"
  bottom: "layer_64_1_conv1_h"
  top: "layer_64_1_conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_64_1_relu2"
  type: "ReLU"
  bottom: "layer_64_1_conv1_h"
  top: "layer_64_1_conv1_h"
}
layer {
  name: "layer_64_1_conv2_h"
  type: "Convolution"
  bottom: "layer_64_1_conv1_h"
  top: "layer_64_1_conv2_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_64_1_sum"
  type: "Eltwise"
  bottom: "layer_64_1_conv2_h"
  bottom: "conv1_pool"
  top: "layer_64_1_sum"
}
layer {
  name: "layer_128_1_bn1_h"
  type: "BatchNorm"
  bottom: "layer_64_1_sum"
  top: "layer_128_1_bn1_h"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "layer_128_1_scale1_h"
  type: "Scale"
  bottom: "layer_128_1_bn1_h"
  top: "layer_128_1_bn1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_1_relu1"
  type: "ReLU"
  bottom: "layer_128_1_bn1_h"
  top: "layer_128_1_bn1_h"
}
layer {
  name: "layer_128_1_conv1_h"
  type: "Convolution"
  bottom: "layer_128_1_bn1_h"
  top: "layer_128_1_conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_bn2"
  type: "BatchNorm"
  bottom: "layer_128_1_conv1_h"
  top: "layer_128_1_conv1_h"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "layer_128_1_scale2"
  type: "Scale"
  bottom: "layer_128_1_conv1_h"
  top: "layer_128_1_conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_128_1_relu2"
  type: "ReLU"
  bottom: "layer_128_1_conv1_h"
  top: "layer_128_1_conv1_h"
}
layer {
  name: "layer_128_1_conv2"
  type: "Convolution"
  bottom: "layer_128_1_conv1_h"
  top: "layer_128_1_conv2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_conv_expand_h"
  type: "Convolution"
  bottom: "layer_128_1_bn1_h"
  top: "layer_128_1_conv_expand_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_128_1_sum"
  type: "Eltwise"
  bottom: "layer_128_1_conv2"
  bottom: "layer_128_1_conv_expand_h"
  top: "layer_128_1_sum"
}
layer {
  name: "layer_256_1_bn1"
  type: "BatchNorm"
  bottom: "layer_128_1_sum"
  top: "layer_256_1_bn1"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "layer_256_1_scale1"
  type: "Scale"
  bottom: "layer_256_1_bn1"
  top: "layer_256_1_bn1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_1_relu1"
  type: "ReLU"
  bottom: "layer_256_1_bn1"
  top: "layer_256_1_bn1"
}
layer {
  name: "layer_256_1_conv1"
  type: "Convolution"
  bottom: "layer_256_1_bn1"
  top: "layer_256_1_conv1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_bn2"
  type: "BatchNorm"
  bottom: "layer_256_1_conv1"
  top: "layer_256_1_conv1"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "layer_256_1_scale2"
  type: "Scale"
  bottom: "layer_256_1_conv1"
  top: "layer_256_1_conv1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_256_1_relu2"
  type: "ReLU"
  bottom: "layer_256_1_conv1"
  top: "layer_256_1_conv1"
}
layer {
  name: "layer_256_1_conv2"
  type: "Convolution"
  bottom: "layer_256_1_conv1"
  top: "layer_256_1_conv2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_conv_expand"
  type: "Convolution"
  bottom: "layer_256_1_bn1"
  top: "layer_256_1_conv_expand"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_256_1_sum"
  type: "Eltwise"
  bottom: "layer_256_1_conv2"
  bottom: "layer_256_1_conv_expand"
  top: "layer_256_1_sum"
}
layer {
  name: "layer_512_1_bn1"
  type: "BatchNorm"
  bottom: "layer_256_1_sum"
  top: "layer_512_1_bn1"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "layer_512_1_scale1"
  type: "Scale"
  bottom: "layer_512_1_bn1"
  top: "layer_512_1_bn1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_1_relu1"
  type: "ReLU"
  bottom: "layer_512_1_bn1"
  top: "layer_512_1_bn1"
}
layer {
  name: "layer_512_1_conv1_h"
  type: "Convolution"
  bottom: "layer_512_1_bn1"
  top: "layer_512_1_conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1 # 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_bn2_h"
  type: "BatchNorm"
  bottom: "layer_512_1_conv1_h"
  top: "layer_512_1_conv1_h"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "layer_512_1_scale2_h"
  type: "Scale"
  bottom: "layer_512_1_conv1_h"
  top: "layer_512_1_conv1_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "layer_512_1_relu2"
  type: "ReLU"
  bottom: "layer_512_1_conv1_h"
  top: "layer_512_1_conv1_h"
}
layer {
  name: "layer_512_1_conv2_h"
  type: "Convolution"
  bottom: "layer_512_1_conv1_h"
  top: "layer_512_1_conv2_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 2 # 1
    kernel_size: 3
    stride: 1
    dilation: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_conv_expand_h"
  type: "Convolution"
  bottom: "layer_512_1_bn1"
  top: "layer_512_1_conv_expand_h"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1 # 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "layer_512_1_sum"
  type: "Eltwise"
  bottom: "layer_512_1_conv2_h"
  bottom: "layer_512_1_conv_expand_h"
  top: "layer_512_1_sum"
}
layer {
  name: "last_bn_h"
  type: "BatchNorm"
  bottom: "layer_512_1_sum"
  top: "layer_512_1_sum"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
}
layer {
  name: "last_scale_h"
  type: "Scale"
  bottom: "layer_512_1_sum"
  top: "layer_512_1_sum"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "last_relu"
  type: "ReLU"
  bottom: "layer_512_1_sum"
  top: "fc7"
}

layer {
  name: "conv6_1_h"
  type: "Convolution"
  bottom: "fc7"
  top: "conv6_1_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv6_1_relu"
  type: "ReLU"
  bottom: "conv6_1_h"
  top: "conv6_1_h"
}
layer {
  name: "conv6_2_h"
  type: "Convolution"
  bottom: "conv6_1_h"
  top: "conv6_2_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv6_2_relu"
  type: "ReLU"
  bottom: "conv6_2_h"
  top: "conv6_2_h"
}
layer {
  name: "conv7_1_h"
  type: "Convolution"
  bottom: "conv6_2_h"
  top: "conv7_1_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv7_1_relu"
  type: "ReLU"
  bottom: "conv7_1_h"
  top: "conv7_1_h"
}
layer {
  name: "conv7_2_h"
  type: "Convolution"
  bottom: "conv7_1_h"
  top: "conv7_2_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv7_2_relu"
  type: "ReLU"
  bottom: "conv7_2_h"
  top: "conv7_2_h"
}
layer {
  name: "conv8_1_h"
  type: "Convolution"
  bottom: "conv7_2_h"
  top: "conv8_1_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv8_1_relu"
  type: "ReLU"
  bottom: "conv8_1_h"
  top: "conv8_1_h"
}
layer {
  name: "conv8_2_h"
  type: "Convolution"
  bottom: "conv8_1_h"
  top: "conv8_2_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv8_2_relu"
  type: "ReLU"
  bottom: "conv8_2_h"
  top: "conv8_2_h"
}
layer {
  name: "conv9_1_h"
  type: "Convolution"
  bottom: "conv8_2_h"
  top: "conv9_1_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv9_1_relu"
  type: "ReLU"
  bottom: "conv9_1_h"
  top: "conv9_1_h"
}
layer {
  name: "conv9_2_h"
  type: "Convolution"
  bottom: "conv9_1_h"
  top: "conv9_2_h"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv9_2_relu"
  type: "ReLU"
  bottom: "conv9_2_h"
  top: "conv9_2_h"
}
layer {
  name: "conv4_3_norm"
  type: "Normalize"
  bottom: "layer_256_1_bn1"
  top: "conv4_3_norm"
  norm_param {
    across_spatial: false
    scale_filler {
      type: "constant"
      value: 20
    }
    channel_shared: false
  }
}
layer {
  name: "conv4_3_norm_mbox_loc"
  type: "Convolution"
  bottom: "conv4_3_norm"
  top: "conv4_3_norm_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3_norm_mbox_loc_perm"
  type: "Permute"
  bottom: "conv4_3_norm_mbox_loc"
  top: "conv4_3_norm_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv4_3_norm_mbox_loc_flat"
  type: "Flatten"
  bottom: "conv4_3_norm_mbox_loc_perm"
  top: "conv4_3_norm_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv4_3_norm_mbox_conf"
  type: "Convolution"
  bottom: "conv4_3_norm"
  top: "conv4_3_norm_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8 # 84
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3_norm_mbox_conf_perm"
  type: "Permute"
  bottom: "conv4_3_norm_mbox_conf"
  top: "conv4_3_norm_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv4_3_norm_mbox_conf_flat"
  type: "Flatten"
  bottom: "conv4_3_norm_mbox_conf_perm"
  top: "conv4_3_norm_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv4_3_norm_mbox_priorbox"
  type: "PriorBox"
  bottom: "conv4_3_norm"
  bottom: "data"
  top: "conv4_3_norm_mbox_priorbox"
  prior_box_param {
    min_size: 30.0
    max_size: 60.0
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    step: 8
    offset: 0.5
  }
}
layer {
  name: "fc7_mbox_loc"
  type: "Convolution"
  bottom: "fc7"
  top: "fc7_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc7_mbox_loc_perm"
  type: "Permute"
  bottom: "fc7_mbox_loc"
  top: "fc7_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "fc7_mbox_loc_flat"
  type: "Flatten"
  bottom: "fc7_mbox_loc_perm"
  top: "fc7_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "fc7_mbox_conf"
  type: "Convolution"
  bottom: "fc7"
  top: "fc7_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 12 # 126
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc7_mbox_conf_perm"
  type: "Permute"
  bottom: "fc7_mbox_conf"
  top: "fc7_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "fc7_mbox_conf_flat"
  type: "Flatten"
  bottom: "fc7_mbox_conf_perm"
  top: "fc7_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "fc7_mbox_priorbox"
  type: "PriorBox"
  bottom: "fc7"
  bottom: "data"
  top: "fc7_mbox_priorbox"
  prior_box_param {
    min_size: 60.0
    max_size: 111.0
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    step: 16
    offset: 0.5
  }
}
layer {
  name: "conv6_2_mbox_loc"
  type: "Convolution"
  bottom: "conv6_2_h"
  top: "conv6_2_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv6_2_mbox_loc_perm"
  type: "Permute"
  bottom: "conv6_2_mbox_loc"
  top: "conv6_2_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv6_2_mbox_loc_flat"
  type: "Flatten"
  bottom: "conv6_2_mbox_loc_perm"
  top: "conv6_2_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv6_2_mbox_conf"
  type: "Convolution"
  bottom: "conv6_2_h"
  top: "conv6_2_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 12 # 126
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv6_2_mbox_conf_perm"
  type: "Permute"
  bottom: "conv6_2_mbox_conf"
  top: "conv6_2_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv6_2_mbox_conf_flat"
  type: "Flatten"
  bottom: "conv6_2_mbox_conf_perm"
  top: "conv6_2_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv6_2_mbox_priorbox"
  type: "PriorBox"
  bottom: "conv6_2_h"
  bottom: "data"
  top: "conv6_2_mbox_priorbox"
  prior_box_param {
    min_size: 111.0
    max_size: 162.0
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    step: 32
    offset: 0.5
  }
}
layer {
  name: "conv7_2_mbox_loc"
  type: "Convolution"
  bottom: "conv7_2_h"
  top: "conv7_2_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 24
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv7_2_mbox_loc_perm"
  type: "Permute"
  bottom: "conv7_2_mbox_loc"
  top: "conv7_2_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv7_2_mbox_loc_flat"
  type: "Flatten"
  bottom: "conv7_2_mbox_loc_perm"
  top: "conv7_2_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv7_2_mbox_conf"
  type: "Convolution"
  bottom: "conv7_2_h"
  top: "conv7_2_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 12 # 126
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv7_2_mbox_conf_perm"
  type: "Permute"
  bottom: "conv7_2_mbox_conf"
  top: "conv7_2_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv7_2_mbox_conf_flat"
  type: "Flatten"
  bottom: "conv7_2_mbox_conf_perm"
  top: "conv7_2_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv7_2_mbox_priorbox"
  type: "PriorBox"
  bottom: "conv7_2_h"
  bottom: "data"
  top: "conv7_2_mbox_priorbox"
  prior_box_param {
    min_size: 162.0
    max_size: 213.0
    aspect_ratio: 2
    aspect_ratio: 3
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    step: 64
    offset: 0.5
  }
}
layer {
  name: "conv8_2_mbox_loc"
  type: "Convolution"
  bottom: "conv8_2_h"
  top: "conv8_2_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv8_2_mbox_loc_perm"
  type: "Permute"
  bottom: "conv8_2_mbox_loc"
  top: "conv8_2_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv8_2_mbox_loc_flat"
  type: "Flatten"
  bottom: "conv8_2_mbox_loc_perm"
  top: "conv8_2_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv8_2_mbox_conf"
  type: "Convolution"
  bottom: "conv8_2_h"
  top: "conv8_2_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8 # 84
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv8_2_mbox_conf_perm"
  type: "Permute"
  bottom: "conv8_2_mbox_conf"
  top: "conv8_2_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv8_2_mbox_conf_flat"
  type: "Flatten"
  bottom: "conv8_2_mbox_conf_perm"
  top: "conv8_2_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv8_2_mbox_priorbox"
  type: "PriorBox"
  bottom: "conv8_2_h"
  bottom: "data"
  top: "conv8_2_mbox_priorbox"
  prior_box_param {
    min_size: 213.0
    max_size: 264.0
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    step: 100
    offset: 0.5
  }
}
layer {
  name: "conv9_2_mbox_loc"
  type: "Convolution"
  bottom: "conv9_2_h"
  top: "conv9_2_mbox_loc"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv9_2_mbox_loc_perm"
  type: "Permute"
  bottom: "conv9_2_mbox_loc"
  top: "conv9_2_mbox_loc_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv9_2_mbox_loc_flat"
  type: "Flatten"
  bottom: "conv9_2_mbox_loc_perm"
  top: "conv9_2_mbox_loc_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv9_2_mbox_conf"
  type: "Convolution"
  bottom: "conv9_2_h"
  top: "conv9_2_mbox_conf"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8 # 84
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv9_2_mbox_conf_perm"
  type: "Permute"
  bottom: "conv9_2_mbox_conf"
  top: "conv9_2_mbox_conf_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "conv9_2_mbox_conf_flat"
  type: "Flatten"
  bottom: "conv9_2_mbox_conf_perm"
  top: "conv9_2_mbox_conf_flat"
  flatten_param {
    axis: 1
  }
}
layer {
  name: "conv9_2_mbox_priorbox"
  type: "PriorBox"
  bottom: "conv9_2_h"
  bottom: "data"
  top: "conv9_2_mbox_priorbox"
  prior_box_param {
    min_size: 264.0
    max_size: 315.0
    aspect_ratio: 2
    flip: true
    clip: false
    variance: 0.1
    variance: 0.1
    variance: 0.2
    variance: 0.2
    step: 300
    offset: 0.5
  }
}
layer {
  name: "mbox_loc"
  type: "Concat"
  bottom: "conv4_3_norm_mbox_loc_flat"
  bottom: "fc7_mbox_loc_flat"
  bottom: "conv6_2_mbox_loc_flat"
  bottom: "conv7_2_mbox_loc_flat"
  bottom: "conv8_2_mbox_loc_flat"
  bottom: "conv9_2_mbox_loc_flat"
  top: "mbox_loc"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_conf"
  type: "Concat"
  bottom: "conv4_3_norm_mbox_conf_flat"
  bottom: "fc7_mbox_conf_flat"
  bottom: "conv6_2_mbox_conf_flat"
  bottom: "conv7_2_mbox_conf_flat"
  bottom: "conv8_2_mbox_conf_flat"
  bottom: "conv9_2_mbox_conf_flat"
  top: "mbox_conf"
  concat_param {
    axis: 1
  }
}
layer {
  name: "mbox_priorbox"
  type: "Concat"
  bottom: "conv4_3_norm_mbox_priorbox"
  bottom: "fc7_mbox_priorbox"
  bottom: "conv6_2_mbox_priorbox"
  bottom: "conv7_2_mbox_priorbox"
  bottom: "conv8_2_mbox_priorbox"
  bottom: "conv9_2_mbox_priorbox"
  top: "mbox_priorbox"
  concat_param {
    axis: 2
  }
}

layer {
  name: "mbox_conf_reshape"
  type: "Reshape"
  bottom: "mbox_conf"
  top: "mbox_conf_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: -1
      dim: 2
    }
  }
}
layer {
  name: "mbox_conf_softmax"
  type: "Softmax"
  bottom: "mbox_conf_reshape"
  top: "mbox_conf_softmax"
  softmax_param {
    axis: 2
  }
}
layer {
  name: "mbox_conf_flatten"
  type: "Flatten"
  bottom: "mbox_conf_softmax"
  top: "mbox_conf_flatten"
  flatten_param {
    axis: 1
  }
}

layer {
  name: "detection_out"
  type: "DetectionOutput"
  bottom: "mbox_loc"
  bottom: "mbox_conf_flatten"
  bottom: "mbox_priorbox"
  top: "detection_out"
  include {
    phase: TEST
  }
  detection_output_param {
    num_classes: 2
    share_location: true
    background_label_id: 0
    nms_param {
      nms_threshold: 0.45
      top_k: 400
    }
    code_type: CENTER_SIZE
    keep_top_k: 200
    confidence_threshold: 0.01
    clip: 1
  }
}
</file>

<file path=".env.example">
GOOGLE_API_KEY=your_google_api_key_here
</file>

<file path=".gitattributes">
# Auto detect text files and perform LF normalization
* text=auto
</file>

<file path=".gitignore">
# Python-related
*.pyc
**/__pycache__/

# Media Files
*.wav
*.mp3
*.gif
*.mp4
*.webm
*.db
/videos

# Virtual environment
.venv/
.vscode/
env/

# Environment variables
.env
</file>

<file path="clean_url.py">
# clean_url.py
import sqlite3
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
import os

db_path = "video_processing.db"
target_id = 1

if not os.path.exists(db_path):
    print(f"Error: Database file '{db_path}' not found.")
    exit()

try:
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Fetch the current URL
    cursor.execute("SELECT youtube_url FROM videos WHERE id = ?", (target_id,))
    result = cursor.fetchone()

    if result:
        original_url = result[0]
        print(f"Original URL for ID {target_id}: {original_url}")

        if original_url:
            # Simple string manipulation for this specific case
            cleaned_url = original_url
            if '&t' in original_url:
                # Find the position of &t
                t_pos = original_url.find('&t')
                # Check if it's followed by '=' or just the end of the string
                if t_pos != -1:
                    # Check if it's the last parameter or followed by another &
                    next_amp_pos = original_url.find('&', t_pos + 1)
                    if next_amp_pos == -1:
                        # &t is the last part, remove it
                        cleaned_url = original_url[:t_pos]
                        print("Removed trailing '&t' parameter.")
                    else:
                        # &t=... is in the middle, need proper parsing (or more complex string logic)
                        # Fall back to original parsing method just in case
                        print("'&t' found, but potentially with a value or followed by other params. Using URL parsing...")
                        parsed_url = urlparse(original_url)
                        query_params = parse_qs(parsed_url.query)
                        if 't' in query_params:
                            del query_params['t']
                            new_query_string = urlencode(query_params, doseq=True)
                            cleaned_url_parts = parsed_url._replace(query=new_query_string)
                            cleaned_url = urlunparse(cleaned_url_parts)
                            print("Removed 't' parameter using URL parsing.")
                        else:
                            # If parsing didn't find t=, maybe it was just &t. Try removing the segment again.
                            cleaned_url = original_url[:t_pos] + original_url[next_amp_pos:]
                            print("Removed '&t' segment between other params.")
                else: # &t not found by find()
                     print("'&t' not found in the URL string. No update needed.")       

            if cleaned_url != original_url:
                print(f"Cleaned URL: {cleaned_url}")
                cursor.execute("UPDATE videos SET youtube_url = ? WHERE id = ?", (cleaned_url, target_id))
                conn.commit()
                print(f"Database updated successfully for ID {target_id}.")
            else:
                print("URL was not changed. No update needed.")
        else:
             print("URL is empty or null in the database. No update needed.")

    else:
        print(f"Error: No entry found for ID {target_id}.")

except sqlite3.Error as e:
    print(f"SQLite error: {e}")
except Exception as e:
    print(f"An unexpected error occurred: {e}")
finally:
    if conn:
        conn.close()
</file>

<file path="clear_highlights.py">
# clear_highlights.py
import sqlite3
import os

db_path = "video_processing.db"

if not os.path.exists(db_path):
    print(f"Error: Database file '{db_path}' not found.")
    exit()

try:
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Confirm with the user (optional but recommended for delete operations)
    confirm = input(f"Are you sure you want to delete ALL entries from the 'highlights' table in {db_path}? (yes/no): ")
    if confirm.lower() != 'yes':
        print("Operation cancelled.")
        exit()

    # Delete all rows from the highlights table
    cursor.execute("DELETE FROM highlights")
    deleted_rows = cursor.rowcount # Get the number of deleted rows
    conn.commit()

    print(f"Successfully deleted {deleted_rows} rows from the 'highlights' table.")

except sqlite3.Error as e:
    print(f"SQLite error: {e}")
    if conn:
        conn.rollback() # Rollback changes on error
except Exception as e:
    print(f"An unexpected error occurred: {e}")
finally:
    if conn:
        conn.close()
        print("Database connection closed.")
</file>

<file path="config.yaml">
# --- Paths & processing hints (comments only, safe to keep as-is) ---
# Все пути теперь резолвятся относительно paths.base_dir.
# Для Colab/контейнеров можно указать base_dir как рабочую директорию окружения;
# fonts_dir и прочие каталоги будут резолвиться относительно base_dir.
#
# Примерные значения по умолчанию (РАЗКОММЕНТИРУЙТЕ при необходимости переопределения):
# paths:
#   base_dir: .
#   fonts_dir: fonts
# processing:
#   transcriptions_dir: "transcriptions"  # экспорт .txt/.json/.srt/.vtt
#   shorts_dir: "shorts"                   # каталог итоговых шортов
processing:
  use_animated_captions: true
  shorts_dir: "shorts"
  videos_dir: "videos"
  transcriptions_dir: "transcriptions"
  crop_bottom_percent: 0.0
  min_video_dimension_px: 100
  log_transcription_preview_len: 200
  crop_mode: "70_percent_blur"  # "average_face" or "70_percent_blur"

llm:
  model_name: "gemini-2.5-flash"
  temperature_highlights: 0.2
  temperature_metadata: 1.0
  max_attempts_highlights: 3
  max_attempts_metadata: 3
  highlight_min_sec: 29
  highlight_max_sec: 61
  max_highlights: 20

# Настройки режима "фильм"
film_mode:
  enabled: true
  generate_shorts: true  # ВКЛЮЧЕНА генерация шортов
  combo_duration: [10, 20]  # секунды для COMBO моментов
  single_duration: [30, 60]  # секунды для SINGLE моментов
  max_moments: 30  # максимальное количество моментов для анализа (монолитный режим)
  pause_threshold: 0.7  # порог для определения длинных пауз (секунды)
  filler_words: ["э-э", "м-м", "ну", "эээ", "гм", "кхм"]  # слова-заполнители для обрезки

  # Интеллектуальный анализ пауз
  intelligent_pause_analysis:
    enabled: true  # Включить ИИ-анализ пауз
    model: "gemini-2.5-flash-lite"  # Модель для простых задач анализа пауз
    temperature: 0.1  # Температура для анализа пауз (низкая для консистентности)
    max_attempts: 2  # Максимум попыток для анализа пауз
    auto_trim_confidence_threshold: 0.8  # Порог уверенности для автоматической обрезки
    batch_size: 10  # Размер батча для пакетного анализа пауз
    cache_enabled: true  # Кеширование результатов анализа пауз
    cache_ttl_hours: 24  # Время жизни кеша в часах

    # Категории пауз для классификации
    pause_categories:
      structural: ["sentence_end", "paragraph_break", "topic_change"]  # Структурные паузы (не обрезать)
      filler: ["um", "uh", "er", "ah", "like", "you_know"]  # Заполнители (обрезать)
      emphasis: ["dramatic_pause", "for_effect"]  # Паузы для эффекта (анализировать контекст)
      breathing: ["breath", "inhale", "exhale"]  # Дыхательные паузы (обрезать при длинных)

    # Весовые коэффициенты для определения важности пауз
    importance_weights:
      duration: -0.4  # Чем длиннее пауза, тем менее важна (отрицательный вес)
      position: 0.3   # Позиция в предложении (начало/середина/конец)
      context: 0.4    # Контекст вокруг паузы
      audio_features: 0.2  # Аудио-характеристики (тишина vs шум)
      linguistic: 0.3  # Лингвистический анализ

    # Оптимизация API
    api_optimization:
      use_batch_processing: true  # Использовать пакетную обработку
      max_concurrent_requests: 3  # Максимум одновременных запросов
      rate_limit_delay: 1.0  # Задержка между запросами (секунды)
      retry_on_failure: true  # Повторять при неудачах
      fallback_to_legacy: true  # Откат на старую логику при ошибках ИИ

  # Film Mode v2: Оконный LLM-конвейер для длинных фильмов
  window_minutes: 12  # размер окна в минутах
  window_overlap_minutes: 3  # перекрытие окон в минутах
  max_moments_per_window: 12  # максимум моментов на окно

  # Цели генерации и лимиты
  target_shorts_count: 30  # целевое количество финальных шортов
  generator_top_k: 30  # сколько топ моментов брать для генерации

  # Дедупликация и диверсификация
  dedupe_iou_threshold: 0.5  # порог IoU для дедупликации по времени
  diversity_bucket_minutes: 5  # размер бакетов для диверсификации по таймлайну
  min_combo_segments: 2  # минимум суб-сегментов в COMBO
  max_combo_segments: 4  # максимум суб-сегментов в COMBO

  # Эталонные ключевые слова для подсчета совпадений
  reference_keywords:
    emotional_peaks: ["эмоции", "чувства", "переживания", "радость", "гнев", "страх", "удивление", "грусть", "любовь", "ненависть", "восторг", "отчаяние", "надежда", "разочарование", "триумф", "поражение"]
    conflict_escalation: ["конфликт", "спор", "ссора", "драка", "ругань", "оскорбление", "критика", "давление", "напряжение", "эскалация", "столкновение", "противостояние", "борьба", "конкуренция"]
    punchlines_wit: ["юмор", "шутка", "сарказм", "ирония", "остроумие", "панчлайн", "каламбур", "смех", "комедия", "прикол", "насмешка", "сатира", "пародия", "абсурд"]
    quotability_memes: ["цитата", "афоризм", "крылатая фраза", "мем", "вирусный", "тренд", "хайп", "легендарный", "знаменитый", "классика", "запоминающийся", "уникальный"]
    stakes_goals: ["ставки", "цель", "риск", "опасность", "выбор", "решение", "судьба", "жизнь", "смерть", "выигрыш", "проигрыш", "успех", "провал", "достижение", "амбиции"]
    hooks_cliffhangers: ["вопрос", "загадка", "тайна", "сюрприз", "интрига", "недосказанность", "продолжение", "развязка", "поворот", "неожиданно", "вдруг", "что дальше", "как же так"]

  # Весовые коэффициенты для ранжирования по совпадениям ключевых слов
  ranking_weights:
    emotional_peaks: 0.20      # Совпадения с эмоциональными ключевыми словами
    conflict_escalation: 0.18  # Совпадения с ключевыми словами конфликта
    punchlines_wit: 0.16       # Совпадения с ключевыми словами юмора
    quotability_memes: 0.14    # Совпадения с ключевыми словами цитатности
    stakes_goals: 0.12         # Совпадения с ключевыми словами ставок
    hooks_cliffhangers: 0.10   # Совпадения с ключевыми словами крючков
    visual_penalty: -0.10      # Штраф за визуальную зависимость
    pace_score: 0.08           # Плотность речи (слова/сек)
    silence_penalty: -0.08     # Наказание за долю длинных пауз
    diversity_bonus: 0.05      # Бонус за диверсификацию

  # Настройки ранжирования и fallback
  ranking:
    min_quality_threshold: 0.3
    soft_min_quality: 0.2
    allow_fallback: true
    fallback_top_n: 15
    max_best_moments: 30

  # Настройки LLM для анализа фильма
  llm:
    model: "gemini-2.5-flash"
    temperature: 0.3
    max_attempts: 3

  # Интеллектуальный анализ пауз
  intelligent_pause_analysis:
    enabled: true
    # Модель для простых задач анализа пауз
    lite_model: "gemini-2.5-flash-lite"
    # Максимальное количество пауз для анализа в одном батче
    max_pauses_per_batch: 50
    # Минимальная длительность паузы для анализа (сек)
    min_pause_duration: 0.3
    # Максимальная длительность паузы для анализа (сек)
    max_pause_duration: 5.0
    # Порог уверенности для автоматической обрезки паузы
    auto_trim_confidence_threshold: 0.8
    # Кэширование результатов анализа пауз
    enable_caching: true
    cache_ttl_hours: 24
    # Категории пауз для классификации
    pause_categories:
      structural: ["между мыслями", "переход", "завершение темы", "начало новой идеи"]
      filler: ["заполнитель", "пауза размышления", "поиск слов", "эмоциональная пауза"]
      important: ["для эффекта", "драматическая", "эмфатическая", "риторическая"]
    # Весовые коэффициенты для ранжирования пауз
    pause_ranking_weights:
      structural: 0.3
      filler: 0.8
      important: 0.1
      duration_factor: 0.2
      context_relevance: 0.4

logging:
  # Основные настройки логирования
  log_dir: "logs"
  log_level: "INFO"
  enable_console_logging: true
  enable_file_logging: true

  # Настройки ротации логов
  log_rotation_max_bytes: 10485760  # 10MB
  log_rotation_backup_count: 5
  log_rotation_when: "midnight"  # daily rotation at midnight
  log_compression: false

  # Отдельные логгеры для разных типов сообщений
  enable_main_logger: true
  enable_performance_logger: true
  enable_error_logger: true
  enable_debug_logger: false

  # Настройки производительности
  enable_performance_monitoring: true
  performance_log_max_bytes: 5242880  # 5MB
  performance_log_backup_count: 3
  performance_monitoring_interval: 0.5  # seconds

  # Настройки GPU мониторинга
  enable_gpu_monitoring: true
  gpu_priority_mode: true
  gpu_memory_threshold: 0.9  # 90% memory usage warning
  gpu_temperature_threshold: 80  # Celsius

  # Настройки CPU и памяти
  enable_cpu_monitoring: true
  enable_memory_monitoring: true
  memory_threshold: 0.85  # 85% memory usage warning
  cpu_threshold: 90.0  # 90% CPU usage warning

  # Настройки прогресс-баров
  enable_progress_bars: true
  progress_bar_update_interval: 0.1  # seconds

  # Системная информация
  enable_system_info_logging: true
  system_info_log_interval: 3600  # 1 hour in seconds

  # Асинхронная обработка
  enable_async_logging: true
  log_queue_size: 1000
  log_worker_threads: 2

  # Форматирование логов
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_date_format: "%Y-%m-%d %H:%M:%S"
  enable_colors: true

  # Фильтры логирования
  log_filters:
    - "urllib3.connectionpool"  # Filter noisy HTTP logs
    - "PIL.PngImagePlugin"      # Filter PIL debug logs

  # Настройки для разных сред
  development_mode: false
  enable_detailed_tracing: false
  enable_function_call_logging: false

  # Ресурсный мониторинг
  resource_monitoring_interval: 1.0  # seconds
  enable_resource_alerts: true
  alert_threshold_duration: 30.0  # seconds for long operations
paths:
  base_dir: .
  fonts_dir: fonts
captions:
  font_size_px: 38
  letter_spacing_px: 1.5
  line_height: 1.3
  base_color: "#FFFFFF"
  strip_punctuation: true
  align_to_audio: true
  fade_in_seconds: 0.15
  fade_out_seconds: 0.12
  shadow:
    x_px: 2
    y_px: 2
    blur_px: 1
    color: "#00000080"
  accent_palette:
    urgency: "#FFD400"
    drama: "#FF3B30"
    positive: "#34C759"
  animate:
    type: "slide-up"         # возможные: "pop-in" | "slide-up"
    duration_s: 0.35         # диапазон 0.2–0.5
    easing: "easeOutCubic"
    per_word_stagger_ms: 120
  position:
    mode: "safe_bottom"      # возможные: "safe_bottom" | "center"
    bottom_offset_pct: 22    # 20–28% рекомендовано, по умолчанию 22
    center_offset_pct: 12
    boundary_padding_px: 10
  emoji:
    enabled: false
    max_per_short: 2
    style: "shiny"           # возможные: "shiny" | "pulse" | "none"
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2024 SamurAIGPT

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="main.py">
from Components.YoutubeDownloader import download_youtube_video
from Components.Edit import extractAudio, crop_video, burn_captions, crop_bottom_video, animate_captions, get_video_dimensions
from Components.Transcription import transcribe_unified, prepare_words_for_segment
from faster_whisper import WhisperModel
import torch
import json
from Components.LanguageTasks import GetHighlights, build_transcription_prompt, compute_tone_and_keywords, compute_emojis_for_segment
from Components.FaceCrop import crop_to_70_percent_with_blur, crop_to_vertical_average_face
from Components.Database import VideoDatabase
from dataclasses import dataclass, field
from typing import Optional, List
import os
import traceback
import time
import math
from Components.config import get_config, AppConfig
from Components.Logger import logger, timed_operation
from Components.Paths import build_short_output_name

# Load config with reload to ensure latest changes are applied
from Components.config import reload_config
cfg = reload_config()
print(f"Конфиг загружен: shorts_dir={cfg.processing.shorts_dir}, model={cfg.llm.model_name}, crop_mode={cfg.processing.crop_mode}")

# Инициализация системы логирования
if cfg.logging.enable_system_info_logging:
    logger.log_system_info()

# --- Configuration Flags ---
# Set to True to use two words-level animated captions (slower but nicer)
# Set to False to use the default, faster ASS subtitle burning
USE_ANIMATED_CAPTIONS = cfg.processing.use_animated_captions

# Define the output directory for final shorts
SHORTS_DIR = cfg.processing.shorts_dir

# Define the crop percentage for the bottom of the video (useful when there are integrated captions in the original video)
CROP_PERCENTAGE_BOTTOM = cfg.processing.crop_bottom_percent

# --- Transcriptions JSON helpers (non-blocking) ---
def build_transcriptions_dir():
    from pathlib import Path
    return Path(__file__).resolve().parent / "transcriptions"


def ensure_dir(path_like):
    from pathlib import Path
    p = Path(path_like)
    try:
        p.parent.mkdir(parents=True, exist_ok=True)
    except Exception:
        try:
            p.mkdir(parents=True, exist_ok=True)
        except Exception:
            pass


def sanitize_base_name(name: str) -> str:
    import re
    from pathlib import Path
    try:
        base = Path(str(name)).stem
    except Exception:
        base = str(name)
    base = base.replace(" ", "_").lower()
    base = re.sub(r"[^A-Za-z0-9_-]", "", base)
    return base


def save_json_safely(data, path):
    import json
    from pathlib import Path
    p = Path(path)
    try:
        ensure_dir(p)
        with p.open("w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        try:
            resolved = p.resolve()
        except Exception:
            resolved = str(p)
        print(f"[WARN] Ошибка при сохранении файла: {resolved} – {e}")
        return False


def _to_float(val, default=None):
    try:
        return float(val)
    except Exception:
        return default


def to_full_segments_payload(segments):
    result = []
    for seg in (segments or []):
        start = 0.0
        end = 0.0
        text = ""
        speaker = None
        confidence = None
        if isinstance(seg, dict):
            start = _to_float(seg.get("start"), 0.0)
            end = _to_float(seg.get("end"), 0.0)
            text = str(seg.get("text", ""))
            speaker_val = seg.get("speaker", None)
            speaker = str(speaker_val) if speaker_val is not None else None
            conf_val = seg.get("confidence", seg.get("prob", seg.get("probability", seg.get("score", None))))
            confidence = _to_float(conf_val, None) if conf_val is not None else None
        elif isinstance(seg, (list, tuple)) and len(seg) >= 3:
            text = str(seg[0]) if seg[0] is not None else ""
            start = _to_float(seg[1], 0.0)
            end = _to_float(seg[2], 0.0)
            speaker = None
            confidence = None
        else:
            start = _to_float(getattr(seg, "start", 0.0), 0.0)
            end = _to_float(getattr(seg, "end", 0.0), 0.0)
            t = getattr(seg, "text", "")
            text = str(t) if t is not None else ""
            sp = getattr(seg, "speaker", None)
            speaker = str(sp) if sp is not None else None
            conf_val = getattr(seg, "confidence", None)
            if conf_val is None:
                for k in ("prob", "probability", "score"):
                    if hasattr(seg, k):
                        conf_val = getattr(seg, k)
                        break
            confidence = _to_float(conf_val, None) if conf_val is not None else None
        result.append({"start": start, "end": end, "text": text, "speaker": speaker, "confidence": confidence})
    return {"segments": result}


def to_words_payload(word_level_result):
    words = []
    segments = []
    if word_level_result is None:
        return {"words": words}
    if isinstance(word_level_result, dict):
        segments = word_level_result.get("segments", [])
        if not segments and "words" in word_level_result:
            segments = [{"words": word_level_result.get("words", [])}]
    else:
        segments = getattr(word_level_result, "segments", []) or []
    for seg in segments:
        seg_words = seg.get("words", []) if isinstance(seg, dict) else getattr(seg, "words", []) or []
        # Wait: indentation alignment; ensure 8 spaces? We'll keep 8 spaces. We'll correct.
        for w in seg_words:
            if isinstance(w, dict):
                start = _to_float(w.get("start", 0.0), 0.0)
                end = _to_float(w.get("end", 0.0), 0.0)
                word_val = w.get("word", w.get("text", ""))
                conf_val = w.get("confidence", w.get("prob", w.get("probability", w.get("score", None))))
            else:
                start = _to_float(getattr(w, "start", 0.0), 0.0)
                end = _to_float(getattr(w, "end", 0.0), 0.0)
                word_val = getattr(w, "word", getattr(w, "text", ""))
                conf_val = getattr(w, "confidence", None)
                if conf_val is None:
                    for k in ("prob", "probability", "score"):
                        if hasattr(w, k):
                            conf_val = getattr(w, k)
                            break
            words.append({
                "start": start,
                "end": end,
                "word": str(word_val) if word_val is not None else "",
                "confidence": _to_float(conf_val, None) if conf_val is not None else None
            })
    return {"words": words}

@dataclass
class ProcessingContext:
    """
    Контекст обработки видео для пайплайна.

    Ключевые поля:
    - cfg: AppConfig
    - db: VideoDatabase
    - url/local_path: входной источник
    - video_path: путь к исходному видео
    - video_id: идентификатор в БД
    - audio_path: путь к извлеченному аудио
    - base_name: базовое имя файла видео
    - initial_width/initial_height: исходные размеры видео
    - transcription_segments: список сегментов транскрипции (dict: start, end, text, speaker?)
    - transcription_text: форматированный текст транскрипции для LLM
    - outputs: список путей к финальным клипам
    """
    cfg: AppConfig
    db: VideoDatabase
    url: Optional[str] = None
    local_path: Optional[str] = None
    video_path: Optional[str] = None
    video_id: Optional[str] = None
    audio_path: Optional[str] = None
    base_name: Optional[str] = None
    initial_width: int = 0
    initial_height: int = 0
    transcription_segments: List[dict] = field(default_factory=list)
    transcription_text: Optional[str] = None
    outputs: List[str] = field(default_factory=list)
    word_level_transcription: Optional[dict] = None


def init_context(url: Optional[str], local_path: Optional[str]) -> ProcessingContext:
    """Инициализирует контекст: загружает конфиг, создаёт БД и гарантирует наличие выходных директорий."""
    cfg_local = get_config()
    # Ensure directories exist
    os.makedirs(cfg_local.processing.shorts_dir, exist_ok=True)
    os.makedirs(cfg_local.processing.videos_dir, exist_ok=True)
    ctx = ProcessingContext(cfg=cfg_local, db=VideoDatabase(), url=url, local_path=local_path)
    return ctx


@timed_operation("resolve_video_source")
def resolve_video_source(ctx: ProcessingContext) -> bool:
    """Определяет источник видео (URL/локальный), учитывает кэш БД, выставляет ctx.video_path и ctx.video_id."""
    if not ctx.url and not ctx.local_path:
        logger.logger.error("Error: Must provide either URL or local path")
        return False

    video_path = None
    video_id = None

    if ctx.url:
        logger.logger.info(f"Processing YouTube URL: {ctx.url}")
        cached_data = ctx.db.get_cached_processing(youtube_url=ctx.url)
        if cached_data:
            logger.logger.info("Found cached video from URL!")
            video_path = cached_data["video"][2]
            video_id = cached_data["video"][0]
            if not os.path.exists(video_path):
                logger.logger.warning(f"Cached video path not found: {video_path}. Re-downloading.")
                video_path = None
                video_id = None
        if not video_path:
            with logger.operation_context("download_youtube_video", {"url": ctx.url}):
                video_path = download_youtube_video(ctx.url)
                if not video_path:
                    logger.logger.error("Failed to download video")
                    return False
                if not video_path.lower().endswith('.mp4'):
                    base, _ = os.path.splitext(video_path)
                    new_path = base + ".mp4"
                    try:
                        os.rename(video_path, new_path)
                        video_path = new_path
                        logger.logger.info(f"Renamed downloaded file to: {video_path}")
                    except OSError as e:
                        logger.logger.warning(f"Error renaming file to mp4: {e}. Trying conversion.")
                        pass
    else:
        logger.logger.info(f"Processing local file: {ctx.local_path}")
        if not os.path.exists(ctx.local_path):
            logger.logger.error("Error: Local file does not exist")
            return False
        video_path = ctx.local_path
        cached_data = ctx.db.get_cached_processing(local_path=ctx.local_path)
        if cached_data:
            logger.logger.info("Found cached local video!")
            video_id = cached_data["video"][0]

    if not video_path or not os.path.exists(video_path):
        logger.logger.error("No valid video path obtained or file does not exist.")
        return False

    ctx.video_path = video_path
    ctx.video_id = video_id
    ctx.base_name = os.path.splitext(os.path.basename(video_path))[0]
    return True


def validate_dimensions(ctx: ProcessingContext) -> bool:
    """Проверяет исходные размеры видео и сохраняет их в контекст."""
    print("\n--- Checking Initial Video Dimensions ---")
    w, h = get_video_dimensions(ctx.video_path)
    if w is None or h is None:
        print("Error: Could not determine initial video dimensions. Aborting.")
        return False
    ctx.initial_width, ctx.initial_height = int(w), int(h)
    if ctx.initial_width < ctx.cfg.processing.min_video_dimension_px or ctx.initial_height < ctx.cfg.processing.min_video_dimension_px:
        print(f"Warning: Initial video dimensions ({ctx.initial_width}x{ctx.initial_height}) seem very small.")
    print("--- Initial Check Done ---")
    return True


def ensure_audio(ctx: ProcessingContext) -> bool:
    """Извлекает или загружает из кэша аудио. Обновляет БД и ctx.audio_path/ctx.video_id."""
    audio_path = None
    cached_data = None
    if ctx.url:
        cached_data = ctx.db.get_cached_processing(youtube_url=ctx.url)
    else:
        cached_data = ctx.db.get_cached_processing(local_path=ctx.local_path or ctx.video_path)

    if cached_data and cached_data["video"][3]:
        print("Using cached audio file reference")
        audio_path = cached_data["video"][3]
        if not os.path.exists(audio_path):
            print("Cached audio file not found, extracting again")
            audio_path = None

    if not audio_path:
        print(f"Extracting audio from video: {ctx.video_path}")
        audio_path = extractAudio(ctx.video_path)
        if not audio_path:
            print("Failed to extract audio")
            return False
        if ctx.video_id:
            ctx.db.update_video_audio_path(ctx.video_id, audio_path)
        else:
            ctx.video_id = ctx.db.add_video(ctx.url, ctx.video_path, audio_path)

    if not ctx.video_id:
        video_entry = ctx.db.get_video(youtube_url=ctx.url, local_path=ctx.video_path)
        if video_entry:
            ctx.video_id = video_entry[0]
        else:
            print("Error: Video ID could not be determined after processing.")
            return False

    ctx.audio_path = audio_path
    return True


def run_unified_transcription(ctx: ProcessingContext, model: WhisperModel) -> bool:
    """
    Выполняет или загружает из кэша транскрипцию (сегменты и слова).
    Использует JSON-файлы как основной кэш для обоих типов данных.
    Сохраняет результаты в контекст.
    """
    base_name = sanitize_base_name(os.path.splitext(os.path.basename(ctx.video_path))[0])
    segments_cache_path = build_transcriptions_dir() / f"{base_name}_full_segments.json"
    words_cache_path = build_transcriptions_dir() / f"{base_name}_word_level.json"

    segments_loaded = False
    words_loaded = False

    # Попытка загрузить из JSON кэша
    if segments_cache_path.exists():
        try:
            with segments_cache_path.open("r", encoding="utf-8") as f:
                data = json.load(f)
                ctx.transcription_segments = data.get("segments", [])
                segments_loaded = True
                print("Loaded segment transcription from JSON cache.")
        except Exception as e:
            print(f"[WARN] Could not load segments from JSON cache: {e}")

    if words_cache_path.exists():
        try:
            with words_cache_path.open("r", encoding="utf-8") as f:
                ctx.word_level_transcription = json.load(f)
                words_loaded = True
                print("Loaded word-level transcription from JSON cache.")
        except Exception as e:
            print(f"[WARN] Could not load words from JSON cache: {e}")

    if segments_loaded and words_loaded:
        print("Both transcriptions loaded from cache. Skipping transcription.")
        # Убедимся, что в контексте не None, а пустой dict, если что-то пошло не так
        if ctx.word_level_transcription is None:
            ctx.word_level_transcription = {"segments": []}
        return True

    # Если чего-то нет, запускаем унифицированную транскрипцию
    print("Cache incomplete. Running unified transcription for segments and words...")
    
    # Используется унифицированный подход для повышения эффективности
    segments_legacy, word_level_transcription = transcribe_unified(ctx.audio_path, model)

    if not segments_legacy:
        print("Unified transcription failed. Cannot proceed.")
        return False

    # Преобразуем и сохраняем результаты в контекст
    full_segments_payload = to_full_segments_payload(segments_legacy)
    ctx.transcription_segments = full_segments_payload.get("segments", [])
    ctx.word_level_transcription = word_level_transcription

    # Сохраняем в БД (как и раньше) и в JSON-кэш
    if ctx.video_id:
        ctx.db.add_transcription(ctx.video_id, segments_legacy)
    
    save_json_safely(full_segments_payload, segments_cache_path)
    save_json_safely(word_level_transcription, words_cache_path)
    
    print("Unified transcription complete. Results saved to context and cache.")
    return True


def prepare_transcript_text(ctx: ProcessingContext) -> None:
    """Формирует текст транскрипции через LanguageTasks.build_transcription_prompt и логирует превью."""
    TransText = build_transcription_prompt(ctx.transcription_segments)
    ctx.transcription_text = TransText
    print(f"\nFirst {cfg.processing.log_transcription_preview_len} characters of transcription:")
    print(TransText[:cfg.processing.log_transcription_preview_len] + "...")


def fetch_highlights(ctx: ProcessingContext) -> list:
    """Запрашивает у LLM список обогащённых хайлайтов по готовому тексту транскрипции."""
    print("Generating new highlights")
    return GetHighlights(ctx.transcription_text or "")


@timed_operation("process_highlight")
def process_highlight(ctx: ProcessingContext, item) -> Optional[str]:
    """Обрабатывает один хайлайт: кропы, субтитры, сохранение; возвращает путь финального файла либо None."""
    temp_segment = None
    cropped_vertical_temp = None
    cropped_vertical_final = None
    segment_audio_path = None
    transcription_result = None

    seq = int(item.get("_seq", 1)) if isinstance(item, dict) else 1
    total = int(item.get("_total", 1)) if isinstance(item, dict) else 1

    try:
        start = float(item["start"])
        stop = float(item["end"])

        # Корректировка stop по фактическому окончанию последнего полного слова (по start < stop)
        adjusted_stop = stop
        if getattr(ctx, "word_level_transcription", None):
            last_word_end = find_last_word_end_time(ctx.word_level_transcription, stop)
            if last_word_end is not None and last_word_end > stop:
                prev_stop = adjusted_stop
                adjusted_stop = last_word_end
                logger.logger.info(f"[WordLevel] Adjusted stop from {prev_stop:.2f}s to {adjusted_stop:.2f}s based on last word end")
        # Гарантия: stop > start
        if adjusted_stop <= start:
            adjusted_stop = start + 0.1

        # Округление времени окончания short в положительную сторону для полного слова
        prev_adjusted_stop = adjusted_stop
        adjusted_stop = math.ceil(adjusted_stop)
        if adjusted_stop != prev_adjusted_stop:
            logger.logger.info(f"[Ceil] Rounded up stop from {prev_adjusted_stop:.2f}s to {adjusted_stop:.2f}s")

        logger.logger.info(f"\n--- Processing Highlight {seq}/{total}: Start={start:.2f}s, End={stop:.2f}s (effective end {adjusted_stop:.2f}s) ---")
        if isinstance(item, dict) and "caption_with_hashtags" in item:
            logger.logger.info(f"Caption: {item['caption_with_hashtags']}")

        # --- Define File Paths ---
        base_name = os.path.splitext(os.path.basename(ctx.video_path))[0]
        output_base = f"{base_name}_highlight_{seq}"
        temp_segment = os.path.join(ctx.cfg.processing.videos_dir, f"{output_base}_temp_segment.mp4")
        cropped_vertical_temp = os.path.join(ctx.cfg.processing.videos_dir, f"{output_base}_vertical_temp.mp4")
        cropped_vertical_final = os.path.join(ctx.cfg.processing.videos_dir, f"{output_base}_vertical_final.mp4")
        # Use unified naming with zero-padded index for final output (and derived temp anim path)
        final_output_with_captions, _unused_temp_anim = build_short_output_name(base_name, seq, SHORTS_DIR)
        if USE_ANIMATED_CAPTIONS:
            segment_audio_path = os.path.join(ctx.cfg.processing.videos_dir, f"{output_base}_temp_audio.wav")

        # 1. Extract Segment (Video + Audio, Original Aspect Ratio)
        with logger.operation_context("extract_segment", {"start": start, "end": adjusted_stop}):
            logger.logger.info("1. Extracting segment...")
            extract_success = crop_video(ctx.video_path, temp_segment, start, adjusted_stop, ctx.initial_width, ctx.initial_height)
            if not extract_success:
                logger.logger.error(f"Failed step 1 for highlight {seq}. Skipping.")
                if os.path.exists(temp_segment):
                    try:
                        os.remove(temp_segment)
                    except Exception as clean_e:
                        logger.logger.warning(f"Warning: Could not remove temp segment file: {clean_e}")
                return None

        # --- CHECK DIMENSIONS: Segment ---
        with logger.operation_context("check_segment_dimensions", {"segment_path": temp_segment}):
            logger.logger.info("\n--- Checking Segment Video Dimensions ---")
            segment_width, segment_height = get_video_dimensions(temp_segment)
            if segment_width is None or segment_height is None:
                logger.logger.error("Error: Could not determine segment video dimensions. Skipping highlight.")
                if os.path.exists(temp_segment):
                    try:
                        os.remove(temp_segment)
                    except Exception as clean_e:
                        logger.logger.warning(f"Warning: Could not remove temp segment file: {clean_e}")
                return None
            if segment_width != ctx.initial_width or segment_height != ctx.initial_height:
                logger.logger.warning(f"Warning: Segment dimensions ({segment_width}x{segment_height}) differ from initial ({ctx.initial_width}x{ctx.initial_height}).")
            logger.logger.info("--- Segment Check Done ---")

        # 2. Create Vertical Crop (Based on crop_mode configuration)
        with logger.operation_context("create_vertical_crop", {"segment_path": temp_segment}):
            crop_mode = cfg.processing.crop_mode
            if crop_mode == "70_percent_blur":
                logger.logger.info("2. Creating 70% width crop with blur background...")
                crop_function = crop_to_70_percent_with_blur
                crop_error_msg = "70% crop with blur"
            elif crop_mode == "average_face":
                logger.logger.info("2. Creating average face centered vertical crop...")
                crop_function = crop_to_vertical_average_face
                crop_error_msg = "average face crop"
            else:
                logger.logger.warning(f"Unknown crop_mode '{crop_mode}', falling back to 70_percent_blur")
                crop_function = crop_to_70_percent_with_blur
                crop_error_msg = "70% crop with blur (fallback)"

            vert_crop_path = crop_function(temp_segment, cropped_vertical_temp)
            if not vert_crop_path:
                logger.logger.error(f"Failed step 2 ({crop_error_msg}) for highlight {seq}. Skipping.")
                if os.path.exists(temp_segment):
                    try:
                        os.remove(temp_segment)
                    except Exception as clean_e:
                        logger.logger.warning(f"Warning: Could not remove temp segment file: {clean_e}")
                if os.path.exists(cropped_vertical_temp):
                    try:
                        os.remove(cropped_vertical_temp)
                    except Exception as clean_e:
                        logger.logger.warning(f"Warning: Could not remove temp vertical crop file: {clean_e}")
                return None
            cropped_vertical_temp = vert_crop_path

        # 3. Crop Bottom Off Vertical Video (Temporary Fix)
        if CROP_PERCENTAGE_BOTTOM > 0:
            print("3. Applying bottom crop to vertical video...")
            bottom_crop_success = crop_bottom_video(cropped_vertical_temp, cropped_vertical_final, CROP_PERCENTAGE_BOTTOM)
            if not bottom_crop_success:
                print(f"Failed step 3 for highlight {seq}. Skipping.")
                if os.path.exists(temp_segment):
                    try:
                        os.remove(temp_segment)
                    except Exception:
                        pass
                if os.path.exists(cropped_vertical_temp):
                    try:
                        os.remove(cropped_vertical_temp)
                    except Exception:
                        pass
                if os.path.exists(cropped_vertical_final):
                    try:
                        os.remove(cropped_vertical_final)
                    except Exception:
                        pass
                return None
        else:
            print("No bottom crop applied")
            cropped_vertical_final = cropped_vertical_temp

        # 4. Choose Captioning Method
        captioning_success = False
        if USE_ANIMATED_CAPTIONS:
            print("Attempting Word-Level Animated Captions (reusing global word-level transcription)...")
            transcription_result = None
            if getattr(ctx, "word_level_transcription", None):
                # Подготовка слов через чистый хелпер
                try:
                    transcription_result = prepare_words_for_segment(ctx.word_level_transcription, start, adjusted_stop)
                    # Логирование количества слов после фильтрации
                    words_count = 0
                    try:
                        segs = transcription_result.get("segments", []) or []
                        if segs:
                            words_count = len(segs[0].get("words", []))
                    except Exception:
                        words_count = 0
                    print(f"[WordLevel] Prepared {words_count} words for animated captions from global transcription.")
                    # Опционально сохраним JSON слов для дебага/просмотра
                    try:
                        base_sanitized = sanitize_base_name(os.path.splitext(os.path.basename(ctx.video_path))[0])
                        words_payload = to_words_payload(transcription_result)
                        target_words = build_transcriptions_dir() / f"{base_sanitized}_highlight_{seq}_words.json"
                        save_json_safely(words_payload, target_words)
                    except Exception:
                        pass
                except Exception as e:
                    print(f"[WordLevel][WARN] Failed to prepare words subset: {e}")
                    transcription_result = None
            else:
                print("[WordLevel][WARN] No global word-level transcription available; cannot animate captions for this highlight.")
    
            if transcription_result and transcription_result.get("segments", []) and transcription_result["segments"][0].get("words"):
                # tone/keywords heuristic — compute meta based on segment text used for captions
                text_for_segment = ""
                try:
                    # Prefer enriched text from highlight item (LLM-extracted for this segment)
                    text_for_segment = (item.get("segment_text", "") if isinstance(item, dict) else "") or ""
                except Exception:
                    text_for_segment = ""
                if not text_for_segment:
                    # Fallback: reconstruct from prepared words list
                    try:
                        segs = transcription_result.get("segments", []) or []
                        if segs:
                            words = segs[0].get("words", []) or []
                            text_for_segment = " ".join(
                                (w.get("text") or w.get("word") or "").strip()
                                for w in words if isinstance(w, dict) and (w.get("text") or w.get("word"))
                            ).strip()
                    except Exception:
                        text_for_segment = ""
                meta = compute_tone_and_keywords(text_for_segment) if text_for_segment else None

                # emoji: heuristics and placement — propagate emoji metadata (backward compatible)
                highlight_meta = meta or {}
                try:
                    cfg_emoji = getattr(ctx.cfg.captions, "emoji", None)
                    if cfg_emoji and getattr(cfg_emoji, "enabled", False) and text_for_segment:
                        tone_val = (highlight_meta.get("tone") if isinstance(highlight_meta, dict) else None) or "neutral"
                        max_per = int(getattr(cfg_emoji, "max_per_short", 0) or 0)
                        emojis = compute_emojis_for_segment(text_for_segment, tone_val, max_per)
                        if isinstance(highlight_meta, dict):
                            highlight_meta = {**highlight_meta, "emojis": list(emojis or [])[:max_per]}
                except Exception:
                    # Полная обратная совместимость: любые ошибки с эмодзи не должны ломать рендер
                    pass

                captioning_success = animate_captions(
                    cropped_vertical_final,
                    temp_segment,
                    transcription_result,
                    final_output_with_captions,
                    style_cfg=ctx.cfg.captions,
                    highlight_meta=highlight_meta
                )
            else:
                print("Word-level data for this segment is empty. Skipping animation.")
                captioning_success = False
        else:
            print("Using Standard ASS Caption Burning...")
            transcriptions_legacy = [[
                str(seg.get("text", "")),
                float(seg.get("start", 0.0)),
                float(seg.get("end", 0.0)),
            ] for seg in (ctx.transcription_segments or [])]
            captioning_success = burn_captions(cropped_vertical_final, temp_segment, transcriptions_legacy, start, adjusted_stop, final_output_with_captions, style_cfg=ctx.cfg.captions)

        # 5. Handle Captioning Result
        if not captioning_success:
            print(f"Animated caption generation failed for highlight {seq}. Attempting ASS burn fallback...")
            transcriptions_legacy = [[
                str(seg.get("text", "")),
                float(seg.get("start", 0.0)),
                float(seg.get("end", 0.0)),
            ] for seg in (ctx.transcription_segments or [])]
            fallback_success = burn_captions(cropped_vertical_final, temp_segment, transcriptions_legacy, start, adjusted_stop, final_output_with_captions, style_cfg=ctx.cfg.captions)
            if not fallback_success:
                print(f"ASS fallback failed for highlight {seq}. Skipping.")
                if os.path.exists(temp_segment):
                    try:
                        os.remove(temp_segment)
                    except Exception as clean_e:
                        print(f"Warning: Could not remove temp segment file: {clean_e}")
                if os.path.exists(cropped_vertical_temp):
                    try:
                        os.remove(cropped_vertical_temp)
                    except Exception as clean_e:
                        print(f"Warning: Could not remove temp vertical file: {clean_e}")
                if os.path.exists(cropped_vertical_final):
                    try:
                        os.remove(cropped_vertical_final)
                    except Exception as clean_e:
                        print(f"Warning: Could not remove final vertical file: {clean_e}")
                if segment_audio_path and os.path.exists(segment_audio_path):
                    try:
                        os.remove(segment_audio_path)
                    except Exception as clean_e:
                        print(f"Warning: Could not remove segment audio file: {clean_e}")
                return None
            else:
                captioning_success = True

        logger.logger.info(f"Successfully processed highlight {seq}.")
        ctx.outputs.append(final_output_with_captions)
        logger.logger.info(f"Saving highlight {seq} info to database: {final_output_with_captions}")

        segment_text = item.get('segment_text', '') if isinstance(item, dict) else ''
        caption = item.get('caption_with_hashtags', '') if isinstance(item, dict) else ''

        with logger.operation_context("save_to_database", {"video_id": ctx.video_id, "highlight_path": final_output_with_captions}):
            ctx.db.add_highlight(
                ctx.video_id,
                start,
                adjusted_stop,
                final_output_with_captions,
                segment_text=segment_text,
                caption_with_hashtags=caption
            )

        # --- Cleanup Intermediate Files ---
        with logger.operation_context("cleanup_intermediate_files", {"highlight_seq": seq}):
            logger.logger.info("Cleaning up intermediate files for this highlight...")
            if os.path.exists(temp_segment):
                try:
                    os.remove(temp_segment)
                except Exception as clean_e:
                    logger.logger.warning(f"Warning: Could not remove temp segment file: {clean_e}")
            if os.path.exists(cropped_vertical_temp):
                try:
                    os.remove(cropped_vertical_temp)
                except Exception as clean_e:
                    logger.logger.warning(f"Warning: Could not remove temp vertical file: {clean_e}")
            if os.path.exists(cropped_vertical_final):
                try:
                    os.remove(cropped_vertical_final)
                except Exception as clean_e:
                    logger.logger.warning(f"Warning: Could not remove final vertical file: {clean_e}")
            if segment_audio_path and os.path.exists(segment_audio_path):
                try:
                    os.remove(segment_audio_path)
                except Exception as clean_e:
                    logger.logger.warning(f"Warning: Could not remove segment audio file: {clean_e}")

        return final_output_with_captions

    except Exception:
        print(f"\n--- Error processing highlight {seq} --- ")
        traceback.print_exc()
        print("Continuing to next highlight if available.")
        if temp_segment and os.path.exists(temp_segment):
            try:
                os.remove(temp_segment)
            except Exception as clean_e:
                print(f"Warning: Could not remove temp segment file: {clean_e}")
        if cropped_vertical_temp and os.path.exists(cropped_vertical_temp):
            try:
                os.remove(cropped_vertical_temp)
            except Exception as clean_e:
                print(f"Warning: Could not remove temp vertical file: {clean_e}")
        if cropped_vertical_final and os.path.exists(cropped_vertical_final):
            try:
                os.remove(cropped_vertical_final)
            except Exception as clean_e:
                print(f"Warning: Could not remove final vertical file: {clean_e}")
        if segment_audio_path and os.path.exists(segment_audio_path):
            try:
                os.remove(segment_audio_path)
            except Exception as clean_e:
                print(f"Warning: Could not remove segment audio file: {clean_e}")
        return None


def process_all_highlights(ctx: ProcessingContext, items: list) -> List[str]:
    """Итерирует по сегментам, вызывает process_highlight, накапливает выходы и печатает итоги/ошибки."""
    try:
        final_output_paths: List[str] = []
        total = len(items or [])
        for i, raw_item in enumerate(items or []):
            item = dict(raw_item) if isinstance(raw_item, dict) else raw_item
            if isinstance(item, dict):
                item["_seq"] = i + 1
                item["_total"] = total
            out_path = process_highlight(ctx, item)
            if out_path:
                final_output_paths.append(out_path)

        if not final_output_paths:
            print("\nProcessing finished, but no highlight segments were successfully converted.")
            return []
        else:
            print(f"\nProcessing finished. Generated {len(final_output_paths)} shorts in '{SHORTS_DIR}' directory.")
            return final_output_paths
    except Exception as e:
        print(f"Error in overall highlight processing: {str(e)}")
        traceback.print_exc()
        return []


def _select_whisper_runtime():
    """Выбирает оптимальные параметры для модели Whisper с GPU-first подходом."""
    try:
        has_cuda = torch.cuda.is_available()
        gpu_count = torch.cuda.device_count() if has_cuda else 0
    except Exception:
        has_cuda = False
        gpu_count = 0

    # GPU-first подход
    if has_cuda and cfg.logging.gpu_priority_mode:
        device = "cuda"
        # Используем лучшую доступную модель для GPU
        model_size = "large-v3"
        compute_type = "float16"
        cpu_threads = 0  # Для GPU используем 0 CPU threads
        logger.logger.info(f"GPU-first режим: Используем {gpu_count} GPU(s), модель {model_size}")
    else:
        device = "cpu"
        model_size = "small"
        compute_type = "int8"
        cpu_threads = max(1, os.cpu_count() - 2) if os.cpu_count() else 4
        logger.logger.info(f"CPU режим: Используем {cpu_threads} потоков, модель {model_size}")

    return model_size, device, compute_type, cpu_threads


@timed_operation("video_processing_pipeline")
def process_video(url: str = None, local_path: str = None):
    """
    Координатор пайплайна. Публичная сигнатура сохранена.
    Возвращает список путей к сгенерированным клипам или None при ошибке/пустом результате.
    """
    with logger.operation_context("initialize_context", {"url": url, "local_path": local_path}):
        ctx = init_context(url, local_path)

    with logger.operation_context("resolve_video_source", {"url": url, "local_path": local_path}):
        if not resolve_video_source(ctx):
            return None

    with logger.operation_context("validate_dimensions", {"video_path": ctx.video_path}):
        if not validate_dimensions(ctx):
            return None

    with logger.operation_context("ensure_audio", {"video_path": ctx.video_path}):
        if not ensure_audio(ctx):
            return None

    # --- Загрузка модели Whisper ---
    with logger.operation_context("load_whisper_model", {"model_size": "auto", "device": "auto"}):
        print("\n--- Loading Whisper Model ---")
        model_size, device, compute_type, cpu_threads = _select_whisper_runtime()
        os.environ["OMP_NUM_THREADS"] = str(cpu_threads)
        os.environ["MKL_NUM_THREADS"] = str(cpu_threads)
        try:
            model = WhisperModel(
                model_size,
                device=device,
                compute_type=compute_type,
                cpu_threads=cpu_threads if device == "cpu" else 0,
                num_workers=2,
            )
            print(f"Faster-Whisper model loaded: {model_size} on {device} ({compute_type})")
        except Exception as e:
            print(f"FATAL: Could not load Whisper model. Error: {e}")
            return None

    # --- Транскрипция (унифицированный вызов) ---
    with logger.operation_context("transcription", {"audio_path": ctx.audio_path}):
        logger.logger.info(f"Начало транскрипции аудио: {ctx.audio_path}")
        transcription_start = time.time()

        if not run_unified_transcription(ctx, model):
            logger.logger.error("Транскрипция завершилась неудачей")
            return None

        transcription_time = time.time() - transcription_start
        logger.logger.info(f"Транскрипция завершена за {transcription_time:.2f} секунд")

    with logger.operation_context("prepare_transcript_text", {"transcription_length": len(ctx.transcription_text or "")}):
        prepare_transcript_text(ctx)

    try:
        with logger.operation_context("fetch_highlights", {"transcription_length": len(ctx.transcription_text or "")}):
            highlights = fetch_highlights(ctx)
            if not highlights or len(highlights) == 0:
                print("No valid highlights found")
                return None

        with logger.operation_context("process_highlights", {"highlights_count": len(highlights)}):
            # Создаем прогресс-бар для обработки хайлайтов
            progress_bar = logger.create_progress_bar(
                total=len(highlights),
                desc="Обработка хайлайтов",
                unit="highlight"
            )

            outputs = []
            for i, highlight in enumerate(highlights):
                # Ensure proper sequencing for unique filenames and logging
                payload = dict(highlight) if isinstance(highlight, dict) else highlight
                if isinstance(payload, dict):
                    payload["_seq"] = i + 1
                    payload["_total"] = len(highlights)
                with logger.operation_context(
                    "process_single_highlight",
                    {"highlight_index": i, "highlight_text": (payload.get("caption_with_hashtags", "")[:50] if isinstance(payload, dict) else "")}
                ):
                    output = process_highlight(ctx, payload)
                    if output:
                        outputs.append(output)

                progress_bar.update(1)
                progress_bar.set_postfix({
                    "Обработано": f"{i+1}/{len(highlights)}",
                    "Успешно": len(outputs)
                })

            progress_bar.close()

        if not outputs:
            return None
        return outputs
    except Exception as e:
        print(f"Error in overall highlight processing: {str(e)}")
        traceback.print_exc()
        return None


    

def find_last_word_end_time(word_level_transcription: dict, segment_end_time: float) -> Optional[float]:
    """
    Определяет фактическое время окончания «последнего слова до segment_end_time».

    Определение «последнего слова до segment_end_time»:
    - Рассматриваются только слова, у которых start < segment_end_time (start — абсолютный).
    - Возвращается максимальный end среди таких слов.

    Почему функция может вернуть end > segment_end_time:
    - Слово может начинаться до segment_end_time, а заканчиваться ПОСЛЕ него. Это важно для
      корректировки stop вправо, чтобы не обрывать слово в анимации (используется max(original_stop, last_word_end)).

    Граничные случаи:
    - Отсутствуют сегменты/слова — возвращается None.
    - Нечисловые или отсутствующие start/end у слова — такие слова пропускаются.
    - Порядок слов/сегментов может быть произвольным — берётся максимум end среди подходящих.

    Безопасность:
    - Функция устойчиво обрабатывает некорректные структуры (не dict / пустые поля), возвращая None.
    """
    try:
        if not isinstance(word_level_transcription, dict):
            return None

        segments = word_level_transcription.get("segments", []) or []
        if not isinstance(segments, list) or not segments:
            return None

        last_end: Optional[float] = None
        for seg in segments:
            # Безопасно достаём список слов
            words = seg.get("words", []) if isinstance(seg, dict) else getattr(seg, "words", []) or []
            if not words:
                continue

            for w in words:
                if isinstance(w, dict):
                    s = _to_float(w.get("start", None), None)
                    e = _to_float(w.get("end", None), None)
                else:
                    s = _to_float(getattr(w, "start", None), None)
                    e = _to_float(getattr(w, "end", None), None)

                # Пропускаем слова без числовых таймкодов
                if s is None or e is None:
                    continue

                # Критерий отбора — слово началось до segment_end_time
                if s < segment_end_time:
                    if last_end is None or e > last_end:
                        last_end = e

        return last_end
    except Exception:
        return None


if __name__ == "__main__":
    print("\nVideo Processing Options:")
    print("1. Process YouTube URL")
    print("2. Process Local File")
    print("3. Process Film Mode (analyze best moments)")
    choice = input("Enter your choice (1, 2, or 3): ")

    if choice == "1":
        url = input("Enter YouTube URL: ")
        output = process_video(url=url)
    elif choice == "2":
        local_file = input("Enter path to local video file: ")
        output = process_video(local_path=local_file)
    elif choice == "3":
        print("\n🎬 Режим 'Фильм' - Анализ лучших моментов")
        print("Выберите источник видео:")

        # Импортируем функции для работы с папкой movies
        from Components.FilmMode import scan_movies_folder, display_movie_selection, select_movie_by_number, analyze_film_main

        # Сканируем папку movies
        video_files = scan_movies_folder()

        # Показываем список файлов
        display_movie_selection(video_files)

        # Если есть файлы, предлагаем выбрать
        if video_files:
            selected_file = select_movie_by_number(video_files)

            if selected_file == "URL_INPUT":
                # Пользователь выбрал ручной ввод
                print("\nВыберите тип источника:")
                print("1. YouTube URL")
                print("2. Путь к локальному файлу")
                manual_choice = input("Введите ваш выбор (1 или 2): ").strip()

                if manual_choice == "1":
                    url = input("Введите YouTube URL: ").strip()
                    result = analyze_film_main(url=url)
                elif manual_choice == "2":
                    local_file = input("Введите путь к видео файлу: ").strip()
                    if not os.path.exists(local_file):
                        print(f"❌ Ошибка: Файл не найден: {local_file}")
                        result = None
                    else:
                        result = analyze_film_main(local_path=local_file)
                else:
                    print("❌ Неверный выбор")
                    result = None
            elif selected_file:
                # Выбран файл из папки movies
                # Дополнительная проверка существования файла
                if not os.path.exists(selected_file):
                    print(f"❌ Ошибка: Файл не найден: {selected_file}")
                    print("Файл мог быть удален после сканирования.")
                    result = None
                else:
                    result = analyze_film_main(local_path=selected_file)
            else:
                # Отмена выбора
                print("Выбор отменен.")
                result = None
        else:
            # Папка movies пуста, предлагаем ручной ввод
            print("\nПапка movies пуста. Выберите тип источника:")
            print("1. YouTube URL")
            print("2. Путь к локальному файлу")
            manual_choice = input("Введите ваш выбор (1 или 2): ").strip()

            if manual_choice == "1":
                url = input("Введите YouTube URL: ").strip()
                result = analyze_film_main(url=url)
            elif manual_choice == "2":
                local_file = input("Введите путь к видео файлу: ").strip()
                if not os.path.exists(local_file):
                    print(f"❌ Ошибка: Файл не найден: {local_file}")
                    result = None
                else:
                    result = analyze_film_main(local_path=local_file)
            else:
                print("❌ Неверный выбор")
                result = None

        if result:
            print(f"\nFilm analysis completed successfully!")
            print(f"Video ID: {result.video_id}")
            print(f"Duration: {result.duration:.1f} seconds")
            print(f"Found {len(result.keep_ranges)} best moments")
            print(f"Generated {len(result.generated_shorts)} shorts")
            print(f"Preview: {result.preview_text}")

            # Выводим информацию о сгенерированных шортах
            if result.generated_shorts:
                print(f"\nGenerated shorts:")
                for i, short_path in enumerate(result.generated_shorts, 1):
                    print(f"  {i}. {os.path.basename(short_path)}")

            # Сохраняем JSON результат
            import json
            import os
            from datetime import datetime

            output_dir = "film_analysis_results"
            os.makedirs(output_dir, exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = os.path.join(output_dir, f"film_analysis_{result.video_id}_{timestamp}.json")

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'video_id': result.video_id,
                    'duration': result.duration,
                    'keep_ranges': result.keep_ranges,
                    'scores': result.scores,
                    'preview_text': result.preview_text,
                    'risks': result.risks,
                    'metadata': result.metadata,
                    'generated_shorts': result.generated_shorts
                }, f, ensure_ascii=False, indent=2)

            print(f"Results saved to: {output_file}")

            # Устанавливаем output для совместимости с остальной логикой
            output = result.generated_shorts if result.generated_shorts else None
        else:
            print("\nFilm analysis failed!")
            output = None
    else:
        print("Invalid choice")
        output = None

    if output:
        # If output is a list (multiple shorts generated)
        if isinstance(output, list):
            print(f"\nSuccess! Output saved to:")
            for path in output:
                print(f"- {path}")
        else: # Should not happen with current logic, but handle just in case
            print(f"\nSuccess! Output saved to: {output}")
    else:
        print("\nProcessing failed or no shorts generated!")
</file>

<file path="MOVIE_SELECTION_README.md">
# Интерфейс выбора файлов из папки movies

## Обзор

Добавлен новый интерфейс для режима "фильм", который позволяет выбирать видео файлы из специальной папки `movies` в корне проекта. Это упрощает работу с локальными видео файлами и делает интерфейс более интуитивным.

## Новые возможности

### 1. Папка movies
- Создана папка `movies/` в корне проекта
- Поддерживаемые форматы: `.mp4`, `.avi`, `.mkv`, `.mov`, `.wmv`
- Файлы сканируются автоматически при запуске режима "фильм"

### 2. Интерфейс выбора файлов
- **Автоматическое сканирование**: При запуске режима "фильм" автоматически сканируется папка movies
- **Нумерованный список**: Файлы отображаются с номерами для удобного выбора
- **Информация о файлах**: Показывается размер файла и примерная длительность (если доступно)
- **Гибкий выбор**: Можно выбрать файл по номеру или перейти к ручному вводу

### 3. Обработка ошибок
- **Пустая папка**: Если папка movies пуста, предлагается ручной ввод
- **Файл не найден**: Проверяется существование файла перед обработкой
- **Неверный выбор**: Валидация ввода пользователя
- **Отмена**: Возможность отменить выбор в любой момент

## Как использовать

### Запуск
1. Запустите `main.py`
2. Выберите опцию "3. Process Film Mode (analyze best moments)"

### Выбор файла
```
🎬 Режим 'Фильм' - Анализ лучших моментов
Выберите источник видео:

🎬 Найдено 3 видео файла в папке movies:
------------------------------------------------------------
1. movie1.mp4 (245.8 MB) (15.3 мин)
2. movie2.avi (189.2 MB) (12.1 мин)
3. movie3.mkv (456.7 MB) (28.9 мин)
------------------------------------------------------------
0. Вернуться в главное меню
URL. Ввести YouTube URL или путь к файлу вручную

Введите номер видео (1-3) или 0 для отмены: 2
✅ Выбрано: movie2.avi (189.2 MB)
```

### Ручной ввод
Если ввести `URL` вместо номера, появится меню ручного ввода:
```
Выберите тип источника:
1. YouTube URL
2. Путь к локальному файлу
```

## Добавленные функции

### В FilmMode.py
- `scan_movies_folder()` - сканирует папку movies и возвращает список видео файлов
- `display_movie_selection()` - отображает нумерованный список файлов
- `select_movie_by_number()` - позволяет выбрать файл по номеру
- `_get_video_duration_quick()` - быстро получает длительность видео

### В main.py
- Обновлена логика обработки режима "фильм"
- Добавлена поддержка выбора из папки movies
- Сохранена совместимость с URL вводом

## Совместимость

✅ **Сохранена полная совместимость** с существующими функциями:
- YouTube URL обработка
- Ручной ввод путей к файлам
- Все существующие параметры и настройки

## Структура файлов

```
project/
├── movies/                    # Новая папка для видео файлов
│   ├── movie1.mp4
│   ├── movie2.avi
│   └── movie3.mkv
├── main.py                    # Обновлен
├── Components/
│   └── FilmMode.py           # Добавлены новые функции
└── MOVIE_SELECTION_README.md  # Эта документация
```

## Примеры использования

### Добавление видео файлов
```bash
# Поместите видео файлы в папку movies
cp ~/Videos/my_movie.mp4 ./movies/
cp ~/Videos/another_video.avi ./movies/
```

### Программное использование
```python
from Components.FilmMode import scan_movies_folder, display_movie_selection, select_movie_by_number

# Сканирование папки
video_files = scan_movies_folder()

# Отображение списка
display_movie_selection(video_files)

# Выбор файла
selected_file = select_movie_by_number(video_files)
if selected_file:
    print(f"Выбран файл: {selected_file}")
```

## Обработка ошибок

### Пустая папка movies
```
📁 Папка movies пуста или не содержит поддерживаемых видео файлов.
Поддерживаемые форматы: .mp4, .avi, .mkv, .mov, .wmv
Поместите видео файлы в папку 'movies' в корне проекта.
```

### Файл не найден
```
❌ Ошибка: Файл не найден: /path/to/missing/file.mp4
```

### Неверный выбор
```
❌ Неверный номер. Введите число от 1 до 3
❌ Неверный ввод. Введите число или 'URL' для ручного ввода
```

## Технические детали

- **Поддерживаемые форматы**: `.mp4`, `.avi`, `.mkv`, `.mov`, `.wmv`
- **Кодировка**: UTF-8 для корректного отображения русских имен файлов
- **Безопасность**: Все пути проверяются на существование перед использованием
- **Производительность**: Быстрое сканирование с отображением размера файлов

## Следующие шаги

1. Поместите видео файлы в папку `movies/`
2. Запустите режим "фильм" в main.py
3. Выберите нужный файл из списка
4. Наслаждайтесь автоматическим анализом лучших моментов!
</file>

<file path="README.md">
# AI-Youtube-Shorts-Generator-Gemini

An AI-powered tool that automatically generates engaging short-form videos from longer YouTube content, optimized for platforms like YouTube Shorts, Instagram Reels, and TikTok and for static videos with a 1 person speaking.

## Key Features

- **Smart Video Download**: 
  - Downloads videos from YouTube URLs with quality selection
  - Supports both progressive and adaptive streams
  - Automatically merges video and audio for best quality
  - Handles local video files as input

- **Advanced Transcription**:
  - Uses `faster-whisper` (base.en model) for efficient transcription
  - Provides both segment-level and word-level timestamps
  - CPU-optimized processing with int8 quantization
  - Multi-threaded performance for faster processing

- **AI-Powered Highlight Detection**:
  - Leverages Google's Gemini-2.0-flash model for content analysis
  - Identifies the most engaging segments from transcriptions
  - Generates relevant hashtags and captions
  - Smart content selection based on engagement potential

- **Intelligent Video Processing**:
  - Multiple vertical cropping strategies:
    - Static centered crop
    - Face-detection based dynamic cropping
    - Average face position based cropping
  - Maintains optimal 9:16 aspect ratio for shorts
  - Automatic bottom margin cropping for better framing
  - Supports both static and animated captions

- **Robust Caching System**:
  - SQLite database for efficient data management
  - Caches processed videos, audio, and transcriptions
  - Prevents redundant processing of previously handled content
  - Easy cache management and cleanup

## Prerequisites

- Python 3.10 or higher
- FFmpeg (latest version recommended)
- CUDA-compatible GPU (optional, for faster processing)
- 4GB+ RAM recommended

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/AI-Youtube-Shorts-Generator.git
   cd AI-Youtube-Shorts-Generator
   ```

2. Create and activate a virtual environment:
   ```bash
   # Windows
   python -m venv venv
   venv\Scripts\activate

   # Linux/MacOS
   python3 -m venv venv
   source venv/bin/activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Set up environment variables:
   Create a `.env` file in the project root:
   ```bash
   GOOGLE_API_KEY=your_google_ai_studio_key_here
   ```

## Usage

1. Start the tool:
   ```bash
   python main.py
   ```

2. Input either:
   - A YouTube URL
   - A path to a local video file

3. Select video quality when prompted (for YouTube downloads)

4. The tool will process your video through several stages:
   - Download/import video
   - Extract and transcribe audio
   - Identify engaging segments
   - Create vertical crops
   - Add captions
   - Generate final shorts

5. Find your processed shorts in the `shorts` directory

## Configuration Options

- `USE_ANIMATED_CAPTIONS`: Toggle between static and animated captions (in main.py) (reccomended)
- `SHORTS_DIR`: Customize output directory for processed videos
- CPU thread optimization in `Components/Transcription.py`

## Project Structure

```
AI-Youtube-Shorts-Generator/
├── Components/
│   ├── Captions.py       # Caption generation and rendering
│   ├── Database.py       # SQLite database management
│   ├── Edit.py          # Video editing and processing
│   ├── FaceCrop.py      # Vertical cropping algorithms
│   ├── LanguageTasks.py # AI content analysis
│   ├── Speaker.py       # Speaker detection (experimental)
│   ├── Transcription.py # Audio transcription
│   └── YoutubeDownloader.py # Video download handling
├── main.py              # Main execution script
├── requirements.txt     # Python dependencies
└── .env                # Environment variables
```

## Database Schema

The SQLite database (`video_processing.db`) contains three main tables:

1. **videos**:
   - id (PRIMARY KEY)
   - youtube_url
   - local_path
   - audio_path
   - created_at

2. **transcriptions**:
   - id (PRIMARY KEY)
   - video_id (FOREIGN KEY)
   - transcription_data
   - created_at

3. **highlights**:
   - id (PRIMARY KEY)
   - video_id (FOREIGN KEY)
   - start_time
   - end_time
   - output_path
   - segment_text
   - caption_with_hashtags
   - created_at

## Known Issues & Limitations

1. **Face Detection**:
   - The face-based cropping can be inconsistent with multiple faces
   - May need manual adjustment for optimal framing in some cases

2. **Speaker Detection**:
   - Current implementation uses basic voice activity detection
   - Full speaker diarization not yet implemented

3. **Resource Usage**:
   - Processing long videos can be memory-intensive
   - GPU acceleration limited to specific components

## Troubleshooting

1. If facing cache-related issues:
   - Delete `video_processing.db` to clear the cache
   - Remove temporary files in the `videos` directory

2. For video processing errors:
   - Ensure FFmpeg is properly installed and accessible
   - Check available disk space for temporary files
   - Verify input video format compatibility

3. For AI-related issues:
   - Confirm Google API key is valid and has sufficient quota
   - Check internet connectivity for API calls

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to your branch
5. Create a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments
- SQL integration made by [YassineKADER](https://github.com/YassineKADER/AI-Youtube-Shorts-Generator-)
- Original project by [SamurAIGPT](https://github.com/SamurAIGPT/AI-Youtube-Shorts-Generator)
- Uses Google's Gemini AI for content analysis
- Powered by faster-whisper for transcription

## Batch‑метаданные

- Цель: пакетно сгенерировать для каждого текстового фрагмента видео метаданные: `title`, `description`, `hashtags`.
- Вход: JSON‑массив объектов вида `{"id": string, "text": string}`.
- Выход: JSON‑массив объектов вида `{"id": string, "title": string, "description": string, "hashtags": string[]}`.
  - Требования:
    - title: 40–70 символов
    - description: ≤ 150 символов
    - hashtags: 3–5 шт., первый элемент строго `#shorts`
    - строго один валидный JSON‑массив без текста вокруг
- Реализация: [generate_metadata_batch()](Components/LanguageTasks.py:576)
- Место интеграции в пайплайн: [GetHighlights()](Components/LanguageTasks.py:711) — после выделения тайм‑сегментов и извлечения текста для каждого сегмента.

Новый системный промпт (точно как в ТЗ), применяемый при пакетной генерации:
```
Ты — эксперт по SMM и продвижению на YouTube, специализирующийся на вирусных Shorts. Тебе на вход подается JSON-массив текстовых фрагментов из видео, каждый с уникальным `id`. Твоя задача — для каждого фрагмента создать оптимальный набор метаданных для максимального вовлечения и охвата.

Правила:
1. Твой ответ должен быть ИСКЛЮЧИТЕЛЬНО одним валидным JSON-массивом. Никакого текста до или после.
2. Для каждого входного объекта с `id` ты должен сгенерировать объект в выходном массиве с тем же `id` и тремя полями: `title`, `description` и `hashtags`.
3. title (заголовок): 40–70 символов, интригующий, задает вопрос или создает предвкушение. Обязательно использовать ключевые слова из текста.
4. description (описание): до 150 символов, кратко раскрывает суть, допускается призыв к действию.
5. hashtags (хэштеги): массив из 3–5 строк; первым ВСЕГДА `#shorts`; остальные — максимально релевантны теме фрагмента.

Пример Входа:
[{"id":"seg_1","text":"Сегодня обсудим, как автоматически находить лучшие моменты в видео..."}]

Пример Выхода:
[{"id":"seg_1","title":"Нейросеть находит лучшие моменты в видео?","description":"Смотрите, как ИИ анализирует ролики для создания шортсов.","hashtags":["#shorts","#ИИ","#нейросети","#видеомонтаж"]}]
```

## Rate‑limiting

- Централизованная обёртка для вызова LLM: [call_llm_with_retry()](Components/LanguageTasks.py:145)
  - Пытается повторить запрос при лимитах API и парсит задержку повтора.
  - Разбор задержки: [parse_retry_delay_seconds()](Components/LanguageTasks.py:89)
- Поддерживаемые форматы retryDelay:
  - `Retry-After: 28`
  - `retry-after: 28`
  - `"retryDelay": "28s"`
  - `retryDelay: 28s`
- Логи (точные формулировки):
  - "Лимит API обработан. Выполняю паузу на X секунд перед попыткой #Y."
  - "Не удалось извлечь retryDelay. Попытки прекращены."
- Поведение:
  - При наличии корректного `retryDelay` выполняется `sleep(X)` и повтор запроса.
  - При отсутствии `retryDelay` попытки прекращаются и исключение пробрасывается дальше.

## Конфигурация (config.yaml)

- Путь: [config.yaml](config.yaml)
- Структура секций и ключи:
  - `processing`:
    - `use_animated_captions`: bool — использовать ли анимированные субтитры
    - `shorts_dir`: str — каталог для итоговых шортов
    - `videos_dir`: str — каталог для промежуточных файлов/видео
    - `crop_bottom_percent`: float — нижний кроп вертикального видео (в процентах)
    - `min_video_dimension_px`: int — минимальный размер видео
    - `log_transcription_preview_len`: int — длина превью транскрипции в логах
  - `llm`:
    - `model_name`: str — модель Gemini
    - `temperature_highlights`: float — температура для поиска хайлайтов
    - `temperature_metadata`: float — температура для метаданных
    - `max_attempts_highlights`: int — попытки при извлечении сегментов
    - `max_attempts_metadata`: int — попытки при генерации метаданных
    - `highlight_min_sec` / `highlight_max_sec`: границы длительности сегмента
    - `max_highlights`: максимум сегментов
- Минимальный пример:
```yaml
processing:
  use_animated_captions: true
  shorts_dir: "shorts"
  videos_dir: "videos"

llm:
  model_name: "gemini-2.5-flash"
  temperature_highlights: 0.2
  temperature_metadata: 1.0
  highlight_min_sec: 29
  highlight_max_sec: 61
  max_highlights: 20
```
- Поведение при отсутствии файла: используются значения по умолчанию; при этом выводится сообщение
  "Конфиг не найден. Использую значения по умолчанию." — см. [Components/config.py](Components/config.py:100).
- Логирование факта загрузки и активной модели — см. [main.py](main.py:15).

## Централизованные пути и конфигурация

Начиная с актуальной версии, все ресурсы и каталоги резолвятся относительно базовой директории из конфига. Ключевые параметры задаются в секции `paths` и `processing`.

Пример фрагмента `config.yaml` (минимальный):
```yaml
paths:
  base_dir: .
  fonts_dir: fonts
processing:
  transcriptions_dir: transcriptions
  shorts_dir: shorts
```

Назначение ключей:
- `base_dir` — корень проекта, относительно которого резолвятся внутренние пути.
- `fonts_dir` — подкаталог со шрифтами (по умолчанию `fonts`), резолвится относительно `base_dir`.
- `transcriptions_dir` — директория, куда сохраняются транскрипции (`.txt/.json/.srt/.vtt`).
- `shorts_dir` — директория, куда сохраняются итоговые шорт‑видео.

Центральные функции резолва путей:
- Ресурсы: [resolve_path()](Components/Paths.py:22)
- Шрифты: [fonts_path()](Components/Paths.py:37)

Пример резолва пути к шрифту «Montserrat-Bold.ttf» с настройками по умолчанию:
- Конфиг: `paths.base_dir: .`, `paths.fonts_dir: fonts`
- Вызов: [fonts_path()](Components/Paths.py:37) для `"Montserrat-Bold.ttf"` даст путь вида: `<ABS_BASE_DIR>/fonts/Montserrat-Bold.ttf`

## Именование выходных short‑файлов

Уникальные имена итоговых и временных short‑файлов формируются функцией [build_short_output_name()](Components/Paths.py:6).

Шаблоны:
- Итоговый: `shorts/{base_name}_highlight_{idx:02d}_final.mp4`  
  Пример: `shorts/master-ABC_highlight_01_final.mp4`
- Временный (анимация): `{final_path}_temp_anim.mp4`

Индекс `idx` — это порядковый номер хайлайта в текущей сессии; он пробрасывается из цикла и логируется (см. [main.py](main.py)).

## Экспорт транскрипций

После завершения унифицированной транскрипции ("Unified transcription complete…") экспорт выполняется автоматически, код — [Components/Transcription.py](Components/Transcription.py).

Создаются файлы:
- `{transcriptions_dir}/{base_name}.txt`
- `{transcriptions_dir}/{base_name}.json`
- `{transcriptions_dir}/{base_name}.srt`
- `{transcriptions_dir}/{base_name}.vtt`

Особенности:
- JSON сохраняет сегменты и слова в UTF‑8 (`ensure_ascii=False`).
- Папка назначения задаётся через `processing.transcriptions_dir` в `config.yaml`.

## Пути и совместимость окружений

- Абсолютные пути вида `/content/uol/*` больше не используются — все ресурсы резолвятся относительно `paths.base_dir`.
- Для Google Colab/контейнеров можно установить `paths.base_dir` на рабочую директорию окружения — остальные относительные пути (`fonts_dir`, `transcriptions_dir`, `shorts_dir`) подхватятся корректно.

### Релевантные тесты

- Проверка уникальности имён short‑файлов: [tests/test_output_naming.py](tests/test_output_naming.py)
- Экспорт транскрипции в 4 формата: [tests/test_transcription_export.py](tests/test_transcription_export.py)
- Резолв путей для ресурсов и шрифта: [tests/test_resource_paths.py](tests/test_resource_paths.py)
## Тесты

- Запуск всех тестов:
  - `python -m unittest -v`
- Покрытие:
  - Форматирование транскрипции [build_transcription_prompt()](Components/LanguageTasks.py:38) —
    [tests/test_language_tasks_prompt_formatting.py](tests/test_language_tasks_prompt_formatting.py)
  - Пакетная генерация метаданных [generate_metadata_batch()](Components/LanguageTasks.py:576) —
    [tests/test_language_tasks_batch_metadata.py](tests/test_language_tasks_batch_metadata.py)
  - Обработка лимитов API [call_llm_with_retry()](Components/LanguageTasks.py:145) —
    [tests/test_language_tasks_rate_limit.py](tests/test_language_tasks_rate_limit.py)
- Для запуска не требуется реальный API/ключ: в тестах используется monkeypatch/mock.
- Опциональный онлайновый сценарий (при наличии GOOGLE_API_KEY): [tests/smoke_gemini_and_whisper.py](tests/smoke_gemini_and_whisper.py)
</file>

</files>
