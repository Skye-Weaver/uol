import math
import cv2
import numpy as np # Add numpy import
import os
import subprocess
import traceback # For detailed error printing in animate_captions
from PIL import Image, ImageDraw, ImageFont # Pillow imports for custom font

# Function to format time in SRT format
def format_time(seconds):
    milliseconds = int((seconds - math.floor(seconds)) * 1000)
    seconds = int(math.floor(seconds))
    minutes = seconds // 60
    seconds %= 60
    hours = minutes // 60
    minutes %= 60
    return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"

# Function to format time in ASS format (hours:mm:ss.cc)
def format_time_ass(seconds):
    centiseconds = int((seconds - math.floor(seconds)) * 100)
    seconds = int(math.floor(seconds))
    minutes = seconds // 60
    seconds %= 60
    hours = minutes // 60
    minutes %= 60
    return f"{hours:d}:{minutes:02d}:{seconds:02d}.{centiseconds:02d}"

# Function to generate ASS content (more compatible with ffmpeg filter)
def generate_ass_content(transcriptions, start_time, end_time):

    ass_content = """[Script Info]
Title: Auto-generated by AI-Youtube-Shorts-Generator
ScriptType: v4.00+
PlayResX: 384 # Assuming a standard vertical resolution base
PlayResY: 720
WrapStyle: 0 # Smart wrapping, respects margins
ScaledBorderAndShadow: yes

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Poppins,36,&H00FFFFFF,&H000000FF,&H00000000,&H70000000,-1,0,0,0,100,100,0,0,1,2,1,2,10,10,30,1
Style: Fallback,Arial,36,&H00FFFFFF,&H000000FF,&H00000000,&H70000000,-1,0,0,0,100,100,0,0,1,2,1,2,10,10,30,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
""" # Using Default style (tries Poppins first)

    for segment in transcriptions:
        text, seg_start, seg_end = segment
        # Filter out segments containing only [*]
        if text.strip() == '[*]':
            continue
        # Filter segments within the highlight time range
        if seg_start >= start_time and seg_end <= end_time:
            # Adjust timestamps to be relative to the highlight start
            relative_start = seg_start - start_time
            relative_end = seg_end - start_time

            # Ensure timestamps are not negative and end is after start
            if relative_start < 0: relative_start = 0.0
            if relative_end <= relative_start: relative_end = relative_start + 0.1

            # Format for ASS file
            ass_content += f"Dialogue: 0,{format_time_ass(relative_start)},{format_time_ass(relative_end)},Default,,0,0,0,,{text.strip()}\\N" # Added \\N for potential multi-line in future

    return ass_content

# Function to burn captions using FFmpeg
def burn_captions(vertical_video_path, audio_source_path, transcriptions, start_time, end_time, output_path):
    """Burns captions onto the vertical video using audio from the source segment."""
    temp_ass_path = "temp_subtitles.ass"  # Simple name in current directory
    try:
        # Create an ASS subtitle file (more compatible than SRT for styling)
        ass_content = generate_ass_content(transcriptions, start_time, end_time)

        if not ass_content.count("Dialogue:"):
            print("No relevant transcriptions found for the highlight duration. Using video without captions.")
            # Need to add audio even if no captions are burned
            ffmpeg_command_no_subs = [
                'ffmpeg',
                '-i', vertical_video_path, # Silent video input
                '-i', audio_source_path,  # Audio source input
                '-map', '0:v:0', # Video from input 0
                '-map', '1:a:0', # Audio from input 1
                '-c:v', 'copy',  # Copy video stream (faster if no filter applied)
                '-c:a', 'aac',   # Re-encode audio
                '-b:a', '128k',
                '-shortest',    # Ensure output duration matches shortest input
                '-y',
                output_path
            ]
            print("Running FFmpeg command (no subtitles, adding audio):")
            cmd_string = ' '.join([str(arg) for arg in ffmpeg_command_no_subs])
            print(f"Command: {cmd_string}")
            process = subprocess.run(ffmpeg_command_no_subs, check=True, capture_output=True, text=True)
            print(f"Successfully muxed audio into: {output_path}")
            return True # Return true as the operation (adding audio) succeeded

        # Write the ASS content to the current directory
        with open(temp_ass_path, 'w', encoding='utf-8') as f:
            f.write(ass_content)

        print(f"Generated subtitle file: {temp_ass_path}")

        # FFmpeg command using two inputs and mapping streams
        ffmpeg_command = [
            'ffmpeg',
            '-i', vertical_video_path,  # Input 0: Vertically cropped video (silent)
            '-i', audio_source_path,   # Input 1: Original segment (with audio)
            # Use absolute path for subtitles file to avoid potential issues with ffmpeg's working directory
            '-filter_complex', f"[0:v]ass='{os.path.abspath(temp_ass_path)}'[video_out]",
            '-map', '[video_out]',     # Map the filtered video stream
            '-map', '1:a:0',           # Map the audio stream from input 1
            '-c:v', 'libx264',
            '-crf', '23',
            '-preset', 'medium',
            '-c:a', 'aac',           # Re-encode audio (required when filtering/mapping)
            '-b:a', '128k',
            '-shortest',             # Finish encoding when the shortest input ends
            '-y',
            output_path
        ]

        # Print the command for debugging
        print("Running FFmpeg command (burning subtitles and adding audio):")
        cmd_string = ' '.join([str(arg) for arg in ffmpeg_command])
        print(f"Command: {cmd_string}")

        # Run FFmpeg with the new command
        process = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)
        print(f"Successfully burned captions and added audio into: {output_path}")
        return True

    except subprocess.CalledProcessError as e:
        print(f"Error running FFmpeg: {e}")
        print(f"FFmpeg stdout: {e.stdout}")
        print(f"FFmpeg stderr: {e.stderr}")
        return False
    except Exception as e:
        print(f"An error occurred during caption burning: {e}")
        return False
    finally:
        # Always clean up the subtitle file
        if os.path.exists(temp_ass_path):
            try:
                os.remove(temp_ass_path)
                print(f"Removed temporary subtitle file: {temp_ass_path}")
            except Exception as e:
                print(f"Warning: Could not remove temporary subtitle file: {e}")


# --- Word-Level Animation Helpers ---

def find_active_segment_and_word(transcription_result, current_time):
    """Finds the segment and word active at the current time."""
    active_segment = None
    active_word_index_in_segment = -1 # Index within the segment's word list

    for segment in transcription_result.get("segments", []):
        # Use segment boundaries to find the active segment
        if segment['start'] <= current_time < segment['end']:
            active_segment = segment
            # Find the specific word within this segment based on word timings
            for i, word_info in enumerate(segment.get("words", [])):
                # Ensure word timings exist before comparing
                if 'start' in word_info and 'end' in word_info:
                    if word_info['start'] <= current_time < word_info['end']:
                        active_word_index_in_segment = i
                        break # Found the active word
            # If no specific word is active but the segment is, keep active_segment
            # active_word_index_in_segment will remain -1 or the found index
            break # Found the active segment

    return active_segment, active_word_index_in_segment


# --- Main Animation Function ---

def animate_captions(vertical_video_path, audio_source_path, transcription_result, output_path):
    """Creates a video with word-by-word highlighted captions based on segments."""
    temp_animated_video = output_path + "_temp_anim.mp4"
    success = False
    cap = None
    out = None

    try:
        # --- Font Setup (Pillow) ---
        font_path = "fonts/Montserrat-Bold.ttf" # ASSUMED PATH - CHANGE IF NEEDED
        font_size = 25 # Adjust size as needed
        try:
            font = ImageFont.truetype(font_path, font_size)
            print(f"Successfully loaded font: {font_path}")
        except IOError:
            print(f"Error: Font file not found at {font_path}. Please check the path.")
            print("Falling back to default Hershey font.")
            font = None # Flag to use fallback
        # --- Pre-filter segments ---
        original_segments = transcription_result.get("segments", [])
        filtered_segments = [seg for seg in original_segments if seg.get('text', '').strip() != '[*]']
        if not filtered_segments:
            print("Warning: No non-[*] segments found in transcription. Captions might be empty.")
            # Optional: Decide if you want to proceed with an empty list or return early
            # return False # Example: Exit if no valid captions

        # Update the transcription_result to use filtered segments for further processing
        transcription_result_filtered = transcription_result.copy() # Avoid modifying original dict directly if reused
        transcription_result_filtered['segments'] = filtered_segments

        print("Starting animated caption generation (Static Window/Highlight Style)...")
        cap = cv2.VideoCapture(vertical_video_path)
        if not cap.isOpened():
            print(f"Error: Cannot open video file {vertical_video_path}")
            return False

        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps <= 0: # Handle zero or negative fps
             print(f"Error: Invalid video FPS ({fps}), cannot calculate time.")
             cap.release() # Release resource
             return False
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        print(f"Input video properties: {width}x{height} @ {fps:.2f}fps")

        # --- Text Styling (Pillow) ---
        # Use RGB for Pillow colors
        text_color_rgb = (255, 255, 0)  # Yellow
        stroke_color_rgb = (0, 0, 0)    # Black outline
        stroke_width = 1                # Outline width in pixels
        bottom_margin = 120 # Increased margin (moves text higher)

        # --- Calculate Fixed Y Position (after getting height) ---
        font_ascent = 0 # Default if font fails
        if font:
            try:
                font_ascent, _ = font.getmetrics()
            except AttributeError: # Handle cases where getmetrics might not exist? (Shouldn't for TTF)
                 print("Warning: Could not get font metrics.")
        fixed_top_y = height - bottom_margin - font_ascent

        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(temp_animated_video, fourcc, fps, (width, height))
        if not out.isOpened():
            print(f"Error: Could not open video writer for {temp_animated_video}")
            cap.release() # Release resource
            return False

        frame_count = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            current_time = frame_count / fps

            # Find the segment and specific word active at this frame's time
            # Use the filtered transcription data
            active_segment, active_word_idx_in_segment = find_active_segment_and_word(transcription_result_filtered, current_time)

            # --- Get Words to Display (Max 2) ---
            words_to_display = ""
            if active_segment:
                segment_words_list = active_segment.get('words', [])
                num_words_in_segment = len(segment_words_list)

                if 0 <= active_word_idx_in_segment < num_words_in_segment:
                    word1_info = segment_words_list[active_word_idx_in_segment]
                    word1_text = word1_info.get('text', '').strip()

                    # Skip if the primary word is [*]
                    if word1_text != '[*]':
                        words_to_display = word1_text
                        # Try to get the next word
                        next_word_idx = active_word_idx_in_segment + 1
                        if next_word_idx < num_words_in_segment:
                            word2_info = segment_words_list[next_word_idx]
                            word2_text = word2_info.get('text', '').strip()
                            # Also skip if the second word is [*]
                            if word2_text != '[*]':
                                words_to_display += f" {word2_text}"

            # --- Drawing Logic (Pillow - Max 2 words) ---
            # Draw only if we have a valid window and an active word within it
            if words_to_display and font: # Only draw if we have words and font loaded
                # Convert frame BGR OpenCV to RGB Pillow
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(frame_rgb)
                draw = ImageDraw.Draw(pil_image)

                # Get text dimensions
                text_bbox = draw.textbbox((0, 0), words_to_display, font=font)
                text_width = text_bbox[2] - text_bbox[0]

                # Calculate position (centered horizontally)
                start_x = (width - text_width) // 2

                # Draw the text (single color)
                draw.text(
                    (start_x, fixed_top_y), # Use fixed vertical position
                    words_to_display,
                    font=font,
                    fill=text_color_rgb,
                    stroke_width=stroke_width,
                    stroke_fill=stroke_color_rgb
                )

                # Convert back to OpenCV BGR format
                frame = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

            # --- Write frame ---
            out.write(frame)
            frame_count += 1
            if frame_count % 100 == 0:
                 print(f"Processed {frame_count}/{total_frames} frames for animation...")

        print("Finished processing frames for animation.")

    except Exception as e:
        print(f"Error during caption animation loop: {e}")
        traceback.print_exc() # Print detailed traceback
        success = False
    finally:
        print("Releasing video resources...")
        if cap and cap.isOpened():
            cap.release()
        if out and out.isOpened():
            out.release()

        # Proceed with muxing only if frames were processed and temp file exists
        if frame_count > 0 and os.path.exists(temp_animated_video):
             try:
                 print("Muxing audio into animated video...")
                 ffmpeg_mux_command = [
                     'ffmpeg',
                     '-i', temp_animated_video,
                     '-i', audio_source_path,
                     '-map', '0:v:0',
                     '-map', '1:a:0',
                     '-c:v', 'copy',
                     '-c:a', 'aac',
                     '-b:a', '128k',
                     '-shortest',
                     '-y',
                     output_path
                 ]
                 cmd_string = ' '.join([str(arg) for arg in ffmpeg_mux_command])
                 print(f"Mux Command: {cmd_string}")
                 # Increased timeout for potentially long muxing operation
                 process = subprocess.run(ffmpeg_mux_command, check=True, capture_output=True, text=True, timeout=300)
                 print(f"Successfully created animated caption video: {output_path}")
                 success = True
             except subprocess.TimeoutExpired:
                 print("Error: FFmpeg muxing timed out.")
                 success = False
             except subprocess.CalledProcessError as mux_e:
                  print(f"Error during audio muxing (FFmpeg): {mux_e}")
                  print(f"FFmpeg stdout: {mux_e.stdout}")
                  print(f"FFmpeg stderr: {mux_e.stderr}")
                  success = False
             except Exception as mux_e:
                  print(f"An unexpected error occurred during audio muxing: {mux_e}")
                  success = False
             finally:
                 # Ensure cleanup even if muxing fails
                 if os.path.exists(temp_animated_video):
                     try:
                         os.remove(temp_animated_video)
                         print(f"Removed temporary animated video: {temp_animated_video}")
                     except Exception as e_clean:
                         print(f"Warning: Could not remove temp animated file: {e_clean}")
        elif not os.path.exists(temp_animated_video) and frame_count > 0:
             print(f"Error: Temp animated video file {temp_animated_video} not found, cannot mux audio.")
             success = False
        else: # frame_count == 0 or initial error before loop
             print("Skipping audio muxing due to processing error or no frames processed.")
             success = False # Ensure success is false if animation failed early

    return success 